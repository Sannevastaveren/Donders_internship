{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Stage_mini.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNlf5rxO0cwJ/AoSx3obdfu",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sannevastaveren/Donders_internship/blob/main/Notebooks/Stage_mini.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WhOaAKrdjJcY"
      },
      "source": [
        "## Mount Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nBNK6_20b3sS",
        "outputId": "24b993f0-ab59-4a93-d4e0-fecb6d26effb"
      },
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZkEGmo8b5sI"
      },
      "source": [
        "#(this will take a few minutes to install all the dependences!)\n",
        "!pip install deeplabcut\n",
        "\n",
        "# Use TensorFlow 1.x:\n",
        "%tensorflow_version 1.x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RrGEK7ItkOt9"
      },
      "source": [
        "# Deeplabcut"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pHcmhJflJCTP"
      },
      "source": [
        "import os\n",
        "\n",
        "all = []\n",
        "for root, dirs, files in os.walk(\"/content/drive/My Drive/Stage/\"):\n",
        "    for file in files:\n",
        "        if file.endswith(\".avi\"):\n",
        "          if 'downsampled' in file:\n",
        "            all.append(os.path.join(root, file))\n",
        "          \n",
        "for root, dirs, files in os.walk(\"/content/drive/My Drive/Stage/\"):\n",
        "    for file in files:\n",
        "        if file.endswith(\".avi\"):\n",
        "          old_file = os.path.join(root, file)\n",
        "          already_in = False      \n",
        "          if 'downsampled' not in file:\n",
        "             file_z = file.replace('.', 'downsampled.')\n",
        "             if os.path.join(root, file_z) in all:\n",
        "                already_in = True\n",
        "                #print(os.path.join(root, file))\n",
        "             if already_in is False:\n",
        "                old_file = os.path.join(root, file)\n",
        "                file_l = file.split('.')\n",
        "                file_l[1] = 'downsampled'\n",
        "                file_l.append('.avi')\n",
        "                file = ''.join(file_l)\n",
        "                new_file =  os.path.join(root, file)\n",
        "                !ffmpeg -i \"$old_file\" -filter:v scale=-1:256 -c:a copy \"$new_file\"\n",
        "                os.remove(old_file)\n",
        "             if already_in is True:\n",
        "                os.remove(old_file)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d68wDMoGj51e"
      },
      "source": [
        "### Variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PDHeL1XScAE4",
        "outputId": "71fd2081-a7ba-4cef-ebe3-59b7e5b87389"
      },
      "source": [
        "import deeplabcut\n",
        "#Setup your project variables:\n",
        "# PLEASE EDIT THESE:\n",
        "  \n",
        "ProjectFolderName = 'Stage/final_tracker-Sanne-2021-10-20'\n",
        "VideoType = 'avi' \n",
        "\n",
        "#don't edit these:\n",
        "videofile_path = '/content/drive/My Drive/'+ProjectFolderName+'/videos/'\n",
        "videofile_path\n",
        "#Enter the list of videos or folder to analyze.\n",
        "\n",
        "#This creates a path variable that links to your google drive copy\n",
        "#No need to edit this, as you set it up before: \n",
        "path_config_file = '/content/drive/My Drive/'+ProjectFolderName+'/config.yaml'\n",
        "path_config_file\n",
        "\n",
        "\n",
        "#GUIs don't work on the cloud, so label your data locally on your computer! This will suppress the GUI support\n",
        "import os\n",
        "os.environ[\"DLClight\"]=\"True\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DLC loaded in light mode; you cannot use any GUI (labeling, relabeling and standalone GUI)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s4JcQQOUj8vE"
      },
      "source": [
        "### Version"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQokhHVVccO1"
      },
      "source": [
        "deeplabcut.__version__"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19J_42S4j_Ci"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8tvti87Pce7h",
        "outputId": "e9165551-a3ee-44e0-8522-c52a27418ea4"
      },
      "source": [
        "deeplabcut.create_training_dataset(path_config_file, net_type='resnet_50', augmenter_type='imgaug')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Stage/final_tracker-Sanne-2021-10-20/training-datasets/iteration-0/UnaugmentedDataSet_final_trackerOct20  already exists!\n",
            "/content/drive/My Drive/Stage/final_tracker-Sanne-2021-10-20/labeled-data/trail_4_bluedownsampled/CollectedData_Sanne.h5  not found (perhaps not annotated).\n",
            "/content/drive/My Drive/Stage/final_tracker-Sanne-2021-10-20/labeled-data/trail_4_greendownsampled/CollectedData_Sanne.h5  not found (perhaps not annotated).\n",
            "/content/drive/My Drive/Stage/final_tracker-Sanne-2021-10-20/labeled-data/trail_4_white(2)downsampled/CollectedData_Sanne.h5  not found (perhaps not annotated).\n",
            "/content/drive/My Drive/Stage/final_tracker-Sanne-2021-10-20/labeled-data/trail_4_whitedownsampled/CollectedData_Sanne.h5  not found (perhaps not annotated).\n",
            "/content/drive/My Drive/Stage/final_tracker-Sanne-2021-10-20/labeled-data/trail_5_greendownsampled/CollectedData_Sanne.h5  not found (perhaps not annotated).\n",
            "/content/drive/My Drive/Stage/final_tracker-Sanne-2021-10-20/labeled-data/trail_5_white(2)downsampled/CollectedData_Sanne.h5  not found (perhaps not annotated).\n",
            "/content/drive/My Drive/Stage/final_tracker-Sanne-2021-10-20/labeled-data/trail_5_whitedownsampled/CollectedData_Sanne.h5  not found (perhaps not annotated).\n",
            "It appears that the images were labeled on a Windows system, but you are currently trying to create a training set on a Unix system. \n",
            " In this case the paths should be converted. Do you want to proceed with the conversion?\n",
            "yes/noy\n",
            "Annotation data converted to unix format...\n",
            "Downloading a ImageNet-pretrained model from http://download.tensorflow.org/models/resnet_v1_50_2016_08_28.tar.gz....\n",
            "/content/drive/My Drive/Stage/final_tracker-Sanne-2021-10-20/dlc-models/iteration-0/final_trackerOct20-trainset95shuffle1  already exists!\n",
            "/content/drive/My Drive/Stage/final_tracker-Sanne-2021-10-20/dlc-models/iteration-0/final_trackerOct20-trainset95shuffle1/train  already exists!\n",
            "/content/drive/My Drive/Stage/final_tracker-Sanne-2021-10-20/dlc-models/iteration-0/final_trackerOct20-trainset95shuffle1/test  already exists!\n",
            "The training dataset is successfully created. Use the function 'train_network' to start training. Happy training!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[(0.95,\n",
              "  1,\n",
              "  (array([153, 244,  13, 195,  32,  49,  74, 316, 189, 154, 114, 305, 107,\n",
              "          267, 318, 295, 255,  50, 221,  83, 214, 339, 242, 171, 245, 187,\n",
              "           34, 115, 302,  11, 313, 119, 231, 164, 218, 151, 317, 323, 156,\n",
              "          262,  15, 182, 257, 307, 232,  40, 249,  17,  93, 178, 265,   1,\n",
              "          141, 191, 236, 117,  86, 286, 104, 225, 294, 234,  92, 161, 241,\n",
              "          300, 209, 174, 150, 113,  37, 293,  80, 126, 198, 299, 143, 148,\n",
              "          135, 311, 190,  91, 110, 291, 181, 296, 258, 263, 341, 272, 152,\n",
              "          288, 136,  47, 109,  20, 325, 227,  29, 335, 212,  65, 213, 192,\n",
              "           39,  41, 310, 273,  70,  68,  95,  35, 336, 337, 252, 327,  98,\n",
              "          256,  59, 144, 199, 229, 274, 146,  36, 196, 306, 169,  30, 179,\n",
              "          173, 138, 132, 284,  94, 319,  48, 333,  16,  88, 125,  26, 103,\n",
              "           72,  87,  25, 285, 277, 170, 131, 134, 158,  21, 120, 207, 157,\n",
              "          282, 180,  46, 253, 290, 175, 276, 235,  19, 102, 108,  78, 210,\n",
              "           63, 283, 112, 116,  76,  54, 162, 183, 228, 315, 230, 219, 200,\n",
              "           51, 137, 309, 145, 271, 217,  12, 303, 279, 204,  43, 248, 224,\n",
              "          259, 159, 269,  31,  53,  44, 324, 176, 206, 338, 266, 216, 133,\n",
              "          343, 322, 100,  52, 268, 320, 184, 281, 344, 111, 129, 238, 254,\n",
              "          122,  75,  24, 166, 334, 297, 314,  66, 233, 251, 105,  96,  79,\n",
              "           10,  18, 142, 185, 260, 163,  69,  55,  81,   4,  23,  60, 222,\n",
              "           45,  82, 118, 261, 237, 123, 312, 220, 340,  85,  22,  57,  84,\n",
              "           73, 326,  97,   9, 186, 332, 330, 177, 280,   0, 239, 331,  89,\n",
              "          270,  27,  77, 165, 202, 321,  62, 193, 215, 289, 246, 149, 172,\n",
              "          203, 139, 201, 160, 342,   6, 240,  56, 275, 287, 292,  42, 127,\n",
              "          278,  28, 328, 130, 308, 194, 205, 188, 155,   5,  90,  67, 101,\n",
              "          264,  38,  99, 168, 226,  14, 301,   2,  61, 197, 298, 106, 304,\n",
              "           58,  33]),\n",
              "   array([ 64, 147, 167, 250, 329, 211, 247,   7, 140,   3, 243, 128, 121,\n",
              "          223,   8, 208,  71, 124])))]"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fkBfQrtkchQC",
        "outputId": "4ce85bf1-1a8c-4e88-ad59-b464695e6480"
      },
      "source": [
        "deeplabcut.train_network(path_config_file, displayiters=10,saveiters=15000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Config:\n",
            "{'all_joints': [[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10]],\n",
            " 'all_joints_names': ['Head',\n",
            "                      'Sideleft',\n",
            "                      'Sideright',\n",
            "                      'Spine1',\n",
            "                      'Tailbase',\n",
            "                      'Tailmiddle',\n",
            "                      'Tailend',\n",
            "                      'CornerTopLeft',\n",
            "                      'CornerBottomLeft',\n",
            "                      'CornerTopRight',\n",
            "                      'CornerBottomRight'],\n",
            " 'alpha_r': 0.02,\n",
            " 'apply_prob': 0.5,\n",
            " 'batch_size': 1,\n",
            " 'clahe': True,\n",
            " 'claheratio': 0.1,\n",
            " 'crop_pad': 0,\n",
            " 'crop_sampling': 'hybrid',\n",
            " 'crop_size': [400, 400],\n",
            " 'cropratio': 0.4,\n",
            " 'dataset': 'training-datasets/iteration-0/UnaugmentedDataSet_final_trackerOct20/final_tracker_Sanne95shuffle1.mat',\n",
            " 'dataset_type': 'imgaug',\n",
            " 'decay_steps': 30000,\n",
            " 'deterministic': False,\n",
            " 'display_iters': 1000,\n",
            " 'edge': False,\n",
            " 'emboss': {'alpha': [0.0, 1.0], 'embossratio': 0.1, 'strength': [0.5, 1.5]},\n",
            " 'fg_fraction': 0.25,\n",
            " 'global_scale': 0.8,\n",
            " 'histeq': True,\n",
            " 'histeqratio': 0.1,\n",
            " 'init_weights': '/usr/local/lib/python3.7/dist-packages/deeplabcut/pose_estimation_tensorflow/models/pretrained/resnet_v1_50.ckpt',\n",
            " 'intermediate_supervision': False,\n",
            " 'intermediate_supervision_layer': 12,\n",
            " 'location_refinement': True,\n",
            " 'locref_huber_loss': True,\n",
            " 'locref_loss_weight': 0.05,\n",
            " 'locref_stdev': 7.2801,\n",
            " 'log_dir': 'log',\n",
            " 'lr_init': 0.0005,\n",
            " 'max_input_size': 1500,\n",
            " 'max_shift': 0.4,\n",
            " 'mean_pixel': [123.68, 116.779, 103.939],\n",
            " 'metadataset': 'training-datasets/iteration-0/UnaugmentedDataSet_final_trackerOct20/Documentation_data-final_tracker_95shuffle1.pickle',\n",
            " 'min_input_size': 64,\n",
            " 'mirror': False,\n",
            " 'multi_stage': False,\n",
            " 'multi_step': [[0.005, 10000],\n",
            "                [0.02, 430000],\n",
            "                [0.002, 730000],\n",
            "                [0.001, 1030000]],\n",
            " 'net_type': 'resnet_50',\n",
            " 'num_joints': 11,\n",
            " 'optimizer': 'sgd',\n",
            " 'pairwise_huber_loss': False,\n",
            " 'pairwise_predict': False,\n",
            " 'partaffinityfield_predict': False,\n",
            " 'pos_dist_thresh': 17,\n",
            " 'pre_resize': [],\n",
            " 'project_path': '/content/drive/My Drive/Stage/final_tracker-Sanne-2021-10-20',\n",
            " 'regularize': False,\n",
            " 'rotation': 25,\n",
            " 'rotratio': 0.4,\n",
            " 'save_iters': 50000,\n",
            " 'scale_jitter_lo': 0.5,\n",
            " 'scale_jitter_up': 1.25,\n",
            " 'scoremap_dir': 'test',\n",
            " 'sharpen': False,\n",
            " 'sharpenratio': 0.3,\n",
            " 'shuffle': True,\n",
            " 'snapshot_prefix': '/content/drive/My '\n",
            "                    'Drive/Stage/final_tracker-Sanne-2021-10-20/dlc-models/iteration-0/final_trackerOct20-trainset95shuffle1/train/snapshot',\n",
            " 'stride': 8.0,\n",
            " 'weigh_negatives': False,\n",
            " 'weigh_only_present_joints': False,\n",
            " 'weigh_part_predictions': False,\n",
            " 'weight_decay': 0.0001}\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Selecting single-animal trainer\n",
            "Batch Size is 1\n",
            "Loading ImageNet-pretrained resnet_50\n",
            "Display_iters overwritten as 10\n",
            "Save_iters overwritten as 1500\n",
            "Training parameter:\n",
            "{'stride': 8.0, 'weigh_part_predictions': False, 'weigh_negatives': False, 'fg_fraction': 0.25, 'mean_pixel': [123.68, 116.779, 103.939], 'shuffle': True, 'snapshot_prefix': '/content/drive/My Drive/Stage/final_tracker-Sanne-2021-10-20/dlc-models/iteration-0/final_trackerOct20-trainset95shuffle1/train/snapshot', 'log_dir': 'log', 'global_scale': 0.8, 'location_refinement': True, 'locref_stdev': 7.2801, 'locref_loss_weight': 0.05, 'locref_huber_loss': True, 'optimizer': 'sgd', 'intermediate_supervision': False, 'intermediate_supervision_layer': 12, 'regularize': False, 'weight_decay': 0.0001, 'crop_pad': 0, 'scoremap_dir': 'test', 'batch_size': 1, 'dataset_type': 'imgaug', 'deterministic': False, 'mirror': False, 'pairwise_huber_loss': False, 'weigh_only_present_joints': False, 'partaffinityfield_predict': False, 'pairwise_predict': False, 'all_joints': [[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10]], 'all_joints_names': ['Head', 'Sideleft', 'Sideright', 'Spine1', 'Tailbase', 'Tailmiddle', 'Tailend', 'CornerTopLeft', 'CornerBottomLeft', 'CornerTopRight', 'CornerBottomRight'], 'alpha_r': 0.02, 'apply_prob': 0.5, 'clahe': True, 'claheratio': 0.1, 'crop_sampling': 'hybrid', 'crop_size': [400, 400], 'cropratio': 0.4, 'dataset': 'training-datasets/iteration-0/UnaugmentedDataSet_final_trackerOct20/final_tracker_Sanne95shuffle1.mat', 'decay_steps': 30000, 'display_iters': 1000, 'edge': False, 'emboss': {'alpha': [0.0, 1.0], 'embossratio': 0.1, 'strength': [0.5, 1.5]}, 'histeq': True, 'histeqratio': 0.1, 'init_weights': '/usr/local/lib/python3.7/dist-packages/deeplabcut/pose_estimation_tensorflow/models/pretrained/resnet_v1_50.ckpt', 'lr_init': 0.0005, 'max_input_size': 1500, 'max_shift': 0.4, 'metadataset': 'training-datasets/iteration-0/UnaugmentedDataSet_final_trackerOct20/Documentation_data-final_tracker_95shuffle1.pickle', 'min_input_size': 64, 'multi_stage': False, 'multi_step': [[0.005, 10000], [0.02, 430000], [0.002, 730000], [0.001, 1030000]], 'net_type': 'resnet_50', 'num_joints': 11, 'pos_dist_thresh': 17, 'pre_resize': [], 'project_path': '/content/drive/My Drive/Stage/final_tracker-Sanne-2021-10-20', 'rotation': 25, 'rotratio': 0.4, 'save_iters': 50000, 'scale_jitter_lo': 0.5, 'scale_jitter_up': 1.25, 'sharpen': False, 'sharpenratio': 0.3, 'covering': True, 'elastic_transform': True, 'motion_blur': True, 'motion_blur_params': {'k': 7, 'angle': (-90, 90)}}\n",
            "Starting training....\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaminguitvoer ingekort tot de laatste 5000 regels.\u001b[0m\n",
            "iteration: 261820 loss: 0.0031 lr: 0.02\n",
            "iteration: 261830 loss: 0.0037 lr: 0.02\n",
            "iteration: 261840 loss: 0.0042 lr: 0.02\n",
            "iteration: 261850 loss: 0.0043 lr: 0.02\n",
            "iteration: 261860 loss: 0.0037 lr: 0.02\n",
            "iteration: 261870 loss: 0.0043 lr: 0.02\n",
            "iteration: 261880 loss: 0.0040 lr: 0.02\n",
            "iteration: 261890 loss: 0.0040 lr: 0.02\n",
            "iteration: 261900 loss: 0.0033 lr: 0.02\n",
            "iteration: 261910 loss: 0.0043 lr: 0.02\n",
            "iteration: 261920 loss: 0.0050 lr: 0.02\n",
            "iteration: 261930 loss: 0.0044 lr: 0.02\n",
            "iteration: 261940 loss: 0.0049 lr: 0.02\n",
            "iteration: 261950 loss: 0.0035 lr: 0.02\n",
            "iteration: 261960 loss: 0.0035 lr: 0.02\n",
            "iteration: 261970 loss: 0.0036 lr: 0.02\n",
            "iteration: 261980 loss: 0.0031 lr: 0.02\n",
            "iteration: 261990 loss: 0.0035 lr: 0.02\n",
            "iteration: 262000 loss: 0.0028 lr: 0.02\n",
            "iteration: 262010 loss: 0.0030 lr: 0.02\n",
            "iteration: 262020 loss: 0.0033 lr: 0.02\n",
            "iteration: 262030 loss: 0.0030 lr: 0.02\n",
            "iteration: 262040 loss: 0.0033 lr: 0.02\n",
            "iteration: 262050 loss: 0.0028 lr: 0.02\n",
            "iteration: 262060 loss: 0.0037 lr: 0.02\n",
            "iteration: 262070 loss: 0.0027 lr: 0.02\n",
            "iteration: 262080 loss: 0.0052 lr: 0.02\n",
            "iteration: 262090 loss: 0.0035 lr: 0.02\n",
            "iteration: 262100 loss: 0.0041 lr: 0.02\n",
            "iteration: 262110 loss: 0.0043 lr: 0.02\n",
            "iteration: 262120 loss: 0.0030 lr: 0.02\n",
            "iteration: 262130 loss: 0.0038 lr: 0.02\n",
            "iteration: 262140 loss: 0.0028 lr: 0.02\n",
            "iteration: 262150 loss: 0.0041 lr: 0.02\n",
            "iteration: 262160 loss: 0.0026 lr: 0.02\n",
            "iteration: 262170 loss: 0.0039 lr: 0.02\n",
            "iteration: 262180 loss: 0.0043 lr: 0.02\n",
            "iteration: 262190 loss: 0.0047 lr: 0.02\n",
            "iteration: 262200 loss: 0.0034 lr: 0.02\n",
            "iteration: 262210 loss: 0.0040 lr: 0.02\n",
            "iteration: 262220 loss: 0.0042 lr: 0.02\n",
            "iteration: 262230 loss: 0.0030 lr: 0.02\n",
            "iteration: 262240 loss: 0.0031 lr: 0.02\n",
            "iteration: 262250 loss: 0.0032 lr: 0.02\n",
            "iteration: 262260 loss: 0.0032 lr: 0.02\n",
            "iteration: 262270 loss: 0.0036 lr: 0.02\n",
            "iteration: 262280 loss: 0.0040 lr: 0.02\n",
            "iteration: 262290 loss: 0.0030 lr: 0.02\n",
            "iteration: 262300 loss: 0.0032 lr: 0.02\n",
            "iteration: 262310 loss: 0.0048 lr: 0.02\n",
            "iteration: 262320 loss: 0.0040 lr: 0.02\n",
            "iteration: 262330 loss: 0.0040 lr: 0.02\n",
            "iteration: 262340 loss: 0.0034 lr: 0.02\n",
            "iteration: 262350 loss: 0.0038 lr: 0.02\n",
            "iteration: 262360 loss: 0.0037 lr: 0.02\n",
            "iteration: 262370 loss: 0.0065 lr: 0.02\n",
            "iteration: 262380 loss: 0.0036 lr: 0.02\n",
            "iteration: 262390 loss: 0.0040 lr: 0.02\n",
            "iteration: 262400 loss: 0.0036 lr: 0.02\n",
            "iteration: 262410 loss: 0.0038 lr: 0.02\n",
            "iteration: 262420 loss: 0.0037 lr: 0.02\n",
            "iteration: 262430 loss: 0.0037 lr: 0.02\n",
            "iteration: 262440 loss: 0.0041 lr: 0.02\n",
            "iteration: 262450 loss: 0.0027 lr: 0.02\n",
            "iteration: 262460 loss: 0.0031 lr: 0.02\n",
            "iteration: 262470 loss: 0.0032 lr: 0.02\n",
            "iteration: 262480 loss: 0.0034 lr: 0.02\n",
            "iteration: 262490 loss: 0.0035 lr: 0.02\n",
            "iteration: 262500 loss: 0.0030 lr: 0.02\n",
            "iteration: 262510 loss: 0.0039 lr: 0.02\n",
            "iteration: 262520 loss: 0.0027 lr: 0.02\n",
            "iteration: 262530 loss: 0.0034 lr: 0.02\n",
            "iteration: 262540 loss: 0.0043 lr: 0.02\n",
            "iteration: 262550 loss: 0.0035 lr: 0.02\n",
            "iteration: 262560 loss: 0.0041 lr: 0.02\n",
            "iteration: 262570 loss: 0.0042 lr: 0.02\n",
            "iteration: 262580 loss: 0.0032 lr: 0.02\n",
            "iteration: 262590 loss: 0.0038 lr: 0.02\n",
            "iteration: 262600 loss: 0.0035 lr: 0.02\n",
            "iteration: 262610 loss: 0.0043 lr: 0.02\n",
            "iteration: 262620 loss: 0.0039 lr: 0.02\n",
            "iteration: 262630 loss: 0.0033 lr: 0.02\n",
            "iteration: 262640 loss: 0.0028 lr: 0.02\n",
            "iteration: 262650 loss: 0.0049 lr: 0.02\n",
            "iteration: 262660 loss: 0.0036 lr: 0.02\n",
            "iteration: 262670 loss: 0.0044 lr: 0.02\n",
            "iteration: 262680 loss: 0.0030 lr: 0.02\n",
            "iteration: 262690 loss: 0.0052 lr: 0.02\n",
            "iteration: 262700 loss: 0.0031 lr: 0.02\n",
            "iteration: 262710 loss: 0.0030 lr: 0.02\n",
            "iteration: 262720 loss: 0.0037 lr: 0.02\n",
            "iteration: 262730 loss: 0.0041 lr: 0.02\n",
            "iteration: 262740 loss: 0.0036 lr: 0.02\n",
            "iteration: 262750 loss: 0.0036 lr: 0.02\n",
            "iteration: 262760 loss: 0.0035 lr: 0.02\n",
            "iteration: 262770 loss: 0.0028 lr: 0.02\n",
            "iteration: 262780 loss: 0.0042 lr: 0.02\n",
            "iteration: 262790 loss: 0.0036 lr: 0.02\n",
            "iteration: 262800 loss: 0.0039 lr: 0.02\n",
            "iteration: 262810 loss: 0.0033 lr: 0.02\n",
            "iteration: 262820 loss: 0.0030 lr: 0.02\n",
            "iteration: 262830 loss: 0.0031 lr: 0.02\n",
            "iteration: 262840 loss: 0.0030 lr: 0.02\n",
            "iteration: 262850 loss: 0.0027 lr: 0.02\n",
            "iteration: 262860 loss: 0.0046 lr: 0.02\n",
            "iteration: 262870 loss: 0.0038 lr: 0.02\n",
            "iteration: 262880 loss: 0.0043 lr: 0.02\n",
            "iteration: 262890 loss: 0.0030 lr: 0.02\n",
            "iteration: 262900 loss: 0.0039 lr: 0.02\n",
            "iteration: 262910 loss: 0.0028 lr: 0.02\n",
            "iteration: 262920 loss: 0.0033 lr: 0.02\n",
            "iteration: 262930 loss: 0.0039 lr: 0.02\n",
            "iteration: 262940 loss: 0.0027 lr: 0.02\n",
            "iteration: 262950 loss: 0.0030 lr: 0.02\n",
            "iteration: 262960 loss: 0.0029 lr: 0.02\n",
            "iteration: 262970 loss: 0.0034 lr: 0.02\n",
            "iteration: 262980 loss: 0.0030 lr: 0.02\n",
            "iteration: 262990 loss: 0.0035 lr: 0.02\n",
            "iteration: 263000 loss: 0.0033 lr: 0.02\n",
            "iteration: 263010 loss: 0.0035 lr: 0.02\n",
            "iteration: 263020 loss: 0.0050 lr: 0.02\n",
            "iteration: 263030 loss: 0.0037 lr: 0.02\n",
            "iteration: 263040 loss: 0.0036 lr: 0.02\n",
            "iteration: 263050 loss: 0.0027 lr: 0.02\n",
            "iteration: 263060 loss: 0.0033 lr: 0.02\n",
            "iteration: 263070 loss: 0.0028 lr: 0.02\n",
            "iteration: 263080 loss: 0.0030 lr: 0.02\n",
            "iteration: 263090 loss: 0.0041 lr: 0.02\n",
            "iteration: 263100 loss: 0.0031 lr: 0.02\n",
            "iteration: 263110 loss: 0.0041 lr: 0.02\n",
            "iteration: 263120 loss: 0.0034 lr: 0.02\n",
            "iteration: 263130 loss: 0.0033 lr: 0.02\n",
            "iteration: 263140 loss: 0.0029 lr: 0.02\n",
            "iteration: 263150 loss: 0.0040 lr: 0.02\n",
            "iteration: 263160 loss: 0.0031 lr: 0.02\n",
            "iteration: 263170 loss: 0.0044 lr: 0.02\n",
            "iteration: 263180 loss: 0.0034 lr: 0.02\n",
            "iteration: 263190 loss: 0.0034 lr: 0.02\n",
            "iteration: 263200 loss: 0.0029 lr: 0.02\n",
            "iteration: 263210 loss: 0.0043 lr: 0.02\n",
            "iteration: 263220 loss: 0.0035 lr: 0.02\n",
            "iteration: 263230 loss: 0.0027 lr: 0.02\n",
            "iteration: 263240 loss: 0.0043 lr: 0.02\n",
            "iteration: 263250 loss: 0.0038 lr: 0.02\n",
            "iteration: 263260 loss: 0.0035 lr: 0.02\n",
            "iteration: 263270 loss: 0.0042 lr: 0.02\n",
            "iteration: 263280 loss: 0.0030 lr: 0.02\n",
            "iteration: 263290 loss: 0.0036 lr: 0.02\n",
            "iteration: 263300 loss: 0.0041 lr: 0.02\n",
            "iteration: 263310 loss: 0.0037 lr: 0.02\n",
            "iteration: 263320 loss: 0.0043 lr: 0.02\n",
            "iteration: 263330 loss: 0.0035 lr: 0.02\n",
            "iteration: 263340 loss: 0.0035 lr: 0.02\n",
            "iteration: 263350 loss: 0.0041 lr: 0.02\n",
            "iteration: 263360 loss: 0.0038 lr: 0.02\n",
            "iteration: 263370 loss: 0.0034 lr: 0.02\n",
            "iteration: 263380 loss: 0.0030 lr: 0.02\n",
            "iteration: 263390 loss: 0.0030 lr: 0.02\n",
            "iteration: 263400 loss: 0.0030 lr: 0.02\n",
            "iteration: 263410 loss: 0.0031 lr: 0.02\n",
            "iteration: 263420 loss: 0.0033 lr: 0.02\n",
            "iteration: 263430 loss: 0.0036 lr: 0.02\n",
            "iteration: 263440 loss: 0.0036 lr: 0.02\n",
            "iteration: 263450 loss: 0.0029 lr: 0.02\n",
            "iteration: 263460 loss: 0.0034 lr: 0.02\n",
            "iteration: 263470 loss: 0.0031 lr: 0.02\n",
            "iteration: 263480 loss: 0.0038 lr: 0.02\n",
            "iteration: 263490 loss: 0.0039 lr: 0.02\n",
            "iteration: 263500 loss: 0.0039 lr: 0.02\n",
            "iteration: 263510 loss: 0.0035 lr: 0.02\n",
            "iteration: 263520 loss: 0.0032 lr: 0.02\n",
            "iteration: 263530 loss: 0.0043 lr: 0.02\n",
            "iteration: 263540 loss: 0.0044 lr: 0.02\n",
            "iteration: 263550 loss: 0.0042 lr: 0.02\n",
            "iteration: 263560 loss: 0.0048 lr: 0.02\n",
            "iteration: 263570 loss: 0.0032 lr: 0.02\n",
            "iteration: 263580 loss: 0.0034 lr: 0.02\n",
            "iteration: 263590 loss: 0.0031 lr: 0.02\n",
            "iteration: 263600 loss: 0.0033 lr: 0.02\n",
            "iteration: 263610 loss: 0.0031 lr: 0.02\n",
            "iteration: 263620 loss: 0.0047 lr: 0.02\n",
            "iteration: 263630 loss: 0.0044 lr: 0.02\n",
            "iteration: 263640 loss: 0.0040 lr: 0.02\n",
            "iteration: 263650 loss: 0.0037 lr: 0.02\n",
            "iteration: 263660 loss: 0.0044 lr: 0.02\n",
            "iteration: 263670 loss: 0.0049 lr: 0.02\n",
            "iteration: 263680 loss: 0.0039 lr: 0.02\n",
            "iteration: 263690 loss: 0.0033 lr: 0.02\n",
            "iteration: 263700 loss: 0.0037 lr: 0.02\n",
            "iteration: 263710 loss: 0.0045 lr: 0.02\n",
            "iteration: 263720 loss: 0.0035 lr: 0.02\n",
            "iteration: 263730 loss: 0.0047 lr: 0.02\n",
            "iteration: 263740 loss: 0.0034 lr: 0.02\n",
            "iteration: 263750 loss: 0.0034 lr: 0.02\n",
            "iteration: 263760 loss: 0.0033 lr: 0.02\n",
            "iteration: 263770 loss: 0.0037 lr: 0.02\n",
            "iteration: 263780 loss: 0.0033 lr: 0.02\n",
            "iteration: 263790 loss: 0.0032 lr: 0.02\n",
            "iteration: 263800 loss: 0.0029 lr: 0.02\n",
            "iteration: 263810 loss: 0.0026 lr: 0.02\n",
            "iteration: 263820 loss: 0.0032 lr: 0.02\n",
            "iteration: 263830 loss: 0.0029 lr: 0.02\n",
            "iteration: 263840 loss: 0.0033 lr: 0.02\n",
            "iteration: 263850 loss: 0.0034 lr: 0.02\n",
            "iteration: 263860 loss: 0.0043 lr: 0.02\n",
            "iteration: 263870 loss: 0.0035 lr: 0.02\n",
            "iteration: 263880 loss: 0.0031 lr: 0.02\n",
            "iteration: 263890 loss: 0.0035 lr: 0.02\n",
            "iteration: 263900 loss: 0.0027 lr: 0.02\n",
            "iteration: 263910 loss: 0.0024 lr: 0.02\n",
            "iteration: 263920 loss: 0.0037 lr: 0.02\n",
            "iteration: 263930 loss: 0.0033 lr: 0.02\n",
            "iteration: 263940 loss: 0.0051 lr: 0.02\n",
            "iteration: 263950 loss: 0.0034 lr: 0.02\n",
            "iteration: 263960 loss: 0.0035 lr: 0.02\n",
            "iteration: 263970 loss: 0.0049 lr: 0.02\n",
            "iteration: 263980 loss: 0.0038 lr: 0.02\n",
            "iteration: 263990 loss: 0.0032 lr: 0.02\n",
            "iteration: 264000 loss: 0.0053 lr: 0.02\n",
            "iteration: 264010 loss: 0.0040 lr: 0.02\n",
            "iteration: 264020 loss: 0.0031 lr: 0.02\n",
            "iteration: 264030 loss: 0.0028 lr: 0.02\n",
            "iteration: 264040 loss: 0.0041 lr: 0.02\n",
            "iteration: 264050 loss: 0.0050 lr: 0.02\n",
            "iteration: 264060 loss: 0.0043 lr: 0.02\n",
            "iteration: 264070 loss: 0.0042 lr: 0.02\n",
            "iteration: 264080 loss: 0.0031 lr: 0.02\n",
            "iteration: 264090 loss: 0.0037 lr: 0.02\n",
            "iteration: 264100 loss: 0.0036 lr: 0.02\n",
            "iteration: 264110 loss: 0.0036 lr: 0.02\n",
            "iteration: 264120 loss: 0.0035 lr: 0.02\n",
            "iteration: 264130 loss: 0.0033 lr: 0.02\n",
            "iteration: 264140 loss: 0.0034 lr: 0.02\n",
            "iteration: 264150 loss: 0.0035 lr: 0.02\n",
            "iteration: 264160 loss: 0.0030 lr: 0.02\n",
            "iteration: 264170 loss: 0.0051 lr: 0.02\n",
            "iteration: 264180 loss: 0.0041 lr: 0.02\n",
            "iteration: 264190 loss: 0.0033 lr: 0.02\n",
            "iteration: 264200 loss: 0.0039 lr: 0.02\n",
            "iteration: 264210 loss: 0.0046 lr: 0.02\n",
            "iteration: 264220 loss: 0.0040 lr: 0.02\n",
            "iteration: 264230 loss: 0.0048 lr: 0.02\n",
            "iteration: 264240 loss: 0.0034 lr: 0.02\n",
            "iteration: 264250 loss: 0.0033 lr: 0.02\n",
            "iteration: 264260 loss: 0.0038 lr: 0.02\n",
            "iteration: 264270 loss: 0.0036 lr: 0.02\n",
            "iteration: 264280 loss: 0.0058 lr: 0.02\n",
            "iteration: 264290 loss: 0.0034 lr: 0.02\n",
            "iteration: 264300 loss: 0.0034 lr: 0.02\n",
            "iteration: 264310 loss: 0.0034 lr: 0.02\n",
            "iteration: 264320 loss: 0.0036 lr: 0.02\n",
            "iteration: 264330 loss: 0.0033 lr: 0.02\n",
            "iteration: 264340 loss: 0.0033 lr: 0.02\n",
            "iteration: 264350 loss: 0.0037 lr: 0.02\n",
            "iteration: 264360 loss: 0.0030 lr: 0.02\n",
            "iteration: 264370 loss: 0.0041 lr: 0.02\n",
            "iteration: 264380 loss: 0.0037 lr: 0.02\n",
            "iteration: 264390 loss: 0.0035 lr: 0.02\n",
            "iteration: 264400 loss: 0.0041 lr: 0.02\n",
            "iteration: 264410 loss: 0.0037 lr: 0.02\n",
            "iteration: 264420 loss: 0.0046 lr: 0.02\n",
            "iteration: 264430 loss: 0.0044 lr: 0.02\n",
            "iteration: 264440 loss: 0.0033 lr: 0.02\n",
            "iteration: 264450 loss: 0.0040 lr: 0.02\n",
            "iteration: 264460 loss: 0.0030 lr: 0.02\n",
            "iteration: 264470 loss: 0.0028 lr: 0.02\n",
            "iteration: 264480 loss: 0.0038 lr: 0.02\n",
            "iteration: 264490 loss: 0.0032 lr: 0.02\n",
            "iteration: 264500 loss: 0.0033 lr: 0.02\n",
            "iteration: 264510 loss: 0.0036 lr: 0.02\n",
            "iteration: 264520 loss: 0.0040 lr: 0.02\n",
            "iteration: 264530 loss: 0.0028 lr: 0.02\n",
            "iteration: 264540 loss: 0.0046 lr: 0.02\n",
            "iteration: 264550 loss: 0.0024 lr: 0.02\n",
            "iteration: 264560 loss: 0.0034 lr: 0.02\n",
            "iteration: 264570 loss: 0.0024 lr: 0.02\n",
            "iteration: 264580 loss: 0.0038 lr: 0.02\n",
            "iteration: 264590 loss: 0.0042 lr: 0.02\n",
            "iteration: 264600 loss: 0.0034 lr: 0.02\n",
            "iteration: 264610 loss: 0.0035 lr: 0.02\n",
            "iteration: 264620 loss: 0.0030 lr: 0.02\n",
            "iteration: 264630 loss: 0.0034 lr: 0.02\n",
            "iteration: 264640 loss: 0.0037 lr: 0.02\n",
            "iteration: 264650 loss: 0.0035 lr: 0.02\n",
            "iteration: 264660 loss: 0.0035 lr: 0.02\n",
            "iteration: 264670 loss: 0.0042 lr: 0.02\n",
            "iteration: 264680 loss: 0.0037 lr: 0.02\n",
            "iteration: 264690 loss: 0.0041 lr: 0.02\n",
            "iteration: 264700 loss: 0.0033 lr: 0.02\n",
            "iteration: 264710 loss: 0.0029 lr: 0.02\n",
            "iteration: 264720 loss: 0.0033 lr: 0.02\n",
            "iteration: 264730 loss: 0.0039 lr: 0.02\n",
            "iteration: 264740 loss: 0.0032 lr: 0.02\n",
            "iteration: 264750 loss: 0.0023 lr: 0.02\n",
            "iteration: 264760 loss: 0.0040 lr: 0.02\n",
            "iteration: 264770 loss: 0.0069 lr: 0.02\n",
            "iteration: 264780 loss: 0.0036 lr: 0.02\n",
            "iteration: 264790 loss: 0.0032 lr: 0.02\n",
            "iteration: 264800 loss: 0.0043 lr: 0.02\n",
            "iteration: 264810 loss: 0.0044 lr: 0.02\n",
            "iteration: 264820 loss: 0.0036 lr: 0.02\n",
            "iteration: 264830 loss: 0.0040 lr: 0.02\n",
            "iteration: 264840 loss: 0.0036 lr: 0.02\n",
            "iteration: 264850 loss: 0.0032 lr: 0.02\n",
            "iteration: 264860 loss: 0.0036 lr: 0.02\n",
            "iteration: 264870 loss: 0.0040 lr: 0.02\n",
            "iteration: 264880 loss: 0.0038 lr: 0.02\n",
            "iteration: 264890 loss: 0.0037 lr: 0.02\n",
            "iteration: 264900 loss: 0.0045 lr: 0.02\n",
            "iteration: 264910 loss: 0.0051 lr: 0.02\n",
            "iteration: 264920 loss: 0.0036 lr: 0.02\n",
            "iteration: 264930 loss: 0.0022 lr: 0.02\n",
            "iteration: 264940 loss: 0.0028 lr: 0.02\n",
            "iteration: 264950 loss: 0.0031 lr: 0.02\n",
            "iteration: 264960 loss: 0.0040 lr: 0.02\n",
            "iteration: 264970 loss: 0.0038 lr: 0.02\n",
            "iteration: 264980 loss: 0.0029 lr: 0.02\n",
            "iteration: 264990 loss: 0.0034 lr: 0.02\n",
            "iteration: 265000 loss: 0.0039 lr: 0.02\n",
            "iteration: 265010 loss: 0.0038 lr: 0.02\n",
            "iteration: 265020 loss: 0.0035 lr: 0.02\n",
            "iteration: 265030 loss: 0.0041 lr: 0.02\n",
            "iteration: 265040 loss: 0.0028 lr: 0.02\n",
            "iteration: 265050 loss: 0.0033 lr: 0.02\n",
            "iteration: 265060 loss: 0.0044 lr: 0.02\n",
            "iteration: 265070 loss: 0.0046 lr: 0.02\n",
            "iteration: 265080 loss: 0.0035 lr: 0.02\n",
            "iteration: 265090 loss: 0.0033 lr: 0.02\n",
            "iteration: 265100 loss: 0.0029 lr: 0.02\n",
            "iteration: 265110 loss: 0.0033 lr: 0.02\n",
            "iteration: 265120 loss: 0.0036 lr: 0.02\n",
            "iteration: 265130 loss: 0.0027 lr: 0.02\n",
            "iteration: 265140 loss: 0.0043 lr: 0.02\n",
            "iteration: 265150 loss: 0.0033 lr: 0.02\n",
            "iteration: 265160 loss: 0.0031 lr: 0.02\n",
            "iteration: 265170 loss: 0.0041 lr: 0.02\n",
            "iteration: 265180 loss: 0.0032 lr: 0.02\n",
            "iteration: 265190 loss: 0.0038 lr: 0.02\n",
            "iteration: 265200 loss: 0.0036 lr: 0.02\n",
            "iteration: 265210 loss: 0.0039 lr: 0.02\n",
            "iteration: 265220 loss: 0.0035 lr: 0.02\n",
            "iteration: 265230 loss: 0.0038 lr: 0.02\n",
            "iteration: 265240 loss: 0.0035 lr: 0.02\n",
            "iteration: 265250 loss: 0.0030 lr: 0.02\n",
            "iteration: 265260 loss: 0.0036 lr: 0.02\n",
            "iteration: 265270 loss: 0.0034 lr: 0.02\n",
            "iteration: 265280 loss: 0.0035 lr: 0.02\n",
            "iteration: 265290 loss: 0.0047 lr: 0.02\n",
            "iteration: 265300 loss: 0.0033 lr: 0.02\n",
            "iteration: 265310 loss: 0.0034 lr: 0.02\n",
            "iteration: 265320 loss: 0.0032 lr: 0.02\n",
            "iteration: 265330 loss: 0.0034 lr: 0.02\n",
            "iteration: 265340 loss: 0.0031 lr: 0.02\n",
            "iteration: 265350 loss: 0.0040 lr: 0.02\n",
            "iteration: 265360 loss: 0.0045 lr: 0.02\n",
            "iteration: 265370 loss: 0.0037 lr: 0.02\n",
            "iteration: 265380 loss: 0.0040 lr: 0.02\n",
            "iteration: 265390 loss: 0.0032 lr: 0.02\n",
            "iteration: 265400 loss: 0.0038 lr: 0.02\n",
            "iteration: 265410 loss: 0.0037 lr: 0.02\n",
            "iteration: 265420 loss: 0.0031 lr: 0.02\n",
            "iteration: 265430 loss: 0.0029 lr: 0.02\n",
            "iteration: 265440 loss: 0.0043 lr: 0.02\n",
            "iteration: 265450 loss: 0.0038 lr: 0.02\n",
            "iteration: 265460 loss: 0.0051 lr: 0.02\n",
            "iteration: 265470 loss: 0.0031 lr: 0.02\n",
            "iteration: 265480 loss: 0.0035 lr: 0.02\n",
            "iteration: 265490 loss: 0.0030 lr: 0.02\n",
            "iteration: 265500 loss: 0.0043 lr: 0.02\n",
            "iteration: 265510 loss: 0.0037 lr: 0.02\n",
            "iteration: 265520 loss: 0.0031 lr: 0.02\n",
            "iteration: 265530 loss: 0.0038 lr: 0.02\n",
            "iteration: 265540 loss: 0.0043 lr: 0.02\n",
            "iteration: 265550 loss: 0.0039 lr: 0.02\n",
            "iteration: 265560 loss: 0.0031 lr: 0.02\n",
            "iteration: 265570 loss: 0.0034 lr: 0.02\n",
            "iteration: 265580 loss: 0.0031 lr: 0.02\n",
            "iteration: 265590 loss: 0.0031 lr: 0.02\n",
            "iteration: 265600 loss: 0.0034 lr: 0.02\n",
            "iteration: 265610 loss: 0.0031 lr: 0.02\n",
            "iteration: 265620 loss: 0.0050 lr: 0.02\n",
            "iteration: 265630 loss: 0.0034 lr: 0.02\n",
            "iteration: 265640 loss: 0.0036 lr: 0.02\n",
            "iteration: 265650 loss: 0.0041 lr: 0.02\n",
            "iteration: 265660 loss: 0.0040 lr: 0.02\n",
            "iteration: 265670 loss: 0.0045 lr: 0.02\n",
            "iteration: 265680 loss: 0.0032 lr: 0.02\n",
            "iteration: 265690 loss: 0.0038 lr: 0.02\n",
            "iteration: 265700 loss: 0.0033 lr: 0.02\n",
            "iteration: 265710 loss: 0.0030 lr: 0.02\n",
            "iteration: 265720 loss: 0.0046 lr: 0.02\n",
            "iteration: 265730 loss: 0.0036 lr: 0.02\n",
            "iteration: 265740 loss: 0.0031 lr: 0.02\n",
            "iteration: 265750 loss: 0.0034 lr: 0.02\n",
            "iteration: 265760 loss: 0.0033 lr: 0.02\n",
            "iteration: 265770 loss: 0.0029 lr: 0.02\n",
            "iteration: 265780 loss: 0.0037 lr: 0.02\n",
            "iteration: 265790 loss: 0.0029 lr: 0.02\n",
            "iteration: 265800 loss: 0.0031 lr: 0.02\n",
            "iteration: 265810 loss: 0.0042 lr: 0.02\n",
            "iteration: 265820 loss: 0.0042 lr: 0.02\n",
            "iteration: 265830 loss: 0.0036 lr: 0.02\n",
            "iteration: 265840 loss: 0.0042 lr: 0.02\n",
            "iteration: 265850 loss: 0.0042 lr: 0.02\n",
            "iteration: 265860 loss: 0.0041 lr: 0.02\n",
            "iteration: 265870 loss: 0.0038 lr: 0.02\n",
            "iteration: 265880 loss: 0.0036 lr: 0.02\n",
            "iteration: 265890 loss: 0.0029 lr: 0.02\n",
            "iteration: 265900 loss: 0.0033 lr: 0.02\n",
            "iteration: 265910 loss: 0.0039 lr: 0.02\n",
            "iteration: 265920 loss: 0.0048 lr: 0.02\n",
            "iteration: 265930 loss: 0.0029 lr: 0.02\n",
            "iteration: 265940 loss: 0.0038 lr: 0.02\n",
            "iteration: 265950 loss: 0.0033 lr: 0.02\n",
            "iteration: 265960 loss: 0.0030 lr: 0.02\n",
            "iteration: 265970 loss: 0.0046 lr: 0.02\n",
            "iteration: 265980 loss: 0.0034 lr: 0.02\n",
            "iteration: 265990 loss: 0.0039 lr: 0.02\n",
            "iteration: 266000 loss: 0.0031 lr: 0.02\n",
            "iteration: 266010 loss: 0.0031 lr: 0.02\n",
            "iteration: 266020 loss: 0.0039 lr: 0.02\n",
            "iteration: 266030 loss: 0.0039 lr: 0.02\n",
            "iteration: 266040 loss: 0.0039 lr: 0.02\n",
            "iteration: 266050 loss: 0.0047 lr: 0.02\n",
            "iteration: 266060 loss: 0.0032 lr: 0.02\n",
            "iteration: 266070 loss: 0.0043 lr: 0.02\n",
            "iteration: 266080 loss: 0.0041 lr: 0.02\n",
            "iteration: 266090 loss: 0.0043 lr: 0.02\n",
            "iteration: 266100 loss: 0.0043 lr: 0.02\n",
            "iteration: 266110 loss: 0.0032 lr: 0.02\n",
            "iteration: 266120 loss: 0.0045 lr: 0.02\n",
            "iteration: 266130 loss: 0.0040 lr: 0.02\n",
            "iteration: 266140 loss: 0.0041 lr: 0.02\n",
            "iteration: 266150 loss: 0.0031 lr: 0.02\n",
            "iteration: 266160 loss: 0.0035 lr: 0.02\n",
            "iteration: 266170 loss: 0.0040 lr: 0.02\n",
            "iteration: 266180 loss: 0.0040 lr: 0.02\n",
            "iteration: 266190 loss: 0.0036 lr: 0.02\n",
            "iteration: 266200 loss: 0.0042 lr: 0.02\n",
            "iteration: 266210 loss: 0.0050 lr: 0.02\n",
            "iteration: 266220 loss: 0.0038 lr: 0.02\n",
            "iteration: 266230 loss: 0.0042 lr: 0.02\n",
            "iteration: 266240 loss: 0.0041 lr: 0.02\n",
            "iteration: 266250 loss: 0.0039 lr: 0.02\n",
            "iteration: 266260 loss: 0.0034 lr: 0.02\n",
            "iteration: 266270 loss: 0.0039 lr: 0.02\n",
            "iteration: 266280 loss: 0.0031 lr: 0.02\n",
            "iteration: 266290 loss: 0.0037 lr: 0.02\n",
            "iteration: 266300 loss: 0.0037 lr: 0.02\n",
            "iteration: 266310 loss: 0.0047 lr: 0.02\n",
            "iteration: 266320 loss: 0.0042 lr: 0.02\n",
            "iteration: 266330 loss: 0.0033 lr: 0.02\n",
            "iteration: 266340 loss: 0.0038 lr: 0.02\n",
            "iteration: 266350 loss: 0.0034 lr: 0.02\n",
            "iteration: 266360 loss: 0.0040 lr: 0.02\n",
            "iteration: 266370 loss: 0.0034 lr: 0.02\n",
            "iteration: 266380 loss: 0.0041 lr: 0.02\n",
            "iteration: 266390 loss: 0.0039 lr: 0.02\n",
            "iteration: 266400 loss: 0.0037 lr: 0.02\n",
            "iteration: 266410 loss: 0.0038 lr: 0.02\n",
            "iteration: 266420 loss: 0.0038 lr: 0.02\n",
            "iteration: 266430 loss: 0.0042 lr: 0.02\n",
            "iteration: 266440 loss: 0.0040 lr: 0.02\n",
            "iteration: 266450 loss: 0.0043 lr: 0.02\n",
            "iteration: 266460 loss: 0.0040 lr: 0.02\n",
            "iteration: 266470 loss: 0.0045 lr: 0.02\n",
            "iteration: 266480 loss: 0.0030 lr: 0.02\n",
            "iteration: 266490 loss: 0.0040 lr: 0.02\n",
            "iteration: 266500 loss: 0.0041 lr: 0.02\n",
            "iteration: 266510 loss: 0.0040 lr: 0.02\n",
            "iteration: 266520 loss: 0.0034 lr: 0.02\n",
            "iteration: 266530 loss: 0.0034 lr: 0.02\n",
            "iteration: 266540 loss: 0.0032 lr: 0.02\n",
            "iteration: 266550 loss: 0.0035 lr: 0.02\n",
            "iteration: 266560 loss: 0.0032 lr: 0.02\n",
            "iteration: 266570 loss: 0.0038 lr: 0.02\n",
            "iteration: 266580 loss: 0.0031 lr: 0.02\n",
            "iteration: 266590 loss: 0.0032 lr: 0.02\n",
            "iteration: 266600 loss: 0.0031 lr: 0.02\n",
            "iteration: 266610 loss: 0.0036 lr: 0.02\n",
            "iteration: 266620 loss: 0.0034 lr: 0.02\n",
            "iteration: 266630 loss: 0.0034 lr: 0.02\n",
            "iteration: 266640 loss: 0.0034 lr: 0.02\n",
            "iteration: 266650 loss: 0.0030 lr: 0.02\n",
            "iteration: 266660 loss: 0.0040 lr: 0.02\n",
            "iteration: 266670 loss: 0.0034 lr: 0.02\n",
            "iteration: 266680 loss: 0.0034 lr: 0.02\n",
            "iteration: 266690 loss: 0.0059 lr: 0.02\n",
            "iteration: 266700 loss: 0.0038 lr: 0.02\n",
            "iteration: 266710 loss: 0.0031 lr: 0.02\n",
            "iteration: 266720 loss: 0.0030 lr: 0.02\n",
            "iteration: 266730 loss: 0.0034 lr: 0.02\n",
            "iteration: 266740 loss: 0.0042 lr: 0.02\n",
            "iteration: 266750 loss: 0.0038 lr: 0.02\n",
            "iteration: 266760 loss: 0.0039 lr: 0.02\n",
            "iteration: 266770 loss: 0.0029 lr: 0.02\n",
            "iteration: 266780 loss: 0.0034 lr: 0.02\n",
            "iteration: 266790 loss: 0.0036 lr: 0.02\n",
            "iteration: 266800 loss: 0.0055 lr: 0.02\n",
            "iteration: 266810 loss: 0.0052 lr: 0.02\n",
            "iteration: 266820 loss: 0.0038 lr: 0.02\n",
            "iteration: 266830 loss: 0.0036 lr: 0.02\n",
            "iteration: 266840 loss: 0.0043 lr: 0.02\n",
            "iteration: 266850 loss: 0.0037 lr: 0.02\n",
            "iteration: 266860 loss: 0.0040 lr: 0.02\n",
            "iteration: 266870 loss: 0.0039 lr: 0.02\n",
            "iteration: 266880 loss: 0.0033 lr: 0.02\n",
            "iteration: 266890 loss: 0.0050 lr: 0.02\n",
            "iteration: 266900 loss: 0.0035 lr: 0.02\n",
            "iteration: 266910 loss: 0.0037 lr: 0.02\n",
            "iteration: 266920 loss: 0.0040 lr: 0.02\n",
            "iteration: 266930 loss: 0.0029 lr: 0.02\n",
            "iteration: 266940 loss: 0.0036 lr: 0.02\n",
            "iteration: 266950 loss: 0.0038 lr: 0.02\n",
            "iteration: 266960 loss: 0.0029 lr: 0.02\n",
            "iteration: 266970 loss: 0.0037 lr: 0.02\n",
            "iteration: 266980 loss: 0.0033 lr: 0.02\n",
            "iteration: 266990 loss: 0.0033 lr: 0.02\n",
            "iteration: 267000 loss: 0.0043 lr: 0.02\n",
            "iteration: 267010 loss: 0.0036 lr: 0.02\n",
            "iteration: 267020 loss: 0.0043 lr: 0.02\n",
            "iteration: 267030 loss: 0.0030 lr: 0.02\n",
            "iteration: 267040 loss: 0.0043 lr: 0.02\n",
            "iteration: 267050 loss: 0.0030 lr: 0.02\n",
            "iteration: 267060 loss: 0.0045 lr: 0.02\n",
            "iteration: 267070 loss: 0.0046 lr: 0.02\n",
            "iteration: 267080 loss: 0.0039 lr: 0.02\n",
            "iteration: 267090 loss: 0.0034 lr: 0.02\n",
            "iteration: 267100 loss: 0.0051 lr: 0.02\n",
            "iteration: 267110 loss: 0.0040 lr: 0.02\n",
            "iteration: 267120 loss: 0.0043 lr: 0.02\n",
            "iteration: 267130 loss: 0.0040 lr: 0.02\n",
            "iteration: 267140 loss: 0.0039 lr: 0.02\n",
            "iteration: 267150 loss: 0.0027 lr: 0.02\n",
            "iteration: 267160 loss: 0.0029 lr: 0.02\n",
            "iteration: 267170 loss: 0.0040 lr: 0.02\n",
            "iteration: 267180 loss: 0.0050 lr: 0.02\n",
            "iteration: 267190 loss: 0.0038 lr: 0.02\n",
            "iteration: 267200 loss: 0.0040 lr: 0.02\n",
            "iteration: 267210 loss: 0.0037 lr: 0.02\n",
            "iteration: 267220 loss: 0.0048 lr: 0.02\n",
            "iteration: 267230 loss: 0.0043 lr: 0.02\n",
            "iteration: 267240 loss: 0.0035 lr: 0.02\n",
            "iteration: 267250 loss: 0.0041 lr: 0.02\n",
            "iteration: 267260 loss: 0.0040 lr: 0.02\n",
            "iteration: 267270 loss: 0.0032 lr: 0.02\n",
            "iteration: 267280 loss: 0.0029 lr: 0.02\n",
            "iteration: 267290 loss: 0.0039 lr: 0.02\n",
            "iteration: 267300 loss: 0.0040 lr: 0.02\n",
            "iteration: 267310 loss: 0.0044 lr: 0.02\n",
            "iteration: 267320 loss: 0.0030 lr: 0.02\n",
            "iteration: 267330 loss: 0.0043 lr: 0.02\n",
            "iteration: 267340 loss: 0.0032 lr: 0.02\n",
            "iteration: 267350 loss: 0.0037 lr: 0.02\n",
            "iteration: 267360 loss: 0.0038 lr: 0.02\n",
            "iteration: 267370 loss: 0.0040 lr: 0.02\n",
            "iteration: 267380 loss: 0.0030 lr: 0.02\n",
            "iteration: 267390 loss: 0.0040 lr: 0.02\n",
            "iteration: 267400 loss: 0.0051 lr: 0.02\n",
            "iteration: 267410 loss: 0.0032 lr: 0.02\n",
            "iteration: 267420 loss: 0.0028 lr: 0.02\n",
            "iteration: 267430 loss: 0.0041 lr: 0.02\n",
            "iteration: 267440 loss: 0.0049 lr: 0.02\n",
            "iteration: 267450 loss: 0.0042 lr: 0.02\n",
            "iteration: 267460 loss: 0.0043 lr: 0.02\n",
            "iteration: 267470 loss: 0.0036 lr: 0.02\n",
            "iteration: 267480 loss: 0.0045 lr: 0.02\n",
            "iteration: 267490 loss: 0.0033 lr: 0.02\n",
            "iteration: 267500 loss: 0.0027 lr: 0.02\n",
            "iteration: 267510 loss: 0.0028 lr: 0.02\n",
            "iteration: 267520 loss: 0.0031 lr: 0.02\n",
            "iteration: 267530 loss: 0.0036 lr: 0.02\n",
            "iteration: 267540 loss: 0.0052 lr: 0.02\n",
            "iteration: 267550 loss: 0.0031 lr: 0.02\n",
            "iteration: 267560 loss: 0.0034 lr: 0.02\n",
            "iteration: 267570 loss: 0.0037 lr: 0.02\n",
            "iteration: 267580 loss: 0.0035 lr: 0.02\n",
            "iteration: 267590 loss: 0.0037 lr: 0.02\n",
            "iteration: 267600 loss: 0.0040 lr: 0.02\n",
            "iteration: 267610 loss: 0.0029 lr: 0.02\n",
            "iteration: 267620 loss: 0.0037 lr: 0.02\n",
            "iteration: 267630 loss: 0.0037 lr: 0.02\n",
            "iteration: 267640 loss: 0.0034 lr: 0.02\n",
            "iteration: 267650 loss: 0.0053 lr: 0.02\n",
            "iteration: 267660 loss: 0.0042 lr: 0.02\n",
            "iteration: 267670 loss: 0.0046 lr: 0.02\n",
            "iteration: 267680 loss: 0.0034 lr: 0.02\n",
            "iteration: 267690 loss: 0.0038 lr: 0.02\n",
            "iteration: 267700 loss: 0.0029 lr: 0.02\n",
            "iteration: 267710 loss: 0.0044 lr: 0.02\n",
            "iteration: 267720 loss: 0.0040 lr: 0.02\n",
            "iteration: 267730 loss: 0.0027 lr: 0.02\n",
            "iteration: 267740 loss: 0.0035 lr: 0.02\n",
            "iteration: 267750 loss: 0.0031 lr: 0.02\n",
            "iteration: 267760 loss: 0.0039 lr: 0.02\n",
            "iteration: 267770 loss: 0.0038 lr: 0.02\n",
            "iteration: 267780 loss: 0.0034 lr: 0.02\n",
            "iteration: 267790 loss: 0.0030 lr: 0.02\n",
            "iteration: 267800 loss: 0.0041 lr: 0.02\n",
            "iteration: 267810 loss: 0.0040 lr: 0.02\n",
            "iteration: 267820 loss: 0.0038 lr: 0.02\n",
            "iteration: 267830 loss: 0.0036 lr: 0.02\n",
            "iteration: 267840 loss: 0.0038 lr: 0.02\n",
            "iteration: 267850 loss: 0.0045 lr: 0.02\n",
            "iteration: 267860 loss: 0.0034 lr: 0.02\n",
            "iteration: 267870 loss: 0.0035 lr: 0.02\n",
            "iteration: 267880 loss: 0.0044 lr: 0.02\n",
            "iteration: 267890 loss: 0.0033 lr: 0.02\n",
            "iteration: 267900 loss: 0.0038 lr: 0.02\n",
            "iteration: 267910 loss: 0.0042 lr: 0.02\n",
            "iteration: 267920 loss: 0.0037 lr: 0.02\n",
            "iteration: 267930 loss: 0.0027 lr: 0.02\n",
            "iteration: 267940 loss: 0.0031 lr: 0.02\n",
            "iteration: 267950 loss: 0.0042 lr: 0.02\n",
            "iteration: 267960 loss: 0.0024 lr: 0.02\n",
            "iteration: 267970 loss: 0.0035 lr: 0.02\n",
            "iteration: 267980 loss: 0.0031 lr: 0.02\n",
            "iteration: 267990 loss: 0.0037 lr: 0.02\n",
            "iteration: 268000 loss: 0.0043 lr: 0.02\n",
            "iteration: 268010 loss: 0.0045 lr: 0.02\n",
            "iteration: 268020 loss: 0.0041 lr: 0.02\n",
            "iteration: 268030 loss: 0.0030 lr: 0.02\n",
            "iteration: 268040 loss: 0.0037 lr: 0.02\n",
            "iteration: 268050 loss: 0.0030 lr: 0.02\n",
            "iteration: 268060 loss: 0.0036 lr: 0.02\n",
            "iteration: 268070 loss: 0.0040 lr: 0.02\n",
            "iteration: 268080 loss: 0.0033 lr: 0.02\n",
            "iteration: 268090 loss: 0.0050 lr: 0.02\n",
            "iteration: 268100 loss: 0.0049 lr: 0.02\n",
            "iteration: 268110 loss: 0.0027 lr: 0.02\n",
            "iteration: 268120 loss: 0.0039 lr: 0.02\n",
            "iteration: 268130 loss: 0.0028 lr: 0.02\n",
            "iteration: 268140 loss: 0.0035 lr: 0.02\n",
            "iteration: 268150 loss: 0.0031 lr: 0.02\n",
            "iteration: 268160 loss: 0.0051 lr: 0.02\n",
            "iteration: 268170 loss: 0.0035 lr: 0.02\n",
            "iteration: 268180 loss: 0.0039 lr: 0.02\n",
            "iteration: 268190 loss: 0.0036 lr: 0.02\n",
            "iteration: 268200 loss: 0.0037 lr: 0.02\n",
            "iteration: 268210 loss: 0.0040 lr: 0.02\n",
            "iteration: 268220 loss: 0.0033 lr: 0.02\n",
            "iteration: 268230 loss: 0.0040 lr: 0.02\n",
            "iteration: 268240 loss: 0.0033 lr: 0.02\n",
            "iteration: 268250 loss: 0.0039 lr: 0.02\n",
            "iteration: 268260 loss: 0.0040 lr: 0.02\n",
            "iteration: 268270 loss: 0.0028 lr: 0.02\n",
            "iteration: 268280 loss: 0.0041 lr: 0.02\n",
            "iteration: 268290 loss: 0.0034 lr: 0.02\n",
            "iteration: 268300 loss: 0.0034 lr: 0.02\n",
            "iteration: 268310 loss: 0.0039 lr: 0.02\n",
            "iteration: 268320 loss: 0.0033 lr: 0.02\n",
            "iteration: 268330 loss: 0.0044 lr: 0.02\n",
            "iteration: 268340 loss: 0.0034 lr: 0.02\n",
            "iteration: 268350 loss: 0.0033 lr: 0.02\n",
            "iteration: 268360 loss: 0.0029 lr: 0.02\n",
            "iteration: 268370 loss: 0.0031 lr: 0.02\n",
            "iteration: 268380 loss: 0.0047 lr: 0.02\n",
            "iteration: 268390 loss: 0.0036 lr: 0.02\n",
            "iteration: 268400 loss: 0.0037 lr: 0.02\n",
            "iteration: 268410 loss: 0.0027 lr: 0.02\n",
            "iteration: 268420 loss: 0.0030 lr: 0.02\n",
            "iteration: 268430 loss: 0.0028 lr: 0.02\n",
            "iteration: 268440 loss: 0.0038 lr: 0.02\n",
            "iteration: 268450 loss: 0.0046 lr: 0.02\n",
            "iteration: 268460 loss: 0.0034 lr: 0.02\n",
            "iteration: 268470 loss: 0.0042 lr: 0.02\n",
            "iteration: 268480 loss: 0.0033 lr: 0.02\n",
            "iteration: 268490 loss: 0.0039 lr: 0.02\n",
            "iteration: 268500 loss: 0.0042 lr: 0.02\n",
            "iteration: 268510 loss: 0.0034 lr: 0.02\n",
            "iteration: 268520 loss: 0.0026 lr: 0.02\n",
            "iteration: 268530 loss: 0.0034 lr: 0.02\n",
            "iteration: 268540 loss: 0.0038 lr: 0.02\n",
            "iteration: 268550 loss: 0.0045 lr: 0.02\n",
            "iteration: 268560 loss: 0.0044 lr: 0.02\n",
            "iteration: 268570 loss: 0.0039 lr: 0.02\n",
            "iteration: 268580 loss: 0.0039 lr: 0.02\n",
            "iteration: 268590 loss: 0.0037 lr: 0.02\n",
            "iteration: 268600 loss: 0.0029 lr: 0.02\n",
            "iteration: 268610 loss: 0.0048 lr: 0.02\n",
            "iteration: 268620 loss: 0.0033 lr: 0.02\n",
            "iteration: 268630 loss: 0.0033 lr: 0.02\n",
            "iteration: 268640 loss: 0.0042 lr: 0.02\n",
            "iteration: 268650 loss: 0.0028 lr: 0.02\n",
            "iteration: 268660 loss: 0.0043 lr: 0.02\n",
            "iteration: 268670 loss: 0.0030 lr: 0.02\n",
            "iteration: 268680 loss: 0.0036 lr: 0.02\n",
            "iteration: 268690 loss: 0.0042 lr: 0.02\n",
            "iteration: 268700 loss: 0.0033 lr: 0.02\n",
            "iteration: 268710 loss: 0.0037 lr: 0.02\n",
            "iteration: 268720 loss: 0.0043 lr: 0.02\n",
            "iteration: 268730 loss: 0.0035 lr: 0.02\n",
            "iteration: 268740 loss: 0.0041 lr: 0.02\n",
            "iteration: 268750 loss: 0.0040 lr: 0.02\n",
            "iteration: 268760 loss: 0.0034 lr: 0.02\n",
            "iteration: 268770 loss: 0.0029 lr: 0.02\n",
            "iteration: 268780 loss: 0.0027 lr: 0.02\n",
            "iteration: 268790 loss: 0.0032 lr: 0.02\n",
            "iteration: 268800 loss: 0.0040 lr: 0.02\n",
            "iteration: 268810 loss: 0.0034 lr: 0.02\n",
            "iteration: 268820 loss: 0.0029 lr: 0.02\n",
            "iteration: 268830 loss: 0.0033 lr: 0.02\n",
            "iteration: 268840 loss: 0.0033 lr: 0.02\n",
            "iteration: 268850 loss: 0.0034 lr: 0.02\n",
            "iteration: 268860 loss: 0.0042 lr: 0.02\n",
            "iteration: 268870 loss: 0.0028 lr: 0.02\n",
            "iteration: 268880 loss: 0.0044 lr: 0.02\n",
            "iteration: 268890 loss: 0.0036 lr: 0.02\n",
            "iteration: 268900 loss: 0.0035 lr: 0.02\n",
            "iteration: 268910 loss: 0.0026 lr: 0.02\n",
            "iteration: 268920 loss: 0.0026 lr: 0.02\n",
            "iteration: 268930 loss: 0.0042 lr: 0.02\n",
            "iteration: 268940 loss: 0.0034 lr: 0.02\n",
            "iteration: 268950 loss: 0.0040 lr: 0.02\n",
            "iteration: 268960 loss: 0.0025 lr: 0.02\n",
            "iteration: 268970 loss: 0.0042 lr: 0.02\n",
            "iteration: 268980 loss: 0.0029 lr: 0.02\n",
            "iteration: 268990 loss: 0.0050 lr: 0.02\n",
            "iteration: 269000 loss: 0.0035 lr: 0.02\n",
            "iteration: 269010 loss: 0.0035 lr: 0.02\n",
            "iteration: 269020 loss: 0.0029 lr: 0.02\n",
            "iteration: 269030 loss: 0.0043 lr: 0.02\n",
            "iteration: 269040 loss: 0.0022 lr: 0.02\n",
            "iteration: 269050 loss: 0.0042 lr: 0.02\n",
            "iteration: 269060 loss: 0.0043 lr: 0.02\n",
            "iteration: 269070 loss: 0.0040 lr: 0.02\n",
            "iteration: 269080 loss: 0.0031 lr: 0.02\n",
            "iteration: 269090 loss: 0.0028 lr: 0.02\n",
            "iteration: 269100 loss: 0.0036 lr: 0.02\n",
            "iteration: 269110 loss: 0.0034 lr: 0.02\n",
            "iteration: 269120 loss: 0.0027 lr: 0.02\n",
            "iteration: 269130 loss: 0.0038 lr: 0.02\n",
            "iteration: 269140 loss: 0.0036 lr: 0.02\n",
            "iteration: 269150 loss: 0.0031 lr: 0.02\n",
            "iteration: 269160 loss: 0.0035 lr: 0.02\n",
            "iteration: 269170 loss: 0.0029 lr: 0.02\n",
            "iteration: 269180 loss: 0.0036 lr: 0.02\n",
            "iteration: 269190 loss: 0.0038 lr: 0.02\n",
            "iteration: 269200 loss: 0.0028 lr: 0.02\n",
            "iteration: 269210 loss: 0.0026 lr: 0.02\n",
            "iteration: 269220 loss: 0.0033 lr: 0.02\n",
            "iteration: 269230 loss: 0.0028 lr: 0.02\n",
            "iteration: 269240 loss: 0.0037 lr: 0.02\n",
            "iteration: 269250 loss: 0.0039 lr: 0.02\n",
            "iteration: 269260 loss: 0.0034 lr: 0.02\n",
            "iteration: 269270 loss: 0.0030 lr: 0.02\n",
            "iteration: 269280 loss: 0.0038 lr: 0.02\n",
            "iteration: 269290 loss: 0.0043 lr: 0.02\n",
            "iteration: 269300 loss: 0.0033 lr: 0.02\n",
            "iteration: 269310 loss: 0.0055 lr: 0.02\n",
            "iteration: 269320 loss: 0.0040 lr: 0.02\n",
            "iteration: 269330 loss: 0.0049 lr: 0.02\n",
            "iteration: 269340 loss: 0.0043 lr: 0.02\n",
            "iteration: 269350 loss: 0.0037 lr: 0.02\n",
            "iteration: 269360 loss: 0.0039 lr: 0.02\n",
            "iteration: 269370 loss: 0.0028 lr: 0.02\n",
            "iteration: 269380 loss: 0.0033 lr: 0.02\n",
            "iteration: 269390 loss: 0.0034 lr: 0.02\n",
            "iteration: 269400 loss: 0.0036 lr: 0.02\n",
            "iteration: 269410 loss: 0.0049 lr: 0.02\n",
            "iteration: 269420 loss: 0.0048 lr: 0.02\n",
            "iteration: 269430 loss: 0.0032 lr: 0.02\n",
            "iteration: 269440 loss: 0.0034 lr: 0.02\n",
            "iteration: 269450 loss: 0.0042 lr: 0.02\n",
            "iteration: 269460 loss: 0.0024 lr: 0.02\n",
            "iteration: 269470 loss: 0.0039 lr: 0.02\n",
            "iteration: 269480 loss: 0.0043 lr: 0.02\n",
            "iteration: 269490 loss: 0.0030 lr: 0.02\n",
            "iteration: 269500 loss: 0.0035 lr: 0.02\n",
            "iteration: 269510 loss: 0.0032 lr: 0.02\n",
            "iteration: 269520 loss: 0.0036 lr: 0.02\n",
            "iteration: 269530 loss: 0.0039 lr: 0.02\n",
            "iteration: 269540 loss: 0.0025 lr: 0.02\n",
            "iteration: 269550 loss: 0.0027 lr: 0.02\n",
            "iteration: 269560 loss: 0.0028 lr: 0.02\n",
            "iteration: 269570 loss: 0.0039 lr: 0.02\n",
            "iteration: 269580 loss: 0.0032 lr: 0.02\n",
            "iteration: 269590 loss: 0.0048 lr: 0.02\n",
            "iteration: 269600 loss: 0.0038 lr: 0.02\n",
            "iteration: 269610 loss: 0.0046 lr: 0.02\n",
            "iteration: 269620 loss: 0.0034 lr: 0.02\n",
            "iteration: 269630 loss: 0.0038 lr: 0.02\n",
            "iteration: 269640 loss: 0.0038 lr: 0.02\n",
            "iteration: 269650 loss: 0.0037 lr: 0.02\n",
            "iteration: 269660 loss: 0.0036 lr: 0.02\n",
            "iteration: 269670 loss: 0.0030 lr: 0.02\n",
            "iteration: 269680 loss: 0.0034 lr: 0.02\n",
            "iteration: 269690 loss: 0.0053 lr: 0.02\n",
            "iteration: 269700 loss: 0.0044 lr: 0.02\n",
            "iteration: 269710 loss: 0.0037 lr: 0.02\n",
            "iteration: 269720 loss: 0.0043 lr: 0.02\n",
            "iteration: 269730 loss: 0.0050 lr: 0.02\n",
            "iteration: 269740 loss: 0.0038 lr: 0.02\n",
            "iteration: 269750 loss: 0.0044 lr: 0.02\n",
            "iteration: 269760 loss: 0.0037 lr: 0.02\n",
            "iteration: 269770 loss: 0.0031 lr: 0.02\n",
            "iteration: 269780 loss: 0.0049 lr: 0.02\n",
            "iteration: 269790 loss: 0.0045 lr: 0.02\n",
            "iteration: 269800 loss: 0.0037 lr: 0.02\n",
            "iteration: 269810 loss: 0.0034 lr: 0.02\n",
            "iteration: 269820 loss: 0.0047 lr: 0.02\n",
            "iteration: 269830 loss: 0.0034 lr: 0.02\n",
            "iteration: 269840 loss: 0.0045 lr: 0.02\n",
            "iteration: 269850 loss: 0.0041 lr: 0.02\n",
            "iteration: 269860 loss: 0.0026 lr: 0.02\n",
            "iteration: 269870 loss: 0.0038 lr: 0.02\n",
            "iteration: 269880 loss: 0.0028 lr: 0.02\n",
            "iteration: 269890 loss: 0.0032 lr: 0.02\n",
            "iteration: 269900 loss: 0.0028 lr: 0.02\n",
            "iteration: 269910 loss: 0.0044 lr: 0.02\n",
            "iteration: 269920 loss: 0.0051 lr: 0.02\n",
            "iteration: 269930 loss: 0.0034 lr: 0.02\n",
            "iteration: 269940 loss: 0.0039 lr: 0.02\n",
            "iteration: 269950 loss: 0.0034 lr: 0.02\n",
            "iteration: 269960 loss: 0.0035 lr: 0.02\n",
            "iteration: 269970 loss: 0.0033 lr: 0.02\n",
            "iteration: 269980 loss: 0.0043 lr: 0.02\n",
            "iteration: 269990 loss: 0.0045 lr: 0.02\n",
            "iteration: 270000 loss: 0.0028 lr: 0.02\n",
            "iteration: 270010 loss: 0.0044 lr: 0.02\n",
            "iteration: 270020 loss: 0.0027 lr: 0.02\n",
            "iteration: 270030 loss: 0.0043 lr: 0.02\n",
            "iteration: 270040 loss: 0.0029 lr: 0.02\n",
            "iteration: 270050 loss: 0.0041 lr: 0.02\n",
            "iteration: 270060 loss: 0.0033 lr: 0.02\n",
            "iteration: 270070 loss: 0.0038 lr: 0.02\n",
            "iteration: 270080 loss: 0.0035 lr: 0.02\n",
            "iteration: 270090 loss: 0.0039 lr: 0.02\n",
            "iteration: 270100 loss: 0.0033 lr: 0.02\n",
            "iteration: 270110 loss: 0.0034 lr: 0.02\n",
            "iteration: 270120 loss: 0.0031 lr: 0.02\n",
            "iteration: 270130 loss: 0.0049 lr: 0.02\n",
            "iteration: 270140 loss: 0.0040 lr: 0.02\n",
            "iteration: 270150 loss: 0.0035 lr: 0.02\n",
            "iteration: 270160 loss: 0.0030 lr: 0.02\n",
            "iteration: 270170 loss: 0.0029 lr: 0.02\n",
            "iteration: 270180 loss: 0.0036 lr: 0.02\n",
            "iteration: 270190 loss: 0.0032 lr: 0.02\n",
            "iteration: 270200 loss: 0.0040 lr: 0.02\n",
            "iteration: 270210 loss: 0.0036 lr: 0.02\n",
            "iteration: 270220 loss: 0.0036 lr: 0.02\n",
            "iteration: 270230 loss: 0.0041 lr: 0.02\n",
            "iteration: 270240 loss: 0.0034 lr: 0.02\n",
            "iteration: 270250 loss: 0.0034 lr: 0.02\n",
            "iteration: 270260 loss: 0.0033 lr: 0.02\n",
            "iteration: 270270 loss: 0.0041 lr: 0.02\n",
            "iteration: 270280 loss: 0.0032 lr: 0.02\n",
            "iteration: 270290 loss: 0.0045 lr: 0.02\n",
            "iteration: 270300 loss: 0.0043 lr: 0.02\n",
            "iteration: 270310 loss: 0.0030 lr: 0.02\n",
            "iteration: 270320 loss: 0.0036 lr: 0.02\n",
            "iteration: 270330 loss: 0.0031 lr: 0.02\n",
            "iteration: 270340 loss: 0.0037 lr: 0.02\n",
            "iteration: 270350 loss: 0.0045 lr: 0.02\n",
            "iteration: 270360 loss: 0.0037 lr: 0.02\n",
            "iteration: 270370 loss: 0.0033 lr: 0.02\n",
            "iteration: 270380 loss: 0.0032 lr: 0.02\n",
            "iteration: 270390 loss: 0.0029 lr: 0.02\n",
            "iteration: 270400 loss: 0.0033 lr: 0.02\n",
            "iteration: 270410 loss: 0.0041 lr: 0.02\n",
            "iteration: 270420 loss: 0.0039 lr: 0.02\n",
            "iteration: 270430 loss: 0.0028 lr: 0.02\n",
            "iteration: 270440 loss: 0.0036 lr: 0.02\n",
            "iteration: 270450 loss: 0.0028 lr: 0.02\n",
            "iteration: 270460 loss: 0.0045 lr: 0.02\n",
            "iteration: 270470 loss: 0.0034 lr: 0.02\n",
            "iteration: 270480 loss: 0.0029 lr: 0.02\n",
            "iteration: 270490 loss: 0.0033 lr: 0.02\n",
            "iteration: 270500 loss: 0.0035 lr: 0.02\n",
            "iteration: 270510 loss: 0.0037 lr: 0.02\n",
            "iteration: 270520 loss: 0.0031 lr: 0.02\n",
            "iteration: 270530 loss: 0.0034 lr: 0.02\n",
            "iteration: 270540 loss: 0.0045 lr: 0.02\n",
            "iteration: 270550 loss: 0.0044 lr: 0.02\n",
            "iteration: 270560 loss: 0.0035 lr: 0.02\n",
            "iteration: 270570 loss: 0.0036 lr: 0.02\n",
            "iteration: 270580 loss: 0.0034 lr: 0.02\n",
            "iteration: 270590 loss: 0.0037 lr: 0.02\n",
            "iteration: 270600 loss: 0.0037 lr: 0.02\n",
            "iteration: 270610 loss: 0.0036 lr: 0.02\n",
            "iteration: 270620 loss: 0.0034 lr: 0.02\n",
            "iteration: 270630 loss: 0.0037 lr: 0.02\n",
            "iteration: 270640 loss: 0.0039 lr: 0.02\n",
            "iteration: 270650 loss: 0.0034 lr: 0.02\n",
            "iteration: 270660 loss: 0.0048 lr: 0.02\n",
            "iteration: 270670 loss: 0.0033 lr: 0.02\n",
            "iteration: 270680 loss: 0.0040 lr: 0.02\n",
            "iteration: 270690 loss: 0.0029 lr: 0.02\n",
            "iteration: 270700 loss: 0.0035 lr: 0.02\n",
            "iteration: 270710 loss: 0.0028 lr: 0.02\n",
            "iteration: 270720 loss: 0.0048 lr: 0.02\n",
            "iteration: 270730 loss: 0.0033 lr: 0.02\n",
            "iteration: 270740 loss: 0.0032 lr: 0.02\n",
            "iteration: 270750 loss: 0.0030 lr: 0.02\n",
            "iteration: 270760 loss: 0.0042 lr: 0.02\n",
            "iteration: 270770 loss: 0.0037 lr: 0.02\n",
            "iteration: 270780 loss: 0.0040 lr: 0.02\n",
            "iteration: 270790 loss: 0.0031 lr: 0.02\n",
            "iteration: 270800 loss: 0.0040 lr: 0.02\n",
            "iteration: 270810 loss: 0.0037 lr: 0.02\n",
            "iteration: 270820 loss: 0.0028 lr: 0.02\n",
            "iteration: 270830 loss: 0.0024 lr: 0.02\n",
            "iteration: 270840 loss: 0.0036 lr: 0.02\n",
            "iteration: 270850 loss: 0.0026 lr: 0.02\n",
            "iteration: 270860 loss: 0.0029 lr: 0.02\n",
            "iteration: 270870 loss: 0.0034 lr: 0.02\n",
            "iteration: 270880 loss: 0.0048 lr: 0.02\n",
            "iteration: 270890 loss: 0.0036 lr: 0.02\n",
            "iteration: 270900 loss: 0.0036 lr: 0.02\n",
            "iteration: 270910 loss: 0.0034 lr: 0.02\n",
            "iteration: 270920 loss: 0.0036 lr: 0.02\n",
            "iteration: 270930 loss: 0.0029 lr: 0.02\n",
            "iteration: 270940 loss: 0.0035 lr: 0.02\n",
            "iteration: 270950 loss: 0.0037 lr: 0.02\n",
            "iteration: 270960 loss: 0.0030 lr: 0.02\n",
            "iteration: 270970 loss: 0.0033 lr: 0.02\n",
            "iteration: 270980 loss: 0.0040 lr: 0.02\n",
            "iteration: 270990 loss: 0.0027 lr: 0.02\n",
            "iteration: 271000 loss: 0.0039 lr: 0.02\n",
            "iteration: 271010 loss: 0.0039 lr: 0.02\n",
            "iteration: 271020 loss: 0.0035 lr: 0.02\n",
            "iteration: 271030 loss: 0.0048 lr: 0.02\n",
            "iteration: 271040 loss: 0.0032 lr: 0.02\n",
            "iteration: 271050 loss: 0.0040 lr: 0.02\n",
            "iteration: 271060 loss: 0.0040 lr: 0.02\n",
            "iteration: 271070 loss: 0.0030 lr: 0.02\n",
            "iteration: 271080 loss: 0.0037 lr: 0.02\n",
            "iteration: 271090 loss: 0.0046 lr: 0.02\n",
            "iteration: 271100 loss: 0.0040 lr: 0.02\n",
            "iteration: 271110 loss: 0.0033 lr: 0.02\n",
            "iteration: 271120 loss: 0.0043 lr: 0.02\n",
            "iteration: 271130 loss: 0.0032 lr: 0.02\n",
            "iteration: 271140 loss: 0.0036 lr: 0.02\n",
            "iteration: 271150 loss: 0.0041 lr: 0.02\n",
            "iteration: 271160 loss: 0.0054 lr: 0.02\n",
            "iteration: 271170 loss: 0.0043 lr: 0.02\n",
            "iteration: 271180 loss: 0.0036 lr: 0.02\n",
            "iteration: 271190 loss: 0.0031 lr: 0.02\n",
            "iteration: 271200 loss: 0.0059 lr: 0.02\n",
            "iteration: 271210 loss: 0.0033 lr: 0.02\n",
            "iteration: 271220 loss: 0.0038 lr: 0.02\n",
            "iteration: 271230 loss: 0.0047 lr: 0.02\n",
            "iteration: 271240 loss: 0.0031 lr: 0.02\n",
            "iteration: 271250 loss: 0.0040 lr: 0.02\n",
            "iteration: 271260 loss: 0.0038 lr: 0.02\n",
            "iteration: 271270 loss: 0.0036 lr: 0.02\n",
            "iteration: 271280 loss: 0.0032 lr: 0.02\n",
            "iteration: 271290 loss: 0.0057 lr: 0.02\n",
            "iteration: 271300 loss: 0.0036 lr: 0.02\n",
            "iteration: 271310 loss: 0.0043 lr: 0.02\n",
            "iteration: 271320 loss: 0.0029 lr: 0.02\n",
            "iteration: 271330 loss: 0.0032 lr: 0.02\n",
            "iteration: 271340 loss: 0.0045 lr: 0.02\n",
            "iteration: 271350 loss: 0.0049 lr: 0.02\n",
            "iteration: 271360 loss: 0.0027 lr: 0.02\n",
            "iteration: 271370 loss: 0.0041 lr: 0.02\n",
            "iteration: 271380 loss: 0.0042 lr: 0.02\n",
            "iteration: 271390 loss: 0.0030 lr: 0.02\n",
            "iteration: 271400 loss: 0.0036 lr: 0.02\n",
            "iteration: 271410 loss: 0.0041 lr: 0.02\n",
            "iteration: 271420 loss: 0.0046 lr: 0.02\n",
            "iteration: 271430 loss: 0.0037 lr: 0.02\n",
            "iteration: 271440 loss: 0.0045 lr: 0.02\n",
            "iteration: 271450 loss: 0.0030 lr: 0.02\n",
            "iteration: 271460 loss: 0.0043 lr: 0.02\n",
            "iteration: 271470 loss: 0.0029 lr: 0.02\n",
            "iteration: 271480 loss: 0.0034 lr: 0.02\n",
            "iteration: 271490 loss: 0.0037 lr: 0.02\n",
            "iteration: 271500 loss: 0.0040 lr: 0.02\n",
            "iteration: 271510 loss: 0.0031 lr: 0.02\n",
            "iteration: 271520 loss: 0.0040 lr: 0.02\n",
            "iteration: 271530 loss: 0.0036 lr: 0.02\n",
            "iteration: 271540 loss: 0.0037 lr: 0.02\n",
            "iteration: 271550 loss: 0.0029 lr: 0.02\n",
            "iteration: 271560 loss: 0.0031 lr: 0.02\n",
            "iteration: 271570 loss: 0.0051 lr: 0.02\n",
            "iteration: 271580 loss: 0.0025 lr: 0.02\n",
            "iteration: 271590 loss: 0.0029 lr: 0.02\n",
            "iteration: 271600 loss: 0.0046 lr: 0.02\n",
            "iteration: 271610 loss: 0.0027 lr: 0.02\n",
            "iteration: 271620 loss: 0.0038 lr: 0.02\n",
            "iteration: 271630 loss: 0.0037 lr: 0.02\n",
            "iteration: 271640 loss: 0.0024 lr: 0.02\n",
            "iteration: 271650 loss: 0.0038 lr: 0.02\n",
            "iteration: 271660 loss: 0.0040 lr: 0.02\n",
            "iteration: 271670 loss: 0.0047 lr: 0.02\n",
            "iteration: 271680 loss: 0.0040 lr: 0.02\n",
            "iteration: 271690 loss: 0.0036 lr: 0.02\n",
            "iteration: 271700 loss: 0.0028 lr: 0.02\n",
            "iteration: 271710 loss: 0.0042 lr: 0.02\n",
            "iteration: 271720 loss: 0.0039 lr: 0.02\n",
            "iteration: 271730 loss: 0.0039 lr: 0.02\n",
            "iteration: 271740 loss: 0.0036 lr: 0.02\n",
            "iteration: 271750 loss: 0.0035 lr: 0.02\n",
            "iteration: 271760 loss: 0.0049 lr: 0.02\n",
            "iteration: 271770 loss: 0.0024 lr: 0.02\n",
            "iteration: 271780 loss: 0.0033 lr: 0.02\n",
            "iteration: 271790 loss: 0.0029 lr: 0.02\n",
            "iteration: 271800 loss: 0.0042 lr: 0.02\n",
            "iteration: 271810 loss: 0.0044 lr: 0.02\n",
            "iteration: 271820 loss: 0.0038 lr: 0.02\n",
            "iteration: 271830 loss: 0.0043 lr: 0.02\n",
            "iteration: 271840 loss: 0.0044 lr: 0.02\n",
            "iteration: 271850 loss: 0.0038 lr: 0.02\n",
            "iteration: 271860 loss: 0.0039 lr: 0.02\n",
            "iteration: 271870 loss: 0.0036 lr: 0.02\n",
            "iteration: 271880 loss: 0.0038 lr: 0.02\n",
            "iteration: 271890 loss: 0.0036 lr: 0.02\n",
            "iteration: 271900 loss: 0.0036 lr: 0.02\n",
            "iteration: 271910 loss: 0.0026 lr: 0.02\n",
            "iteration: 271920 loss: 0.0039 lr: 0.02\n",
            "iteration: 271930 loss: 0.0032 lr: 0.02\n",
            "iteration: 271940 loss: 0.0037 lr: 0.02\n",
            "iteration: 271950 loss: 0.0034 lr: 0.02\n",
            "iteration: 271960 loss: 0.0048 lr: 0.02\n",
            "iteration: 271970 loss: 0.0038 lr: 0.02\n",
            "iteration: 271980 loss: 0.0050 lr: 0.02\n",
            "iteration: 271990 loss: 0.0035 lr: 0.02\n",
            "iteration: 272000 loss: 0.0036 lr: 0.02\n",
            "iteration: 272010 loss: 0.0039 lr: 0.02\n",
            "iteration: 272020 loss: 0.0035 lr: 0.02\n",
            "iteration: 272030 loss: 0.0037 lr: 0.02\n",
            "iteration: 272040 loss: 0.0042 lr: 0.02\n",
            "iteration: 272050 loss: 0.0029 lr: 0.02\n",
            "iteration: 272060 loss: 0.0043 lr: 0.02\n",
            "iteration: 272070 loss: 0.0038 lr: 0.02\n",
            "iteration: 272080 loss: 0.0037 lr: 0.02\n",
            "iteration: 272090 loss: 0.0036 lr: 0.02\n",
            "iteration: 272100 loss: 0.0042 lr: 0.02\n",
            "iteration: 272110 loss: 0.0040 lr: 0.02\n",
            "iteration: 272120 loss: 0.0033 lr: 0.02\n",
            "iteration: 272130 loss: 0.0033 lr: 0.02\n",
            "iteration: 272140 loss: 0.0036 lr: 0.02\n",
            "iteration: 272150 loss: 0.0038 lr: 0.02\n",
            "iteration: 272160 loss: 0.0026 lr: 0.02\n",
            "iteration: 272170 loss: 0.0041 lr: 0.02\n",
            "iteration: 272180 loss: 0.0038 lr: 0.02\n",
            "iteration: 272190 loss: 0.0049 lr: 0.02\n",
            "iteration: 272200 loss: 0.0044 lr: 0.02\n",
            "iteration: 272210 loss: 0.0032 lr: 0.02\n",
            "iteration: 272220 loss: 0.0045 lr: 0.02\n",
            "iteration: 272230 loss: 0.0029 lr: 0.02\n",
            "iteration: 272240 loss: 0.0042 lr: 0.02\n",
            "iteration: 272250 loss: 0.0034 lr: 0.02\n",
            "iteration: 272260 loss: 0.0030 lr: 0.02\n",
            "iteration: 272270 loss: 0.0029 lr: 0.02\n",
            "iteration: 272280 loss: 0.0036 lr: 0.02\n",
            "iteration: 272290 loss: 0.0041 lr: 0.02\n",
            "iteration: 272300 loss: 0.0045 lr: 0.02\n",
            "iteration: 272310 loss: 0.0040 lr: 0.02\n",
            "iteration: 272320 loss: 0.0033 lr: 0.02\n",
            "iteration: 272330 loss: 0.0051 lr: 0.02\n",
            "iteration: 272340 loss: 0.0033 lr: 0.02\n",
            "iteration: 272350 loss: 0.0037 lr: 0.02\n",
            "iteration: 272360 loss: 0.0035 lr: 0.02\n",
            "iteration: 272370 loss: 0.0033 lr: 0.02\n",
            "iteration: 272380 loss: 0.0036 lr: 0.02\n",
            "iteration: 272390 loss: 0.0038 lr: 0.02\n",
            "iteration: 272400 loss: 0.0033 lr: 0.02\n",
            "iteration: 272410 loss: 0.0049 lr: 0.02\n",
            "iteration: 272420 loss: 0.0033 lr: 0.02\n",
            "iteration: 272430 loss: 0.0044 lr: 0.02\n",
            "iteration: 272440 loss: 0.0047 lr: 0.02\n",
            "iteration: 272450 loss: 0.0032 lr: 0.02\n",
            "iteration: 272460 loss: 0.0039 lr: 0.02\n",
            "iteration: 272470 loss: 0.0033 lr: 0.02\n",
            "iteration: 272480 loss: 0.0028 lr: 0.02\n",
            "iteration: 272490 loss: 0.0039 lr: 0.02\n",
            "iteration: 272500 loss: 0.0040 lr: 0.02\n",
            "iteration: 272510 loss: 0.0035 lr: 0.02\n",
            "iteration: 272520 loss: 0.0047 lr: 0.02\n",
            "iteration: 272530 loss: 0.0037 lr: 0.02\n",
            "iteration: 272540 loss: 0.0031 lr: 0.02\n",
            "iteration: 272550 loss: 0.0038 lr: 0.02\n",
            "iteration: 272560 loss: 0.0043 lr: 0.02\n",
            "iteration: 272570 loss: 0.0048 lr: 0.02\n",
            "iteration: 272580 loss: 0.0059 lr: 0.02\n",
            "iteration: 272590 loss: 0.0035 lr: 0.02\n",
            "iteration: 272600 loss: 0.0027 lr: 0.02\n",
            "iteration: 272610 loss: 0.0033 lr: 0.02\n",
            "iteration: 272620 loss: 0.0033 lr: 0.02\n",
            "iteration: 272630 loss: 0.0029 lr: 0.02\n",
            "iteration: 272640 loss: 0.0046 lr: 0.02\n",
            "iteration: 272650 loss: 0.0038 lr: 0.02\n",
            "iteration: 272660 loss: 0.0033 lr: 0.02\n",
            "iteration: 272670 loss: 0.0038 lr: 0.02\n",
            "iteration: 272680 loss: 0.0030 lr: 0.02\n",
            "iteration: 272690 loss: 0.0034 lr: 0.02\n",
            "iteration: 272700 loss: 0.0031 lr: 0.02\n",
            "iteration: 272710 loss: 0.0031 lr: 0.02\n",
            "iteration: 272720 loss: 0.0043 lr: 0.02\n",
            "iteration: 272730 loss: 0.0029 lr: 0.02\n",
            "iteration: 272740 loss: 0.0034 lr: 0.02\n",
            "iteration: 272750 loss: 0.0024 lr: 0.02\n",
            "iteration: 272760 loss: 0.0036 lr: 0.02\n",
            "iteration: 272770 loss: 0.0033 lr: 0.02\n",
            "iteration: 272780 loss: 0.0028 lr: 0.02\n",
            "iteration: 272790 loss: 0.0030 lr: 0.02\n",
            "iteration: 272800 loss: 0.0029 lr: 0.02\n",
            "iteration: 272810 loss: 0.0039 lr: 0.02\n",
            "iteration: 272820 loss: 0.0036 lr: 0.02\n",
            "iteration: 272830 loss: 0.0031 lr: 0.02\n",
            "iteration: 272840 loss: 0.0033 lr: 0.02\n",
            "iteration: 272850 loss: 0.0031 lr: 0.02\n",
            "iteration: 272860 loss: 0.0037 lr: 0.02\n",
            "iteration: 272870 loss: 0.0041 lr: 0.02\n",
            "iteration: 272880 loss: 0.0039 lr: 0.02\n",
            "iteration: 272890 loss: 0.0031 lr: 0.02\n",
            "iteration: 272900 loss: 0.0043 lr: 0.02\n",
            "iteration: 272910 loss: 0.0040 lr: 0.02\n",
            "iteration: 272920 loss: 0.0039 lr: 0.02\n",
            "iteration: 272930 loss: 0.0034 lr: 0.02\n",
            "iteration: 272940 loss: 0.0041 lr: 0.02\n",
            "iteration: 272950 loss: 0.0037 lr: 0.02\n",
            "iteration: 272960 loss: 0.0029 lr: 0.02\n",
            "iteration: 272970 loss: 0.0032 lr: 0.02\n",
            "iteration: 272980 loss: 0.0032 lr: 0.02\n",
            "iteration: 272990 loss: 0.0032 lr: 0.02\n",
            "iteration: 273000 loss: 0.0039 lr: 0.02\n",
            "iteration: 273010 loss: 0.0030 lr: 0.02\n",
            "iteration: 273020 loss: 0.0041 lr: 0.02\n",
            "iteration: 273030 loss: 0.0035 lr: 0.02\n",
            "iteration: 273040 loss: 0.0035 lr: 0.02\n",
            "iteration: 273050 loss: 0.0032 lr: 0.02\n",
            "iteration: 273060 loss: 0.0033 lr: 0.02\n",
            "iteration: 273070 loss: 0.0031 lr: 0.02\n",
            "iteration: 273080 loss: 0.0038 lr: 0.02\n",
            "iteration: 273090 loss: 0.0035 lr: 0.02\n",
            "iteration: 273100 loss: 0.0035 lr: 0.02\n",
            "iteration: 273110 loss: 0.0023 lr: 0.02\n",
            "iteration: 273120 loss: 0.0034 lr: 0.02\n",
            "iteration: 273130 loss: 0.0049 lr: 0.02\n",
            "iteration: 273140 loss: 0.0053 lr: 0.02\n",
            "iteration: 273150 loss: 0.0035 lr: 0.02\n",
            "iteration: 273160 loss: 0.0041 lr: 0.02\n",
            "iteration: 273170 loss: 0.0026 lr: 0.02\n",
            "iteration: 273180 loss: 0.0042 lr: 0.02\n",
            "iteration: 273190 loss: 0.0032 lr: 0.02\n",
            "iteration: 273200 loss: 0.0037 lr: 0.02\n",
            "iteration: 273210 loss: 0.0044 lr: 0.02\n",
            "iteration: 273220 loss: 0.0044 lr: 0.02\n",
            "iteration: 273230 loss: 0.0037 lr: 0.02\n",
            "iteration: 273240 loss: 0.0040 lr: 0.02\n",
            "iteration: 273250 loss: 0.0033 lr: 0.02\n",
            "iteration: 273260 loss: 0.0029 lr: 0.02\n",
            "iteration: 273270 loss: 0.0036 lr: 0.02\n",
            "iteration: 273280 loss: 0.0030 lr: 0.02\n",
            "iteration: 273290 loss: 0.0036 lr: 0.02\n",
            "iteration: 273300 loss: 0.0039 lr: 0.02\n",
            "iteration: 273310 loss: 0.0037 lr: 0.02\n",
            "iteration: 273320 loss: 0.0031 lr: 0.02\n",
            "iteration: 273330 loss: 0.0040 lr: 0.02\n",
            "iteration: 273340 loss: 0.0037 lr: 0.02\n",
            "iteration: 273350 loss: 0.0036 lr: 0.02\n",
            "iteration: 273360 loss: 0.0044 lr: 0.02\n",
            "iteration: 273370 loss: 0.0031 lr: 0.02\n",
            "iteration: 273380 loss: 0.0029 lr: 0.02\n",
            "iteration: 273390 loss: 0.0045 lr: 0.02\n",
            "iteration: 273400 loss: 0.0034 lr: 0.02\n",
            "iteration: 273410 loss: 0.0036 lr: 0.02\n",
            "iteration: 273420 loss: 0.0035 lr: 0.02\n",
            "iteration: 273430 loss: 0.0044 lr: 0.02\n",
            "iteration: 273440 loss: 0.0037 lr: 0.02\n",
            "iteration: 273450 loss: 0.0037 lr: 0.02\n",
            "iteration: 273460 loss: 0.0043 lr: 0.02\n",
            "iteration: 273470 loss: 0.0033 lr: 0.02\n",
            "iteration: 273480 loss: 0.0043 lr: 0.02\n",
            "iteration: 273490 loss: 0.0035 lr: 0.02\n",
            "iteration: 273500 loss: 0.0046 lr: 0.02\n",
            "iteration: 273510 loss: 0.0036 lr: 0.02\n",
            "iteration: 273520 loss: 0.0034 lr: 0.02\n",
            "iteration: 273530 loss: 0.0049 lr: 0.02\n",
            "iteration: 273540 loss: 0.0035 lr: 0.02\n",
            "iteration: 273550 loss: 0.0036 lr: 0.02\n",
            "iteration: 273560 loss: 0.0039 lr: 0.02\n",
            "iteration: 273570 loss: 0.0044 lr: 0.02\n",
            "iteration: 273580 loss: 0.0034 lr: 0.02\n",
            "iteration: 273590 loss: 0.0041 lr: 0.02\n",
            "iteration: 273600 loss: 0.0042 lr: 0.02\n",
            "iteration: 273610 loss: 0.0047 lr: 0.02\n",
            "iteration: 273620 loss: 0.0030 lr: 0.02\n",
            "iteration: 273630 loss: 0.0035 lr: 0.02\n",
            "iteration: 273640 loss: 0.0037 lr: 0.02\n",
            "iteration: 273650 loss: 0.0036 lr: 0.02\n",
            "iteration: 273660 loss: 0.0034 lr: 0.02\n",
            "iteration: 273670 loss: 0.0044 lr: 0.02\n",
            "iteration: 273680 loss: 0.0036 lr: 0.02\n",
            "iteration: 273690 loss: 0.0032 lr: 0.02\n",
            "iteration: 273700 loss: 0.0039 lr: 0.02\n",
            "iteration: 273710 loss: 0.0039 lr: 0.02\n",
            "iteration: 273720 loss: 0.0040 lr: 0.02\n",
            "iteration: 273730 loss: 0.0033 lr: 0.02\n",
            "iteration: 273740 loss: 0.0042 lr: 0.02\n",
            "iteration: 273750 loss: 0.0021 lr: 0.02\n",
            "iteration: 273760 loss: 0.0034 lr: 0.02\n",
            "iteration: 273770 loss: 0.0038 lr: 0.02\n",
            "iteration: 273780 loss: 0.0030 lr: 0.02\n",
            "iteration: 273790 loss: 0.0036 lr: 0.02\n",
            "iteration: 273800 loss: 0.0037 lr: 0.02\n",
            "iteration: 273810 loss: 0.0035 lr: 0.02\n",
            "iteration: 273820 loss: 0.0035 lr: 0.02\n",
            "iteration: 273830 loss: 0.0035 lr: 0.02\n",
            "iteration: 273840 loss: 0.0031 lr: 0.02\n",
            "iteration: 273850 loss: 0.0036 lr: 0.02\n",
            "iteration: 273860 loss: 0.0031 lr: 0.02\n",
            "iteration: 273870 loss: 0.0040 lr: 0.02\n",
            "iteration: 273880 loss: 0.0035 lr: 0.02\n",
            "iteration: 273890 loss: 0.0041 lr: 0.02\n",
            "iteration: 273900 loss: 0.0032 lr: 0.02\n",
            "iteration: 273910 loss: 0.0044 lr: 0.02\n",
            "iteration: 273920 loss: 0.0037 lr: 0.02\n",
            "iteration: 273930 loss: 0.0048 lr: 0.02\n",
            "iteration: 273940 loss: 0.0030 lr: 0.02\n",
            "iteration: 273950 loss: 0.0038 lr: 0.02\n",
            "iteration: 273960 loss: 0.0028 lr: 0.02\n",
            "iteration: 273970 loss: 0.0039 lr: 0.02\n",
            "iteration: 273980 loss: 0.0048 lr: 0.02\n",
            "iteration: 273990 loss: 0.0045 lr: 0.02\n",
            "iteration: 274000 loss: 0.0034 lr: 0.02\n",
            "iteration: 274010 loss: 0.0037 lr: 0.02\n",
            "iteration: 274020 loss: 0.0041 lr: 0.02\n",
            "iteration: 274030 loss: 0.0032 lr: 0.02\n",
            "iteration: 274040 loss: 0.0039 lr: 0.02\n",
            "iteration: 274050 loss: 0.0049 lr: 0.02\n",
            "iteration: 274060 loss: 0.0040 lr: 0.02\n",
            "iteration: 274070 loss: 0.0042 lr: 0.02\n",
            "iteration: 274080 loss: 0.0039 lr: 0.02\n",
            "iteration: 274090 loss: 0.0035 lr: 0.02\n",
            "iteration: 274100 loss: 0.0038 lr: 0.02\n",
            "iteration: 274110 loss: 0.0040 lr: 0.02\n",
            "iteration: 274120 loss: 0.0029 lr: 0.02\n",
            "iteration: 274130 loss: 0.0048 lr: 0.02\n",
            "iteration: 274140 loss: 0.0055 lr: 0.02\n",
            "iteration: 274150 loss: 0.0034 lr: 0.02\n",
            "iteration: 274160 loss: 0.0039 lr: 0.02\n",
            "iteration: 274170 loss: 0.0033 lr: 0.02\n",
            "iteration: 274180 loss: 0.0041 lr: 0.02\n",
            "iteration: 274190 loss: 0.0042 lr: 0.02\n",
            "iteration: 274200 loss: 0.0035 lr: 0.02\n",
            "iteration: 274210 loss: 0.0036 lr: 0.02\n",
            "iteration: 274220 loss: 0.0046 lr: 0.02\n",
            "iteration: 274230 loss: 0.0037 lr: 0.02\n",
            "iteration: 274240 loss: 0.0033 lr: 0.02\n",
            "iteration: 274250 loss: 0.0036 lr: 0.02\n",
            "iteration: 274260 loss: 0.0035 lr: 0.02\n",
            "iteration: 274270 loss: 0.0038 lr: 0.02\n",
            "iteration: 274280 loss: 0.0040 lr: 0.02\n",
            "iteration: 274290 loss: 0.0045 lr: 0.02\n",
            "iteration: 274300 loss: 0.0028 lr: 0.02\n",
            "iteration: 274310 loss: 0.0033 lr: 0.02\n",
            "iteration: 274320 loss: 0.0032 lr: 0.02\n",
            "iteration: 274330 loss: 0.0038 lr: 0.02\n",
            "iteration: 274340 loss: 0.0057 lr: 0.02\n",
            "iteration: 274350 loss: 0.0035 lr: 0.02\n",
            "iteration: 274360 loss: 0.0031 lr: 0.02\n",
            "iteration: 274370 loss: 0.0039 lr: 0.02\n",
            "iteration: 274380 loss: 0.0034 lr: 0.02\n",
            "iteration: 274390 loss: 0.0031 lr: 0.02\n",
            "iteration: 274400 loss: 0.0037 lr: 0.02\n",
            "iteration: 274410 loss: 0.0035 lr: 0.02\n",
            "iteration: 274420 loss: 0.0041 lr: 0.02\n",
            "iteration: 274430 loss: 0.0042 lr: 0.02\n",
            "iteration: 274440 loss: 0.0052 lr: 0.02\n",
            "iteration: 274450 loss: 0.0036 lr: 0.02\n",
            "iteration: 274460 loss: 0.0034 lr: 0.02\n",
            "iteration: 274470 loss: 0.0035 lr: 0.02\n",
            "iteration: 274480 loss: 0.0039 lr: 0.02\n",
            "iteration: 274490 loss: 0.0035 lr: 0.02\n",
            "iteration: 274500 loss: 0.0036 lr: 0.02\n",
            "iteration: 274510 loss: 0.0038 lr: 0.02\n",
            "iteration: 274520 loss: 0.0053 lr: 0.02\n",
            "iteration: 274530 loss: 0.0033 lr: 0.02\n",
            "iteration: 274540 loss: 0.0040 lr: 0.02\n",
            "iteration: 274550 loss: 0.0036 lr: 0.02\n",
            "iteration: 274560 loss: 0.0039 lr: 0.02\n",
            "iteration: 274570 loss: 0.0042 lr: 0.02\n",
            "iteration: 274580 loss: 0.0032 lr: 0.02\n",
            "iteration: 274590 loss: 0.0034 lr: 0.02\n",
            "iteration: 274600 loss: 0.0034 lr: 0.02\n",
            "iteration: 274610 loss: 0.0040 lr: 0.02\n",
            "iteration: 274620 loss: 0.0034 lr: 0.02\n",
            "iteration: 274630 loss: 0.0035 lr: 0.02\n",
            "iteration: 274640 loss: 0.0025 lr: 0.02\n",
            "iteration: 274650 loss: 0.0052 lr: 0.02\n",
            "iteration: 274660 loss: 0.0023 lr: 0.02\n",
            "iteration: 274670 loss: 0.0031 lr: 0.02\n",
            "iteration: 274680 loss: 0.0037 lr: 0.02\n",
            "iteration: 274690 loss: 0.0026 lr: 0.02\n",
            "iteration: 274700 loss: 0.0036 lr: 0.02\n",
            "iteration: 274710 loss: 0.0041 lr: 0.02\n",
            "iteration: 274720 loss: 0.0032 lr: 0.02\n",
            "iteration: 274730 loss: 0.0036 lr: 0.02\n",
            "iteration: 274740 loss: 0.0031 lr: 0.02\n",
            "iteration: 274750 loss: 0.0033 lr: 0.02\n",
            "iteration: 274760 loss: 0.0034 lr: 0.02\n",
            "iteration: 274770 loss: 0.0046 lr: 0.02\n",
            "iteration: 274780 loss: 0.0037 lr: 0.02\n",
            "iteration: 274790 loss: 0.0031 lr: 0.02\n",
            "iteration: 274800 loss: 0.0037 lr: 0.02\n",
            "iteration: 274810 loss: 0.0027 lr: 0.02\n",
            "iteration: 274820 loss: 0.0039 lr: 0.02\n",
            "iteration: 274830 loss: 0.0025 lr: 0.02\n",
            "iteration: 274840 loss: 0.0035 lr: 0.02\n",
            "iteration: 274850 loss: 0.0037 lr: 0.02\n",
            "iteration: 274860 loss: 0.0023 lr: 0.02\n",
            "iteration: 274870 loss: 0.0029 lr: 0.02\n",
            "iteration: 274880 loss: 0.0037 lr: 0.02\n",
            "iteration: 274890 loss: 0.0032 lr: 0.02\n",
            "iteration: 274900 loss: 0.0031 lr: 0.02\n",
            "iteration: 274910 loss: 0.0031 lr: 0.02\n",
            "iteration: 274920 loss: 0.0035 lr: 0.02\n",
            "iteration: 274930 loss: 0.0032 lr: 0.02\n",
            "iteration: 274940 loss: 0.0025 lr: 0.02\n",
            "iteration: 274950 loss: 0.0037 lr: 0.02\n",
            "iteration: 274960 loss: 0.0038 lr: 0.02\n",
            "iteration: 274970 loss: 0.0034 lr: 0.02\n",
            "iteration: 274980 loss: 0.0037 lr: 0.02\n",
            "iteration: 274990 loss: 0.0041 lr: 0.02\n",
            "iteration: 275000 loss: 0.0038 lr: 0.02\n",
            "iteration: 275010 loss: 0.0029 lr: 0.02\n",
            "iteration: 275020 loss: 0.0035 lr: 0.02\n",
            "iteration: 275030 loss: 0.0035 lr: 0.02\n",
            "iteration: 275040 loss: 0.0037 lr: 0.02\n",
            "iteration: 275050 loss: 0.0034 lr: 0.02\n",
            "iteration: 275060 loss: 0.0033 lr: 0.02\n",
            "iteration: 275070 loss: 0.0044 lr: 0.02\n",
            "iteration: 275080 loss: 0.0031 lr: 0.02\n",
            "iteration: 275090 loss: 0.0034 lr: 0.02\n",
            "iteration: 275100 loss: 0.0043 lr: 0.02\n",
            "iteration: 275110 loss: 0.0048 lr: 0.02\n",
            "iteration: 275120 loss: 0.0032 lr: 0.02\n",
            "iteration: 275130 loss: 0.0039 lr: 0.02\n",
            "iteration: 275140 loss: 0.0029 lr: 0.02\n",
            "iteration: 275150 loss: 0.0036 lr: 0.02\n",
            "iteration: 275160 loss: 0.0034 lr: 0.02\n",
            "iteration: 275170 loss: 0.0040 lr: 0.02\n",
            "iteration: 275180 loss: 0.0036 lr: 0.02\n",
            "iteration: 275190 loss: 0.0045 lr: 0.02\n",
            "iteration: 275200 loss: 0.0034 lr: 0.02\n",
            "iteration: 275210 loss: 0.0029 lr: 0.02\n",
            "iteration: 275220 loss: 0.0028 lr: 0.02\n",
            "iteration: 275230 loss: 0.0041 lr: 0.02\n",
            "iteration: 275240 loss: 0.0030 lr: 0.02\n",
            "iteration: 275250 loss: 0.0028 lr: 0.02\n",
            "iteration: 275260 loss: 0.0036 lr: 0.02\n",
            "iteration: 275270 loss: 0.0042 lr: 0.02\n",
            "iteration: 275280 loss: 0.0031 lr: 0.02\n",
            "iteration: 275290 loss: 0.0035 lr: 0.02\n",
            "iteration: 275300 loss: 0.0035 lr: 0.02\n",
            "iteration: 275310 loss: 0.0035 lr: 0.02\n",
            "iteration: 275320 loss: 0.0034 lr: 0.02\n",
            "iteration: 275330 loss: 0.0042 lr: 0.02\n",
            "iteration: 275340 loss: 0.0028 lr: 0.02\n",
            "iteration: 275350 loss: 0.0045 lr: 0.02\n",
            "iteration: 275360 loss: 0.0030 lr: 0.02\n",
            "iteration: 275370 loss: 0.0032 lr: 0.02\n",
            "iteration: 275380 loss: 0.0052 lr: 0.02\n",
            "iteration: 275390 loss: 0.0034 lr: 0.02\n",
            "iteration: 275400 loss: 0.0034 lr: 0.02\n",
            "iteration: 275410 loss: 0.0035 lr: 0.02\n",
            "iteration: 275420 loss: 0.0031 lr: 0.02\n",
            "iteration: 275430 loss: 0.0033 lr: 0.02\n",
            "iteration: 275440 loss: 0.0029 lr: 0.02\n",
            "iteration: 275450 loss: 0.0039 lr: 0.02\n",
            "iteration: 275460 loss: 0.0036 lr: 0.02\n",
            "iteration: 275470 loss: 0.0044 lr: 0.02\n",
            "iteration: 275480 loss: 0.0035 lr: 0.02\n",
            "iteration: 275490 loss: 0.0043 lr: 0.02\n",
            "iteration: 275500 loss: 0.0040 lr: 0.02\n",
            "iteration: 275510 loss: 0.0035 lr: 0.02\n",
            "iteration: 275520 loss: 0.0033 lr: 0.02\n",
            "iteration: 275530 loss: 0.0032 lr: 0.02\n",
            "iteration: 275540 loss: 0.0033 lr: 0.02\n",
            "iteration: 275550 loss: 0.0046 lr: 0.02\n",
            "iteration: 275560 loss: 0.0046 lr: 0.02\n",
            "iteration: 275570 loss: 0.0037 lr: 0.02\n",
            "iteration: 275580 loss: 0.0023 lr: 0.02\n",
            "iteration: 275590 loss: 0.0036 lr: 0.02\n",
            "iteration: 275600 loss: 0.0028 lr: 0.02\n",
            "iteration: 275610 loss: 0.0024 lr: 0.02\n",
            "iteration: 275620 loss: 0.0039 lr: 0.02\n",
            "iteration: 275630 loss: 0.0043 lr: 0.02\n",
            "iteration: 275640 loss: 0.0038 lr: 0.02\n",
            "iteration: 275650 loss: 0.0039 lr: 0.02\n",
            "iteration: 275660 loss: 0.0028 lr: 0.02\n",
            "iteration: 275670 loss: 0.0033 lr: 0.02\n",
            "iteration: 275680 loss: 0.0037 lr: 0.02\n",
            "iteration: 275690 loss: 0.0037 lr: 0.02\n",
            "iteration: 275700 loss: 0.0036 lr: 0.02\n",
            "iteration: 275710 loss: 0.0033 lr: 0.02\n",
            "iteration: 275720 loss: 0.0032 lr: 0.02\n",
            "iteration: 275730 loss: 0.0035 lr: 0.02\n",
            "iteration: 275740 loss: 0.0033 lr: 0.02\n",
            "iteration: 275750 loss: 0.0054 lr: 0.02\n",
            "iteration: 275760 loss: 0.0044 lr: 0.02\n",
            "iteration: 275770 loss: 0.0033 lr: 0.02\n",
            "iteration: 275780 loss: 0.0035 lr: 0.02\n",
            "iteration: 275790 loss: 0.0037 lr: 0.02\n",
            "iteration: 275800 loss: 0.0034 lr: 0.02\n",
            "iteration: 275810 loss: 0.0042 lr: 0.02\n",
            "iteration: 275820 loss: 0.0028 lr: 0.02\n",
            "iteration: 275830 loss: 0.0030 lr: 0.02\n",
            "iteration: 275840 loss: 0.0039 lr: 0.02\n",
            "iteration: 275850 loss: 0.0029 lr: 0.02\n",
            "iteration: 275860 loss: 0.0033 lr: 0.02\n",
            "iteration: 275870 loss: 0.0033 lr: 0.02\n",
            "iteration: 275880 loss: 0.0037 lr: 0.02\n",
            "iteration: 275890 loss: 0.0031 lr: 0.02\n",
            "iteration: 275900 loss: 0.0036 lr: 0.02\n",
            "iteration: 275910 loss: 0.0043 lr: 0.02\n",
            "iteration: 275920 loss: 0.0031 lr: 0.02\n",
            "iteration: 275930 loss: 0.0054 lr: 0.02\n",
            "iteration: 275940 loss: 0.0042 lr: 0.02\n",
            "iteration: 275950 loss: 0.0045 lr: 0.02\n",
            "iteration: 275960 loss: 0.0035 lr: 0.02\n",
            "iteration: 275970 loss: 0.0035 lr: 0.02\n",
            "iteration: 275980 loss: 0.0034 lr: 0.02\n",
            "iteration: 275990 loss: 0.0045 lr: 0.02\n",
            "iteration: 276000 loss: 0.0047 lr: 0.02\n",
            "iteration: 276010 loss: 0.0031 lr: 0.02\n",
            "iteration: 276020 loss: 0.0033 lr: 0.02\n",
            "iteration: 276030 loss: 0.0035 lr: 0.02\n",
            "iteration: 276040 loss: 0.0039 lr: 0.02\n",
            "iteration: 276050 loss: 0.0043 lr: 0.02\n",
            "iteration: 276060 loss: 0.0036 lr: 0.02\n",
            "iteration: 276070 loss: 0.0034 lr: 0.02\n",
            "iteration: 276080 loss: 0.0048 lr: 0.02\n",
            "iteration: 276090 loss: 0.0050 lr: 0.02\n",
            "iteration: 276100 loss: 0.0029 lr: 0.02\n",
            "iteration: 276110 loss: 0.0033 lr: 0.02\n",
            "iteration: 276120 loss: 0.0037 lr: 0.02\n",
            "iteration: 276130 loss: 0.0040 lr: 0.02\n",
            "iteration: 276140 loss: 0.0041 lr: 0.02\n",
            "iteration: 276150 loss: 0.0035 lr: 0.02\n",
            "iteration: 276160 loss: 0.0036 lr: 0.02\n",
            "iteration: 276170 loss: 0.0035 lr: 0.02\n",
            "iteration: 276180 loss: 0.0041 lr: 0.02\n",
            "iteration: 276190 loss: 0.0032 lr: 0.02\n",
            "iteration: 276200 loss: 0.0032 lr: 0.02\n",
            "iteration: 276210 loss: 0.0035 lr: 0.02\n",
            "iteration: 276220 loss: 0.0039 lr: 0.02\n",
            "iteration: 276230 loss: 0.0042 lr: 0.02\n",
            "iteration: 276240 loss: 0.0039 lr: 0.02\n",
            "iteration: 276250 loss: 0.0039 lr: 0.02\n",
            "iteration: 276260 loss: 0.0033 lr: 0.02\n",
            "iteration: 276270 loss: 0.0034 lr: 0.02\n",
            "iteration: 276280 loss: 0.0031 lr: 0.02\n",
            "iteration: 276290 loss: 0.0036 lr: 0.02\n",
            "iteration: 276300 loss: 0.0032 lr: 0.02\n",
            "iteration: 276310 loss: 0.0034 lr: 0.02\n",
            "iteration: 276320 loss: 0.0033 lr: 0.02\n",
            "iteration: 276330 loss: 0.0036 lr: 0.02\n",
            "iteration: 276340 loss: 0.0048 lr: 0.02\n",
            "iteration: 276350 loss: 0.0033 lr: 0.02\n",
            "iteration: 276360 loss: 0.0041 lr: 0.02\n",
            "iteration: 276370 loss: 0.0031 lr: 0.02\n",
            "iteration: 276380 loss: 0.0042 lr: 0.02\n",
            "iteration: 276390 loss: 0.0034 lr: 0.02\n",
            "iteration: 276400 loss: 0.0043 lr: 0.02\n",
            "iteration: 276410 loss: 0.0034 lr: 0.02\n",
            "iteration: 276420 loss: 0.0039 lr: 0.02\n",
            "iteration: 276430 loss: 0.0035 lr: 0.02\n",
            "iteration: 276440 loss: 0.0033 lr: 0.02\n",
            "iteration: 276450 loss: 0.0035 lr: 0.02\n",
            "iteration: 276460 loss: 0.0030 lr: 0.02\n",
            "iteration: 276470 loss: 0.0039 lr: 0.02\n",
            "iteration: 276480 loss: 0.0040 lr: 0.02\n",
            "iteration: 276490 loss: 0.0021 lr: 0.02\n",
            "iteration: 276500 loss: 0.0043 lr: 0.02\n",
            "iteration: 276510 loss: 0.0029 lr: 0.02\n",
            "iteration: 276520 loss: 0.0032 lr: 0.02\n",
            "iteration: 276530 loss: 0.0034 lr: 0.02\n",
            "iteration: 276540 loss: 0.0033 lr: 0.02\n",
            "iteration: 276550 loss: 0.0038 lr: 0.02\n",
            "iteration: 276560 loss: 0.0038 lr: 0.02\n",
            "iteration: 276570 loss: 0.0030 lr: 0.02\n",
            "iteration: 276580 loss: 0.0037 lr: 0.02\n",
            "iteration: 276590 loss: 0.0033 lr: 0.02\n",
            "iteration: 276600 loss: 0.0041 lr: 0.02\n",
            "iteration: 276610 loss: 0.0030 lr: 0.02\n",
            "iteration: 276620 loss: 0.0037 lr: 0.02\n",
            "iteration: 276630 loss: 0.0035 lr: 0.02\n",
            "iteration: 276640 loss: 0.0036 lr: 0.02\n",
            "iteration: 276650 loss: 0.0040 lr: 0.02\n",
            "iteration: 276660 loss: 0.0041 lr: 0.02\n",
            "iteration: 276670 loss: 0.0034 lr: 0.02\n",
            "iteration: 276680 loss: 0.0034 lr: 0.02\n",
            "iteration: 276690 loss: 0.0026 lr: 0.02\n",
            "iteration: 276700 loss: 0.0037 lr: 0.02\n",
            "iteration: 276710 loss: 0.0033 lr: 0.02\n",
            "iteration: 276720 loss: 0.0036 lr: 0.02\n",
            "iteration: 276730 loss: 0.0037 lr: 0.02\n",
            "iteration: 276740 loss: 0.0031 lr: 0.02\n",
            "iteration: 276750 loss: 0.0044 lr: 0.02\n",
            "iteration: 276760 loss: 0.0039 lr: 0.02\n",
            "iteration: 276770 loss: 0.0042 lr: 0.02\n",
            "iteration: 276780 loss: 0.0031 lr: 0.02\n",
            "iteration: 276790 loss: 0.0044 lr: 0.02\n",
            "iteration: 276800 loss: 0.0035 lr: 0.02\n",
            "iteration: 276810 loss: 0.0034 lr: 0.02\n",
            "iteration: 276820 loss: 0.0030 lr: 0.02\n",
            "iteration: 276830 loss: 0.0037 lr: 0.02\n",
            "iteration: 276840 loss: 0.0034 lr: 0.02\n",
            "iteration: 276850 loss: 0.0035 lr: 0.02\n",
            "iteration: 276860 loss: 0.0029 lr: 0.02\n",
            "iteration: 276870 loss: 0.0036 lr: 0.02\n",
            "iteration: 276880 loss: 0.0040 lr: 0.02\n",
            "iteration: 276890 loss: 0.0034 lr: 0.02\n",
            "iteration: 276900 loss: 0.0035 lr: 0.02\n",
            "iteration: 276910 loss: 0.0030 lr: 0.02\n",
            "iteration: 276920 loss: 0.0039 lr: 0.02\n",
            "iteration: 276930 loss: 0.0040 lr: 0.02\n",
            "iteration: 276940 loss: 0.0039 lr: 0.02\n",
            "iteration: 276950 loss: 0.0034 lr: 0.02\n",
            "iteration: 276960 loss: 0.0037 lr: 0.02\n",
            "iteration: 276970 loss: 0.0036 lr: 0.02\n",
            "iteration: 276980 loss: 0.0045 lr: 0.02\n",
            "iteration: 276990 loss: 0.0046 lr: 0.02\n",
            "iteration: 277000 loss: 0.0032 lr: 0.02\n",
            "iteration: 277010 loss: 0.0030 lr: 0.02\n",
            "iteration: 277020 loss: 0.0032 lr: 0.02\n",
            "iteration: 277030 loss: 0.0036 lr: 0.02\n",
            "iteration: 277040 loss: 0.0036 lr: 0.02\n",
            "iteration: 277050 loss: 0.0041 lr: 0.02\n",
            "iteration: 277060 loss: 0.0039 lr: 0.02\n",
            "iteration: 277070 loss: 0.0040 lr: 0.02\n",
            "iteration: 277080 loss: 0.0039 lr: 0.02\n",
            "iteration: 277090 loss: 0.0034 lr: 0.02\n",
            "iteration: 277100 loss: 0.0039 lr: 0.02\n",
            "iteration: 277110 loss: 0.0029 lr: 0.02\n",
            "iteration: 277120 loss: 0.0044 lr: 0.02\n",
            "iteration: 277130 loss: 0.0029 lr: 0.02\n",
            "iteration: 277140 loss: 0.0042 lr: 0.02\n",
            "iteration: 277150 loss: 0.0036 lr: 0.02\n",
            "iteration: 277160 loss: 0.0050 lr: 0.02\n",
            "iteration: 277170 loss: 0.0041 lr: 0.02\n",
            "iteration: 277180 loss: 0.0041 lr: 0.02\n",
            "iteration: 277190 loss: 0.0030 lr: 0.02\n",
            "iteration: 277200 loss: 0.0032 lr: 0.02\n",
            "iteration: 277210 loss: 0.0035 lr: 0.02\n",
            "iteration: 277220 loss: 0.0038 lr: 0.02\n",
            "iteration: 277230 loss: 0.0037 lr: 0.02\n",
            "iteration: 277240 loss: 0.0038 lr: 0.02\n",
            "iteration: 277250 loss: 0.0035 lr: 0.02\n",
            "iteration: 277260 loss: 0.0038 lr: 0.02\n",
            "iteration: 277270 loss: 0.0043 lr: 0.02\n",
            "iteration: 277280 loss: 0.0039 lr: 0.02\n",
            "iteration: 277290 loss: 0.0044 lr: 0.02\n",
            "iteration: 277300 loss: 0.0053 lr: 0.02\n",
            "iteration: 277310 loss: 0.0038 lr: 0.02\n",
            "iteration: 277320 loss: 0.0024 lr: 0.02\n",
            "iteration: 277330 loss: 0.0037 lr: 0.02\n",
            "iteration: 277340 loss: 0.0031 lr: 0.02\n",
            "iteration: 277350 loss: 0.0030 lr: 0.02\n",
            "iteration: 277360 loss: 0.0046 lr: 0.02\n",
            "iteration: 277370 loss: 0.0034 lr: 0.02\n",
            "iteration: 277380 loss: 0.0037 lr: 0.02\n",
            "iteration: 277390 loss: 0.0029 lr: 0.02\n",
            "iteration: 277400 loss: 0.0035 lr: 0.02\n",
            "iteration: 277410 loss: 0.0045 lr: 0.02\n",
            "iteration: 277420 loss: 0.0040 lr: 0.02\n",
            "iteration: 277430 loss: 0.0029 lr: 0.02\n",
            "iteration: 277440 loss: 0.0034 lr: 0.02\n",
            "iteration: 277450 loss: 0.0043 lr: 0.02\n",
            "iteration: 277460 loss: 0.0035 lr: 0.02\n",
            "iteration: 277470 loss: 0.0028 lr: 0.02\n",
            "iteration: 277480 loss: 0.0035 lr: 0.02\n",
            "iteration: 277490 loss: 0.0036 lr: 0.02\n",
            "iteration: 277500 loss: 0.0024 lr: 0.02\n",
            "iteration: 277510 loss: 0.0031 lr: 0.02\n",
            "iteration: 277520 loss: 0.0032 lr: 0.02\n",
            "iteration: 277530 loss: 0.0038 lr: 0.02\n",
            "iteration: 277540 loss: 0.0043 lr: 0.02\n",
            "iteration: 277550 loss: 0.0040 lr: 0.02\n",
            "iteration: 277560 loss: 0.0033 lr: 0.02\n",
            "iteration: 277570 loss: 0.0040 lr: 0.02\n",
            "iteration: 277580 loss: 0.0040 lr: 0.02\n",
            "iteration: 277590 loss: 0.0032 lr: 0.02\n",
            "iteration: 277600 loss: 0.0039 lr: 0.02\n",
            "iteration: 277610 loss: 0.0046 lr: 0.02\n",
            "iteration: 277620 loss: 0.0028 lr: 0.02\n",
            "iteration: 277630 loss: 0.0035 lr: 0.02\n",
            "iteration: 277640 loss: 0.0029 lr: 0.02\n",
            "iteration: 277650 loss: 0.0036 lr: 0.02\n",
            "iteration: 277660 loss: 0.0034 lr: 0.02\n",
            "iteration: 277670 loss: 0.0036 lr: 0.02\n",
            "iteration: 277680 loss: 0.0044 lr: 0.02\n",
            "iteration: 277690 loss: 0.0038 lr: 0.02\n",
            "iteration: 277700 loss: 0.0034 lr: 0.02\n",
            "iteration: 277710 loss: 0.0035 lr: 0.02\n",
            "iteration: 277720 loss: 0.0033 lr: 0.02\n",
            "iteration: 277730 loss: 0.0045 lr: 0.02\n",
            "iteration: 277740 loss: 0.0044 lr: 0.02\n",
            "iteration: 277750 loss: 0.0034 lr: 0.02\n",
            "iteration: 277760 loss: 0.0027 lr: 0.02\n",
            "iteration: 277770 loss: 0.0030 lr: 0.02\n",
            "iteration: 277780 loss: 0.0030 lr: 0.02\n",
            "iteration: 277790 loss: 0.0034 lr: 0.02\n",
            "iteration: 277800 loss: 0.0032 lr: 0.02\n",
            "iteration: 277810 loss: 0.0041 lr: 0.02\n",
            "iteration: 277820 loss: 0.0037 lr: 0.02\n",
            "iteration: 277830 loss: 0.0035 lr: 0.02\n",
            "iteration: 277840 loss: 0.0036 lr: 0.02\n",
            "iteration: 277850 loss: 0.0038 lr: 0.02\n",
            "iteration: 277860 loss: 0.0038 lr: 0.02\n",
            "iteration: 277870 loss: 0.0029 lr: 0.02\n",
            "iteration: 277880 loss: 0.0035 lr: 0.02\n",
            "iteration: 277890 loss: 0.0039 lr: 0.02\n",
            "iteration: 277900 loss: 0.0028 lr: 0.02\n",
            "iteration: 277910 loss: 0.0040 lr: 0.02\n",
            "iteration: 277920 loss: 0.0040 lr: 0.02\n",
            "iteration: 277930 loss: 0.0026 lr: 0.02\n",
            "iteration: 277940 loss: 0.0043 lr: 0.02\n",
            "iteration: 277950 loss: 0.0039 lr: 0.02\n",
            "iteration: 277960 loss: 0.0041 lr: 0.02\n",
            "iteration: 277970 loss: 0.0030 lr: 0.02\n",
            "iteration: 277980 loss: 0.0046 lr: 0.02\n",
            "iteration: 277990 loss: 0.0047 lr: 0.02\n",
            "iteration: 278000 loss: 0.0039 lr: 0.02\n",
            "iteration: 278010 loss: 0.0025 lr: 0.02\n",
            "iteration: 278020 loss: 0.0034 lr: 0.02\n",
            "iteration: 278030 loss: 0.0041 lr: 0.02\n",
            "iteration: 278040 loss: 0.0040 lr: 0.02\n",
            "iteration: 278050 loss: 0.0044 lr: 0.02\n",
            "iteration: 278060 loss: 0.0038 lr: 0.02\n",
            "iteration: 278070 loss: 0.0047 lr: 0.02\n",
            "iteration: 278080 loss: 0.0043 lr: 0.02\n",
            "iteration: 278090 loss: 0.0038 lr: 0.02\n",
            "iteration: 278100 loss: 0.0036 lr: 0.02\n",
            "iteration: 278110 loss: 0.0040 lr: 0.02\n",
            "iteration: 278120 loss: 0.0035 lr: 0.02\n",
            "iteration: 278130 loss: 0.0037 lr: 0.02\n",
            "iteration: 278140 loss: 0.0033 lr: 0.02\n",
            "iteration: 278150 loss: 0.0029 lr: 0.02\n",
            "iteration: 278160 loss: 0.0039 lr: 0.02\n",
            "iteration: 278170 loss: 0.0028 lr: 0.02\n",
            "iteration: 278180 loss: 0.0038 lr: 0.02\n",
            "iteration: 278190 loss: 0.0039 lr: 0.02\n",
            "iteration: 278200 loss: 0.0035 lr: 0.02\n",
            "iteration: 278210 loss: 0.0030 lr: 0.02\n",
            "iteration: 278220 loss: 0.0037 lr: 0.02\n",
            "iteration: 278230 loss: 0.0032 lr: 0.02\n",
            "iteration: 278240 loss: 0.0038 lr: 0.02\n",
            "iteration: 278250 loss: 0.0036 lr: 0.02\n",
            "iteration: 278260 loss: 0.0036 lr: 0.02\n",
            "iteration: 278270 loss: 0.0038 lr: 0.02\n",
            "iteration: 278280 loss: 0.0029 lr: 0.02\n",
            "iteration: 278290 loss: 0.0034 lr: 0.02\n",
            "iteration: 278300 loss: 0.0030 lr: 0.02\n",
            "iteration: 278310 loss: 0.0033 lr: 0.02\n",
            "iteration: 278320 loss: 0.0032 lr: 0.02\n",
            "iteration: 278330 loss: 0.0033 lr: 0.02\n",
            "iteration: 278340 loss: 0.0044 lr: 0.02\n",
            "iteration: 278350 loss: 0.0034 lr: 0.02\n",
            "iteration: 278360 loss: 0.0031 lr: 0.02\n",
            "iteration: 278370 loss: 0.0046 lr: 0.02\n",
            "iteration: 278380 loss: 0.0040 lr: 0.02\n",
            "iteration: 278390 loss: 0.0035 lr: 0.02\n",
            "iteration: 278400 loss: 0.0044 lr: 0.02\n",
            "iteration: 278410 loss: 0.0047 lr: 0.02\n",
            "iteration: 278420 loss: 0.0038 lr: 0.02\n",
            "iteration: 278430 loss: 0.0036 lr: 0.02\n",
            "iteration: 278440 loss: 0.0034 lr: 0.02\n",
            "iteration: 278450 loss: 0.0034 lr: 0.02\n",
            "iteration: 278460 loss: 0.0046 lr: 0.02\n",
            "iteration: 278470 loss: 0.0039 lr: 0.02\n",
            "iteration: 278480 loss: 0.0033 lr: 0.02\n",
            "iteration: 278490 loss: 0.0035 lr: 0.02\n",
            "iteration: 278500 loss: 0.0033 lr: 0.02\n",
            "iteration: 278510 loss: 0.0031 lr: 0.02\n",
            "iteration: 278520 loss: 0.0038 lr: 0.02\n",
            "iteration: 278530 loss: 0.0037 lr: 0.02\n",
            "iteration: 278540 loss: 0.0032 lr: 0.02\n",
            "iteration: 278550 loss: 0.0038 lr: 0.02\n",
            "iteration: 278560 loss: 0.0038 lr: 0.02\n",
            "iteration: 278570 loss: 0.0034 lr: 0.02\n",
            "iteration: 278580 loss: 0.0034 lr: 0.02\n",
            "iteration: 278590 loss: 0.0029 lr: 0.02\n",
            "iteration: 278600 loss: 0.0038 lr: 0.02\n",
            "iteration: 278610 loss: 0.0030 lr: 0.02\n",
            "iteration: 278620 loss: 0.0039 lr: 0.02\n",
            "iteration: 278630 loss: 0.0025 lr: 0.02\n",
            "iteration: 278640 loss: 0.0035 lr: 0.02\n",
            "iteration: 278650 loss: 0.0043 lr: 0.02\n",
            "iteration: 278660 loss: 0.0031 lr: 0.02\n",
            "iteration: 278670 loss: 0.0030 lr: 0.02\n",
            "iteration: 278680 loss: 0.0041 lr: 0.02\n",
            "iteration: 278690 loss: 0.0029 lr: 0.02\n",
            "iteration: 278700 loss: 0.0041 lr: 0.02\n",
            "iteration: 278710 loss: 0.0026 lr: 0.02\n",
            "iteration: 278720 loss: 0.0029 lr: 0.02\n",
            "iteration: 278730 loss: 0.0024 lr: 0.02\n",
            "iteration: 278740 loss: 0.0029 lr: 0.02\n",
            "iteration: 278750 loss: 0.0033 lr: 0.02\n",
            "iteration: 278760 loss: 0.0032 lr: 0.02\n",
            "iteration: 278770 loss: 0.0029 lr: 0.02\n",
            "iteration: 278780 loss: 0.0036 lr: 0.02\n",
            "iteration: 278790 loss: 0.0031 lr: 0.02\n",
            "iteration: 278800 loss: 0.0039 lr: 0.02\n",
            "iteration: 278810 loss: 0.0032 lr: 0.02\n",
            "iteration: 278820 loss: 0.0044 lr: 0.02\n",
            "iteration: 278830 loss: 0.0040 lr: 0.02\n",
            "iteration: 278840 loss: 0.0036 lr: 0.02\n",
            "iteration: 278850 loss: 0.0033 lr: 0.02\n",
            "iteration: 278860 loss: 0.0048 lr: 0.02\n",
            "iteration: 278870 loss: 0.0027 lr: 0.02\n",
            "iteration: 278880 loss: 0.0032 lr: 0.02\n",
            "iteration: 278890 loss: 0.0039 lr: 0.02\n",
            "iteration: 278900 loss: 0.0034 lr: 0.02\n",
            "iteration: 278910 loss: 0.0039 lr: 0.02\n",
            "iteration: 278920 loss: 0.0032 lr: 0.02\n",
            "iteration: 278930 loss: 0.0034 lr: 0.02\n",
            "iteration: 278940 loss: 0.0030 lr: 0.02\n",
            "iteration: 278950 loss: 0.0040 lr: 0.02\n",
            "iteration: 278960 loss: 0.0036 lr: 0.02\n",
            "iteration: 278970 loss: 0.0035 lr: 0.02\n",
            "iteration: 278980 loss: 0.0032 lr: 0.02\n",
            "iteration: 278990 loss: 0.0027 lr: 0.02\n",
            "iteration: 279000 loss: 0.0036 lr: 0.02\n",
            "iteration: 279010 loss: 0.0037 lr: 0.02\n",
            "iteration: 279020 loss: 0.0025 lr: 0.02\n",
            "iteration: 279030 loss: 0.0053 lr: 0.02\n",
            "iteration: 279040 loss: 0.0031 lr: 0.02\n",
            "iteration: 279050 loss: 0.0046 lr: 0.02\n",
            "iteration: 279060 loss: 0.0031 lr: 0.02\n",
            "iteration: 279070 loss: 0.0038 lr: 0.02\n",
            "iteration: 279080 loss: 0.0028 lr: 0.02\n",
            "iteration: 279090 loss: 0.0034 lr: 0.02\n",
            "iteration: 279100 loss: 0.0035 lr: 0.02\n",
            "iteration: 279110 loss: 0.0042 lr: 0.02\n",
            "iteration: 279120 loss: 0.0034 lr: 0.02\n",
            "iteration: 279130 loss: 0.0029 lr: 0.02\n",
            "iteration: 279140 loss: 0.0039 lr: 0.02\n",
            "iteration: 279150 loss: 0.0041 lr: 0.02\n",
            "iteration: 279160 loss: 0.0035 lr: 0.02\n",
            "iteration: 279170 loss: 0.0032 lr: 0.02\n",
            "iteration: 279180 loss: 0.0029 lr: 0.02\n",
            "iteration: 279190 loss: 0.0037 lr: 0.02\n",
            "iteration: 279200 loss: 0.0039 lr: 0.02\n",
            "iteration: 279210 loss: 0.0040 lr: 0.02\n",
            "iteration: 279220 loss: 0.0037 lr: 0.02\n",
            "iteration: 279230 loss: 0.0044 lr: 0.02\n",
            "iteration: 279240 loss: 0.0040 lr: 0.02\n",
            "iteration: 279250 loss: 0.0031 lr: 0.02\n",
            "iteration: 279260 loss: 0.0043 lr: 0.02\n",
            "iteration: 279270 loss: 0.0048 lr: 0.02\n",
            "iteration: 279280 loss: 0.0030 lr: 0.02\n",
            "iteration: 279290 loss: 0.0027 lr: 0.02\n",
            "iteration: 279300 loss: 0.0033 lr: 0.02\n",
            "iteration: 279310 loss: 0.0036 lr: 0.02\n",
            "iteration: 279320 loss: 0.0037 lr: 0.02\n",
            "iteration: 279330 loss: 0.0039 lr: 0.02\n",
            "iteration: 279340 loss: 0.0039 lr: 0.02\n",
            "iteration: 279350 loss: 0.0035 lr: 0.02\n",
            "iteration: 279360 loss: 0.0033 lr: 0.02\n",
            "iteration: 279370 loss: 0.0040 lr: 0.02\n",
            "iteration: 279380 loss: 0.0040 lr: 0.02\n",
            "iteration: 279390 loss: 0.0037 lr: 0.02\n",
            "iteration: 279400 loss: 0.0031 lr: 0.02\n",
            "iteration: 279410 loss: 0.0034 lr: 0.02\n",
            "iteration: 279420 loss: 0.0038 lr: 0.02\n",
            "iteration: 279430 loss: 0.0042 lr: 0.02\n",
            "iteration: 279440 loss: 0.0033 lr: 0.02\n",
            "iteration: 279450 loss: 0.0035 lr: 0.02\n",
            "iteration: 279460 loss: 0.0057 lr: 0.02\n",
            "iteration: 279470 loss: 0.0034 lr: 0.02\n",
            "iteration: 279480 loss: 0.0041 lr: 0.02\n",
            "iteration: 279490 loss: 0.0039 lr: 0.02\n",
            "iteration: 279500 loss: 0.0033 lr: 0.02\n",
            "iteration: 279510 loss: 0.0031 lr: 0.02\n",
            "iteration: 279520 loss: 0.0029 lr: 0.02\n",
            "iteration: 279530 loss: 0.0034 lr: 0.02\n",
            "iteration: 279540 loss: 0.0024 lr: 0.02\n",
            "iteration: 279550 loss: 0.0042 lr: 0.02\n",
            "iteration: 279560 loss: 0.0033 lr: 0.02\n",
            "iteration: 279570 loss: 0.0041 lr: 0.02\n",
            "iteration: 279580 loss: 0.0031 lr: 0.02\n",
            "iteration: 279590 loss: 0.0035 lr: 0.02\n",
            "iteration: 279600 loss: 0.0033 lr: 0.02\n",
            "iteration: 279610 loss: 0.0049 lr: 0.02\n",
            "iteration: 279620 loss: 0.0028 lr: 0.02\n",
            "iteration: 279630 loss: 0.0032 lr: 0.02\n",
            "iteration: 279640 loss: 0.0026 lr: 0.02\n",
            "iteration: 279650 loss: 0.0036 lr: 0.02\n",
            "iteration: 279660 loss: 0.0037 lr: 0.02\n",
            "iteration: 279670 loss: 0.0052 lr: 0.02\n",
            "iteration: 279680 loss: 0.0037 lr: 0.02\n",
            "iteration: 279690 loss: 0.0037 lr: 0.02\n",
            "iteration: 279700 loss: 0.0038 lr: 0.02\n",
            "iteration: 279710 loss: 0.0031 lr: 0.02\n",
            "iteration: 279720 loss: 0.0038 lr: 0.02\n",
            "iteration: 279730 loss: 0.0043 lr: 0.02\n",
            "iteration: 279740 loss: 0.0035 lr: 0.02\n",
            "iteration: 279750 loss: 0.0049 lr: 0.02\n",
            "iteration: 279760 loss: 0.0031 lr: 0.02\n",
            "iteration: 279770 loss: 0.0036 lr: 0.02\n",
            "iteration: 279780 loss: 0.0042 lr: 0.02\n",
            "iteration: 279790 loss: 0.0031 lr: 0.02\n",
            "iteration: 279800 loss: 0.0035 lr: 0.02\n",
            "iteration: 279810 loss: 0.0031 lr: 0.02\n",
            "iteration: 279820 loss: 0.0040 lr: 0.02\n",
            "iteration: 279830 loss: 0.0035 lr: 0.02\n",
            "iteration: 279840 loss: 0.0031 lr: 0.02\n",
            "iteration: 279850 loss: 0.0028 lr: 0.02\n",
            "iteration: 279860 loss: 0.0032 lr: 0.02\n",
            "iteration: 279870 loss: 0.0030 lr: 0.02\n",
            "iteration: 279880 loss: 0.0031 lr: 0.02\n",
            "iteration: 279890 loss: 0.0036 lr: 0.02\n",
            "iteration: 279900 loss: 0.0033 lr: 0.02\n",
            "iteration: 279910 loss: 0.0040 lr: 0.02\n",
            "iteration: 279920 loss: 0.0028 lr: 0.02\n",
            "iteration: 279930 loss: 0.0028 lr: 0.02\n",
            "iteration: 279940 loss: 0.0030 lr: 0.02\n",
            "iteration: 279950 loss: 0.0037 lr: 0.02\n",
            "iteration: 279960 loss: 0.0036 lr: 0.02\n",
            "iteration: 279970 loss: 0.0024 lr: 0.02\n",
            "iteration: 279980 loss: 0.0036 lr: 0.02\n",
            "iteration: 279990 loss: 0.0030 lr: 0.02\n",
            "iteration: 280000 loss: 0.0039 lr: 0.02\n",
            "iteration: 280010 loss: 0.0035 lr: 0.02\n",
            "iteration: 280020 loss: 0.0037 lr: 0.02\n",
            "iteration: 280030 loss: 0.0038 lr: 0.02\n",
            "iteration: 280040 loss: 0.0031 lr: 0.02\n",
            "iteration: 280050 loss: 0.0030 lr: 0.02\n",
            "iteration: 280060 loss: 0.0038 lr: 0.02\n",
            "iteration: 280070 loss: 0.0048 lr: 0.02\n",
            "iteration: 280080 loss: 0.0036 lr: 0.02\n",
            "iteration: 280090 loss: 0.0030 lr: 0.02\n",
            "iteration: 280100 loss: 0.0038 lr: 0.02\n",
            "iteration: 280110 loss: 0.0042 lr: 0.02\n",
            "iteration: 280120 loss: 0.0059 lr: 0.02\n",
            "iteration: 280130 loss: 0.0034 lr: 0.02\n",
            "iteration: 280140 loss: 0.0040 lr: 0.02\n",
            "iteration: 280150 loss: 0.0035 lr: 0.02\n",
            "iteration: 280160 loss: 0.0033 lr: 0.02\n",
            "iteration: 280170 loss: 0.0048 lr: 0.02\n",
            "iteration: 280180 loss: 0.0039 lr: 0.02\n",
            "iteration: 280190 loss: 0.0036 lr: 0.02\n",
            "iteration: 280200 loss: 0.0046 lr: 0.02\n",
            "iteration: 280210 loss: 0.0032 lr: 0.02\n",
            "iteration: 280220 loss: 0.0037 lr: 0.02\n",
            "iteration: 280230 loss: 0.0033 lr: 0.02\n",
            "iteration: 280240 loss: 0.0032 lr: 0.02\n",
            "iteration: 280250 loss: 0.0040 lr: 0.02\n",
            "iteration: 280260 loss: 0.0038 lr: 0.02\n",
            "iteration: 280270 loss: 0.0035 lr: 0.02\n",
            "iteration: 280280 loss: 0.0039 lr: 0.02\n",
            "iteration: 280290 loss: 0.0029 lr: 0.02\n",
            "iteration: 280300 loss: 0.0025 lr: 0.02\n",
            "iteration: 280310 loss: 0.0044 lr: 0.02\n",
            "iteration: 280320 loss: 0.0023 lr: 0.02\n",
            "iteration: 280330 loss: 0.0041 lr: 0.02\n",
            "iteration: 280340 loss: 0.0038 lr: 0.02\n",
            "iteration: 280350 loss: 0.0035 lr: 0.02\n",
            "iteration: 280360 loss: 0.0037 lr: 0.02\n",
            "iteration: 280370 loss: 0.0029 lr: 0.02\n",
            "iteration: 280380 loss: 0.0035 lr: 0.02\n",
            "iteration: 280390 loss: 0.0040 lr: 0.02\n",
            "iteration: 280400 loss: 0.0032 lr: 0.02\n",
            "iteration: 280410 loss: 0.0033 lr: 0.02\n",
            "iteration: 280420 loss: 0.0036 lr: 0.02\n",
            "iteration: 280430 loss: 0.0040 lr: 0.02\n",
            "iteration: 280440 loss: 0.0034 lr: 0.02\n",
            "iteration: 280450 loss: 0.0037 lr: 0.02\n",
            "iteration: 280460 loss: 0.0025 lr: 0.02\n",
            "iteration: 280470 loss: 0.0029 lr: 0.02\n",
            "iteration: 280480 loss: 0.0039 lr: 0.02\n",
            "iteration: 280490 loss: 0.0039 lr: 0.02\n",
            "iteration: 280500 loss: 0.0032 lr: 0.02\n",
            "iteration: 280510 loss: 0.0029 lr: 0.02\n",
            "iteration: 280520 loss: 0.0033 lr: 0.02\n",
            "iteration: 280530 loss: 0.0036 lr: 0.02\n",
            "iteration: 280540 loss: 0.0035 lr: 0.02\n",
            "iteration: 280550 loss: 0.0025 lr: 0.02\n",
            "iteration: 280560 loss: 0.0038 lr: 0.02\n",
            "iteration: 280570 loss: 0.0032 lr: 0.02\n",
            "iteration: 280580 loss: 0.0038 lr: 0.02\n",
            "iteration: 280590 loss: 0.0043 lr: 0.02\n",
            "iteration: 280600 loss: 0.0040 lr: 0.02\n",
            "iteration: 280610 loss: 0.0043 lr: 0.02\n",
            "iteration: 280620 loss: 0.0043 lr: 0.02\n",
            "iteration: 280630 loss: 0.0031 lr: 0.02\n",
            "iteration: 280640 loss: 0.0036 lr: 0.02\n",
            "iteration: 280650 loss: 0.0034 lr: 0.02\n",
            "iteration: 280660 loss: 0.0025 lr: 0.02\n",
            "iteration: 280670 loss: 0.0039 lr: 0.02\n",
            "iteration: 280680 loss: 0.0036 lr: 0.02\n",
            "iteration: 280690 loss: 0.0029 lr: 0.02\n",
            "iteration: 280700 loss: 0.0033 lr: 0.02\n",
            "iteration: 280710 loss: 0.0034 lr: 0.02\n",
            "iteration: 280720 loss: 0.0036 lr: 0.02\n",
            "iteration: 280730 loss: 0.0036 lr: 0.02\n",
            "iteration: 280740 loss: 0.0046 lr: 0.02\n",
            "iteration: 280750 loss: 0.0035 lr: 0.02\n",
            "iteration: 280760 loss: 0.0039 lr: 0.02\n",
            "iteration: 280770 loss: 0.0033 lr: 0.02\n",
            "iteration: 280780 loss: 0.0037 lr: 0.02\n",
            "iteration: 280790 loss: 0.0027 lr: 0.02\n",
            "iteration: 280800 loss: 0.0036 lr: 0.02\n",
            "iteration: 280810 loss: 0.0040 lr: 0.02\n",
            "iteration: 280820 loss: 0.0033 lr: 0.02\n",
            "iteration: 280830 loss: 0.0039 lr: 0.02\n",
            "iteration: 280840 loss: 0.0045 lr: 0.02\n",
            "iteration: 280850 loss: 0.0029 lr: 0.02\n",
            "iteration: 280860 loss: 0.0038 lr: 0.02\n",
            "iteration: 280870 loss: 0.0033 lr: 0.02\n",
            "iteration: 280880 loss: 0.0038 lr: 0.02\n",
            "iteration: 280890 loss: 0.0042 lr: 0.02\n",
            "iteration: 280900 loss: 0.0035 lr: 0.02\n",
            "iteration: 280910 loss: 0.0030 lr: 0.02\n",
            "iteration: 280920 loss: 0.0033 lr: 0.02\n",
            "iteration: 280930 loss: 0.0045 lr: 0.02\n",
            "iteration: 280940 loss: 0.0040 lr: 0.02\n",
            "iteration: 280950 loss: 0.0034 lr: 0.02\n",
            "iteration: 280960 loss: 0.0039 lr: 0.02\n",
            "iteration: 280970 loss: 0.0053 lr: 0.02\n",
            "iteration: 280980 loss: 0.0041 lr: 0.02\n",
            "iteration: 280990 loss: 0.0038 lr: 0.02\n",
            "iteration: 281000 loss: 0.0031 lr: 0.02\n",
            "iteration: 281010 loss: 0.0029 lr: 0.02\n",
            "iteration: 281020 loss: 0.0039 lr: 0.02\n",
            "iteration: 281030 loss: 0.0038 lr: 0.02\n",
            "iteration: 281040 loss: 0.0039 lr: 0.02\n",
            "iteration: 281050 loss: 0.0044 lr: 0.02\n",
            "iteration: 281060 loss: 0.0034 lr: 0.02\n",
            "iteration: 281070 loss: 0.0036 lr: 0.02\n",
            "iteration: 281080 loss: 0.0035 lr: 0.02\n",
            "iteration: 281090 loss: 0.0040 lr: 0.02\n",
            "iteration: 281100 loss: 0.0032 lr: 0.02\n",
            "iteration: 281110 loss: 0.0039 lr: 0.02\n",
            "iteration: 281120 loss: 0.0038 lr: 0.02\n",
            "iteration: 281130 loss: 0.0035 lr: 0.02\n",
            "iteration: 281140 loss: 0.0028 lr: 0.02\n",
            "iteration: 281150 loss: 0.0034 lr: 0.02\n",
            "iteration: 281160 loss: 0.0038 lr: 0.02\n",
            "iteration: 281170 loss: 0.0033 lr: 0.02\n",
            "iteration: 281180 loss: 0.0031 lr: 0.02\n",
            "iteration: 281190 loss: 0.0032 lr: 0.02\n",
            "iteration: 281200 loss: 0.0032 lr: 0.02\n",
            "iteration: 281210 loss: 0.0028 lr: 0.02\n",
            "iteration: 281220 loss: 0.0033 lr: 0.02\n",
            "iteration: 281230 loss: 0.0031 lr: 0.02\n",
            "iteration: 281240 loss: 0.0039 lr: 0.02\n",
            "iteration: 281250 loss: 0.0033 lr: 0.02\n",
            "iteration: 281260 loss: 0.0030 lr: 0.02\n",
            "iteration: 281270 loss: 0.0036 lr: 0.02\n",
            "iteration: 281280 loss: 0.0036 lr: 0.02\n",
            "iteration: 281290 loss: 0.0045 lr: 0.02\n",
            "iteration: 281300 loss: 0.0028 lr: 0.02\n",
            "iteration: 281310 loss: 0.0039 lr: 0.02\n",
            "iteration: 281320 loss: 0.0034 lr: 0.02\n",
            "iteration: 281330 loss: 0.0031 lr: 0.02\n",
            "iteration: 281340 loss: 0.0046 lr: 0.02\n",
            "iteration: 281350 loss: 0.0038 lr: 0.02\n",
            "iteration: 281360 loss: 0.0042 lr: 0.02\n",
            "iteration: 281370 loss: 0.0033 lr: 0.02\n",
            "iteration: 281380 loss: 0.0039 lr: 0.02\n",
            "iteration: 281390 loss: 0.0034 lr: 0.02\n",
            "iteration: 281400 loss: 0.0034 lr: 0.02\n",
            "iteration: 281410 loss: 0.0036 lr: 0.02\n",
            "iteration: 281420 loss: 0.0028 lr: 0.02\n",
            "iteration: 281430 loss: 0.0037 lr: 0.02\n",
            "iteration: 281440 loss: 0.0039 lr: 0.02\n",
            "iteration: 281450 loss: 0.0051 lr: 0.02\n",
            "iteration: 281460 loss: 0.0045 lr: 0.02\n",
            "iteration: 281470 loss: 0.0035 lr: 0.02\n",
            "iteration: 281480 loss: 0.0041 lr: 0.02\n",
            "iteration: 281490 loss: 0.0048 lr: 0.02\n",
            "iteration: 281500 loss: 0.0047 lr: 0.02\n",
            "iteration: 281510 loss: 0.0037 lr: 0.02\n",
            "iteration: 281520 loss: 0.0033 lr: 0.02\n",
            "iteration: 281530 loss: 0.0038 lr: 0.02\n",
            "iteration: 281540 loss: 0.0036 lr: 0.02\n",
            "iteration: 281550 loss: 0.0042 lr: 0.02\n",
            "iteration: 281560 loss: 0.0035 lr: 0.02\n",
            "iteration: 281570 loss: 0.0031 lr: 0.02\n",
            "iteration: 281580 loss: 0.0046 lr: 0.02\n",
            "iteration: 281590 loss: 0.0043 lr: 0.02\n",
            "iteration: 281600 loss: 0.0028 lr: 0.02\n",
            "iteration: 281610 loss: 0.0037 lr: 0.02\n",
            "iteration: 281620 loss: 0.0035 lr: 0.02\n",
            "iteration: 281630 loss: 0.0038 lr: 0.02\n",
            "iteration: 281640 loss: 0.0029 lr: 0.02\n",
            "iteration: 281650 loss: 0.0033 lr: 0.02\n",
            "iteration: 281660 loss: 0.0033 lr: 0.02\n",
            "iteration: 281670 loss: 0.0043 lr: 0.02\n",
            "iteration: 281680 loss: 0.0034 lr: 0.02\n",
            "iteration: 281690 loss: 0.0033 lr: 0.02\n",
            "iteration: 281700 loss: 0.0032 lr: 0.02\n",
            "iteration: 281710 loss: 0.0036 lr: 0.02\n",
            "iteration: 281720 loss: 0.0041 lr: 0.02\n",
            "iteration: 281730 loss: 0.0041 lr: 0.02\n",
            "iteration: 281740 loss: 0.0025 lr: 0.02\n",
            "iteration: 281750 loss: 0.0036 lr: 0.02\n",
            "iteration: 281760 loss: 0.0031 lr: 0.02\n",
            "iteration: 281770 loss: 0.0053 lr: 0.02\n",
            "iteration: 281780 loss: 0.0033 lr: 0.02\n",
            "iteration: 281790 loss: 0.0048 lr: 0.02\n",
            "iteration: 281800 loss: 0.0048 lr: 0.02\n",
            "iteration: 281810 loss: 0.0038 lr: 0.02\n",
            "iteration: 281820 loss: 0.0037 lr: 0.02\n",
            "iteration: 281830 loss: 0.0031 lr: 0.02\n",
            "iteration: 281840 loss: 0.0030 lr: 0.02\n",
            "iteration: 281850 loss: 0.0039 lr: 0.02\n",
            "iteration: 281860 loss: 0.0030 lr: 0.02\n",
            "iteration: 281870 loss: 0.0029 lr: 0.02\n",
            "iteration: 281880 loss: 0.0044 lr: 0.02\n",
            "iteration: 281890 loss: 0.0032 lr: 0.02\n",
            "iteration: 281900 loss: 0.0036 lr: 0.02\n",
            "iteration: 281910 loss: 0.0033 lr: 0.02\n",
            "iteration: 281920 loss: 0.0044 lr: 0.02\n",
            "iteration: 281930 loss: 0.0039 lr: 0.02\n",
            "iteration: 281940 loss: 0.0050 lr: 0.02\n",
            "iteration: 281950 loss: 0.0037 lr: 0.02\n",
            "iteration: 281960 loss: 0.0026 lr: 0.02\n",
            "iteration: 281970 loss: 0.0035 lr: 0.02\n",
            "iteration: 281980 loss: 0.0037 lr: 0.02\n",
            "iteration: 281990 loss: 0.0034 lr: 0.02\n",
            "iteration: 282000 loss: 0.0033 lr: 0.02\n",
            "iteration: 282010 loss: 0.0041 lr: 0.02\n",
            "iteration: 282020 loss: 0.0038 lr: 0.02\n",
            "iteration: 282030 loss: 0.0038 lr: 0.02\n",
            "iteration: 282040 loss: 0.0023 lr: 0.02\n",
            "iteration: 282050 loss: 0.0034 lr: 0.02\n",
            "iteration: 282060 loss: 0.0038 lr: 0.02\n",
            "iteration: 282070 loss: 0.0029 lr: 0.02\n",
            "iteration: 282080 loss: 0.0032 lr: 0.02\n",
            "iteration: 282090 loss: 0.0035 lr: 0.02\n",
            "iteration: 282100 loss: 0.0038 lr: 0.02\n",
            "iteration: 282110 loss: 0.0026 lr: 0.02\n",
            "iteration: 282120 loss: 0.0040 lr: 0.02\n",
            "iteration: 282130 loss: 0.0035 lr: 0.02\n",
            "iteration: 282140 loss: 0.0033 lr: 0.02\n",
            "iteration: 282150 loss: 0.0032 lr: 0.02\n",
            "iteration: 282160 loss: 0.0036 lr: 0.02\n",
            "iteration: 282170 loss: 0.0039 lr: 0.02\n",
            "iteration: 282180 loss: 0.0042 lr: 0.02\n",
            "iteration: 282190 loss: 0.0039 lr: 0.02\n",
            "iteration: 282200 loss: 0.0037 lr: 0.02\n",
            "iteration: 282210 loss: 0.0036 lr: 0.02\n",
            "iteration: 282220 loss: 0.0051 lr: 0.02\n",
            "iteration: 282230 loss: 0.0035 lr: 0.02\n",
            "iteration: 282240 loss: 0.0039 lr: 0.02\n",
            "iteration: 282250 loss: 0.0055 lr: 0.02\n",
            "iteration: 282260 loss: 0.0048 lr: 0.02\n",
            "iteration: 282270 loss: 0.0037 lr: 0.02\n",
            "iteration: 282280 loss: 0.0038 lr: 0.02\n",
            "iteration: 282290 loss: 0.0035 lr: 0.02\n",
            "iteration: 282300 loss: 0.0031 lr: 0.02\n",
            "iteration: 282310 loss: 0.0032 lr: 0.02\n",
            "iteration: 282320 loss: 0.0040 lr: 0.02\n",
            "iteration: 282330 loss: 0.0036 lr: 0.02\n",
            "iteration: 282340 loss: 0.0026 lr: 0.02\n",
            "iteration: 282350 loss: 0.0045 lr: 0.02\n",
            "iteration: 282360 loss: 0.0034 lr: 0.02\n",
            "iteration: 282370 loss: 0.0038 lr: 0.02\n",
            "iteration: 282380 loss: 0.0053 lr: 0.02\n",
            "iteration: 282390 loss: 0.0043 lr: 0.02\n",
            "iteration: 282400 loss: 0.0038 lr: 0.02\n",
            "iteration: 282410 loss: 0.0032 lr: 0.02\n",
            "iteration: 282420 loss: 0.0028 lr: 0.02\n",
            "iteration: 282430 loss: 0.0041 lr: 0.02\n",
            "iteration: 282440 loss: 0.0033 lr: 0.02\n",
            "iteration: 282450 loss: 0.0037 lr: 0.02\n",
            "iteration: 282460 loss: 0.0034 lr: 0.02\n",
            "iteration: 282470 loss: 0.0044 lr: 0.02\n",
            "iteration: 282480 loss: 0.0030 lr: 0.02\n",
            "iteration: 282490 loss: 0.0033 lr: 0.02\n",
            "iteration: 282500 loss: 0.0032 lr: 0.02\n",
            "iteration: 282510 loss: 0.0042 lr: 0.02\n",
            "iteration: 282520 loss: 0.0040 lr: 0.02\n",
            "iteration: 282530 loss: 0.0032 lr: 0.02\n",
            "iteration: 282540 loss: 0.0041 lr: 0.02\n",
            "iteration: 282550 loss: 0.0038 lr: 0.02\n",
            "iteration: 282560 loss: 0.0033 lr: 0.02\n",
            "iteration: 282570 loss: 0.0034 lr: 0.02\n",
            "iteration: 282580 loss: 0.0042 lr: 0.02\n",
            "iteration: 282590 loss: 0.0033 lr: 0.02\n",
            "iteration: 282600 loss: 0.0036 lr: 0.02\n",
            "iteration: 282610 loss: 0.0037 lr: 0.02\n",
            "iteration: 282620 loss: 0.0033 lr: 0.02\n",
            "iteration: 282630 loss: 0.0044 lr: 0.02\n",
            "iteration: 282640 loss: 0.0038 lr: 0.02\n",
            "iteration: 282650 loss: 0.0031 lr: 0.02\n",
            "iteration: 282660 loss: 0.0028 lr: 0.02\n",
            "iteration: 282670 loss: 0.0037 lr: 0.02\n",
            "iteration: 282680 loss: 0.0032 lr: 0.02\n",
            "iteration: 282690 loss: 0.0029 lr: 0.02\n",
            "iteration: 282700 loss: 0.0030 lr: 0.02\n",
            "iteration: 282710 loss: 0.0039 lr: 0.02\n",
            "iteration: 282720 loss: 0.0034 lr: 0.02\n",
            "iteration: 282730 loss: 0.0036 lr: 0.02\n",
            "iteration: 282740 loss: 0.0039 lr: 0.02\n",
            "iteration: 282750 loss: 0.0035 lr: 0.02\n",
            "iteration: 282760 loss: 0.0026 lr: 0.02\n",
            "iteration: 282770 loss: 0.0040 lr: 0.02\n",
            "iteration: 282780 loss: 0.0039 lr: 0.02\n",
            "iteration: 282790 loss: 0.0038 lr: 0.02\n",
            "iteration: 282800 loss: 0.0028 lr: 0.02\n",
            "iteration: 282810 loss: 0.0038 lr: 0.02\n",
            "iteration: 282820 loss: 0.0032 lr: 0.02\n",
            "iteration: 282830 loss: 0.0034 lr: 0.02\n",
            "iteration: 282840 loss: 0.0025 lr: 0.02\n",
            "iteration: 282850 loss: 0.0024 lr: 0.02\n",
            "iteration: 282860 loss: 0.0033 lr: 0.02\n",
            "iteration: 282870 loss: 0.0041 lr: 0.02\n",
            "iteration: 282880 loss: 0.0045 lr: 0.02\n",
            "iteration: 282890 loss: 0.0031 lr: 0.02\n",
            "iteration: 282900 loss: 0.0034 lr: 0.02\n",
            "iteration: 282910 loss: 0.0027 lr: 0.02\n",
            "iteration: 282920 loss: 0.0029 lr: 0.02\n",
            "iteration: 282930 loss: 0.0034 lr: 0.02\n",
            "iteration: 282940 loss: 0.0043 lr: 0.02\n",
            "iteration: 282950 loss: 0.0040 lr: 0.02\n",
            "iteration: 282960 loss: 0.0034 lr: 0.02\n",
            "iteration: 282970 loss: 0.0044 lr: 0.02\n",
            "iteration: 282980 loss: 0.0027 lr: 0.02\n",
            "iteration: 282990 loss: 0.0034 lr: 0.02\n",
            "iteration: 283000 loss: 0.0036 lr: 0.02\n",
            "iteration: 283010 loss: 0.0033 lr: 0.02\n",
            "iteration: 283020 loss: 0.0042 lr: 0.02\n",
            "iteration: 283030 loss: 0.0031 lr: 0.02\n",
            "iteration: 283040 loss: 0.0036 lr: 0.02\n",
            "iteration: 283050 loss: 0.0049 lr: 0.02\n",
            "iteration: 283060 loss: 0.0050 lr: 0.02\n",
            "iteration: 283070 loss: 0.0046 lr: 0.02\n",
            "iteration: 283080 loss: 0.0029 lr: 0.02\n",
            "iteration: 283090 loss: 0.0043 lr: 0.02\n",
            "iteration: 283100 loss: 0.0041 lr: 0.02\n",
            "iteration: 283110 loss: 0.0030 lr: 0.02\n",
            "iteration: 283120 loss: 0.0031 lr: 0.02\n",
            "iteration: 283130 loss: 0.0023 lr: 0.02\n",
            "iteration: 283140 loss: 0.0030 lr: 0.02\n",
            "iteration: 283150 loss: 0.0035 lr: 0.02\n",
            "iteration: 283160 loss: 0.0029 lr: 0.02\n",
            "iteration: 283170 loss: 0.0030 lr: 0.02\n",
            "iteration: 283180 loss: 0.0030 lr: 0.02\n",
            "iteration: 283190 loss: 0.0036 lr: 0.02\n",
            "iteration: 283200 loss: 0.0041 lr: 0.02\n",
            "iteration: 283210 loss: 0.0039 lr: 0.02\n",
            "iteration: 283220 loss: 0.0030 lr: 0.02\n",
            "iteration: 283230 loss: 0.0038 lr: 0.02\n",
            "iteration: 283240 loss: 0.0027 lr: 0.02\n",
            "iteration: 283250 loss: 0.0028 lr: 0.02\n",
            "iteration: 283260 loss: 0.0026 lr: 0.02\n",
            "iteration: 283270 loss: 0.0046 lr: 0.02\n",
            "iteration: 283280 loss: 0.0038 lr: 0.02\n",
            "iteration: 283290 loss: 0.0035 lr: 0.02\n",
            "iteration: 283300 loss: 0.0039 lr: 0.02\n",
            "iteration: 283310 loss: 0.0030 lr: 0.02\n",
            "iteration: 283320 loss: 0.0043 lr: 0.02\n",
            "iteration: 283330 loss: 0.0031 lr: 0.02\n",
            "iteration: 283340 loss: 0.0033 lr: 0.02\n",
            "iteration: 283350 loss: 0.0042 lr: 0.02\n",
            "iteration: 283360 loss: 0.0040 lr: 0.02\n",
            "iteration: 283370 loss: 0.0042 lr: 0.02\n",
            "iteration: 283380 loss: 0.0040 lr: 0.02\n",
            "iteration: 283390 loss: 0.0040 lr: 0.02\n",
            "iteration: 283400 loss: 0.0030 lr: 0.02\n",
            "iteration: 283410 loss: 0.0032 lr: 0.02\n",
            "iteration: 283420 loss: 0.0028 lr: 0.02\n",
            "iteration: 283430 loss: 0.0030 lr: 0.02\n",
            "iteration: 283440 loss: 0.0054 lr: 0.02\n",
            "iteration: 283450 loss: 0.0036 lr: 0.02\n",
            "iteration: 283460 loss: 0.0036 lr: 0.02\n",
            "iteration: 283470 loss: 0.0028 lr: 0.02\n",
            "iteration: 283480 loss: 0.0031 lr: 0.02\n",
            "iteration: 283490 loss: 0.0038 lr: 0.02\n",
            "iteration: 283500 loss: 0.0025 lr: 0.02\n",
            "iteration: 283510 loss: 0.0042 lr: 0.02\n",
            "iteration: 283520 loss: 0.0033 lr: 0.02\n",
            "iteration: 283530 loss: 0.0038 lr: 0.02\n",
            "iteration: 283540 loss: 0.0026 lr: 0.02\n",
            "iteration: 283550 loss: 0.0033 lr: 0.02\n",
            "iteration: 283560 loss: 0.0040 lr: 0.02\n",
            "iteration: 283570 loss: 0.0036 lr: 0.02\n",
            "iteration: 283580 loss: 0.0042 lr: 0.02\n",
            "iteration: 283590 loss: 0.0036 lr: 0.02\n",
            "iteration: 283600 loss: 0.0027 lr: 0.02\n",
            "iteration: 283610 loss: 0.0029 lr: 0.02\n",
            "iteration: 283620 loss: 0.0034 lr: 0.02\n",
            "iteration: 283630 loss: 0.0040 lr: 0.02\n",
            "iteration: 283640 loss: 0.0045 lr: 0.02\n",
            "iteration: 283650 loss: 0.0045 lr: 0.02\n",
            "iteration: 283660 loss: 0.0030 lr: 0.02\n",
            "iteration: 283670 loss: 0.0042 lr: 0.02\n",
            "iteration: 283680 loss: 0.0033 lr: 0.02\n",
            "iteration: 283690 loss: 0.0024 lr: 0.02\n",
            "iteration: 283700 loss: 0.0041 lr: 0.02\n",
            "iteration: 283710 loss: 0.0037 lr: 0.02\n",
            "iteration: 283720 loss: 0.0029 lr: 0.02\n",
            "iteration: 283730 loss: 0.0040 lr: 0.02\n",
            "iteration: 283740 loss: 0.0029 lr: 0.02\n",
            "iteration: 283750 loss: 0.0025 lr: 0.02\n",
            "iteration: 283760 loss: 0.0041 lr: 0.02\n",
            "iteration: 283770 loss: 0.0042 lr: 0.02\n",
            "iteration: 283780 loss: 0.0035 lr: 0.02\n",
            "iteration: 283790 loss: 0.0036 lr: 0.02\n",
            "iteration: 283800 loss: 0.0035 lr: 0.02\n",
            "iteration: 283810 loss: 0.0033 lr: 0.02\n",
            "iteration: 283820 loss: 0.0040 lr: 0.02\n",
            "iteration: 283830 loss: 0.0030 lr: 0.02\n",
            "iteration: 283840 loss: 0.0036 lr: 0.02\n",
            "iteration: 283850 loss: 0.0040 lr: 0.02\n",
            "iteration: 283860 loss: 0.0036 lr: 0.02\n",
            "iteration: 283870 loss: 0.0030 lr: 0.02\n",
            "iteration: 283880 loss: 0.0048 lr: 0.02\n",
            "iteration: 283890 loss: 0.0041 lr: 0.02\n",
            "iteration: 283900 loss: 0.0035 lr: 0.02\n",
            "iteration: 283910 loss: 0.0039 lr: 0.02\n",
            "iteration: 283920 loss: 0.0028 lr: 0.02\n",
            "iteration: 283930 loss: 0.0033 lr: 0.02\n",
            "iteration: 283940 loss: 0.0053 lr: 0.02\n",
            "iteration: 283950 loss: 0.0029 lr: 0.02\n",
            "iteration: 283960 loss: 0.0030 lr: 0.02\n",
            "iteration: 283970 loss: 0.0033 lr: 0.02\n",
            "iteration: 283980 loss: 0.0041 lr: 0.02\n",
            "iteration: 283990 loss: 0.0033 lr: 0.02\n",
            "iteration: 284000 loss: 0.0039 lr: 0.02\n",
            "iteration: 284010 loss: 0.0040 lr: 0.02\n",
            "iteration: 284020 loss: 0.0025 lr: 0.02\n",
            "iteration: 284030 loss: 0.0026 lr: 0.02\n",
            "iteration: 284040 loss: 0.0034 lr: 0.02\n",
            "iteration: 284050 loss: 0.0030 lr: 0.02\n",
            "iteration: 284060 loss: 0.0026 lr: 0.02\n",
            "iteration: 284070 loss: 0.0030 lr: 0.02\n",
            "iteration: 284080 loss: 0.0037 lr: 0.02\n",
            "iteration: 284090 loss: 0.0039 lr: 0.02\n",
            "iteration: 284100 loss: 0.0061 lr: 0.02\n",
            "iteration: 284110 loss: 0.0033 lr: 0.02\n",
            "iteration: 284120 loss: 0.0033 lr: 0.02\n",
            "iteration: 284130 loss: 0.0032 lr: 0.02\n",
            "iteration: 284140 loss: 0.0038 lr: 0.02\n",
            "iteration: 284150 loss: 0.0035 lr: 0.02\n",
            "iteration: 284160 loss: 0.0039 lr: 0.02\n",
            "iteration: 284170 loss: 0.0030 lr: 0.02\n",
            "iteration: 284180 loss: 0.0028 lr: 0.02\n",
            "iteration: 284190 loss: 0.0035 lr: 0.02\n",
            "iteration: 284200 loss: 0.0037 lr: 0.02\n",
            "iteration: 284210 loss: 0.0038 lr: 0.02\n",
            "iteration: 284220 loss: 0.0039 lr: 0.02\n",
            "iteration: 284230 loss: 0.0042 lr: 0.02\n",
            "iteration: 284240 loss: 0.0048 lr: 0.02\n",
            "iteration: 284250 loss: 0.0045 lr: 0.02\n",
            "iteration: 284260 loss: 0.0033 lr: 0.02\n",
            "iteration: 284270 loss: 0.0031 lr: 0.02\n",
            "iteration: 284280 loss: 0.0033 lr: 0.02\n",
            "iteration: 284290 loss: 0.0037 lr: 0.02\n",
            "iteration: 284300 loss: 0.0035 lr: 0.02\n",
            "iteration: 284310 loss: 0.0035 lr: 0.02\n",
            "iteration: 284320 loss: 0.0032 lr: 0.02\n",
            "iteration: 284330 loss: 0.0027 lr: 0.02\n",
            "iteration: 284340 loss: 0.0032 lr: 0.02\n",
            "iteration: 284350 loss: 0.0041 lr: 0.02\n",
            "iteration: 284360 loss: 0.0034 lr: 0.02\n",
            "iteration: 284370 loss: 0.0043 lr: 0.02\n",
            "iteration: 284380 loss: 0.0038 lr: 0.02\n",
            "iteration: 284390 loss: 0.0039 lr: 0.02\n",
            "iteration: 284400 loss: 0.0036 lr: 0.02\n",
            "iteration: 284410 loss: 0.0038 lr: 0.02\n",
            "iteration: 284420 loss: 0.0038 lr: 0.02\n",
            "iteration: 284430 loss: 0.0036 lr: 0.02\n",
            "iteration: 284440 loss: 0.0027 lr: 0.02\n",
            "iteration: 284450 loss: 0.0037 lr: 0.02\n",
            "iteration: 284460 loss: 0.0032 lr: 0.02\n",
            "iteration: 284470 loss: 0.0038 lr: 0.02\n",
            "iteration: 284480 loss: 0.0035 lr: 0.02\n",
            "iteration: 284490 loss: 0.0043 lr: 0.02\n",
            "iteration: 284500 loss: 0.0041 lr: 0.02\n",
            "iteration: 284510 loss: 0.0039 lr: 0.02\n",
            "iteration: 284520 loss: 0.0040 lr: 0.02\n",
            "iteration: 284530 loss: 0.0038 lr: 0.02\n",
            "iteration: 284540 loss: 0.0041 lr: 0.02\n",
            "iteration: 284550 loss: 0.0030 lr: 0.02\n",
            "iteration: 284560 loss: 0.0031 lr: 0.02\n",
            "iteration: 284570 loss: 0.0030 lr: 0.02\n",
            "iteration: 284580 loss: 0.0035 lr: 0.02\n",
            "iteration: 284590 loss: 0.0031 lr: 0.02\n",
            "iteration: 284600 loss: 0.0029 lr: 0.02\n",
            "iteration: 284610 loss: 0.0033 lr: 0.02\n",
            "iteration: 284620 loss: 0.0029 lr: 0.02\n",
            "iteration: 284630 loss: 0.0043 lr: 0.02\n",
            "iteration: 284640 loss: 0.0042 lr: 0.02\n",
            "iteration: 284650 loss: 0.0048 lr: 0.02\n",
            "iteration: 284660 loss: 0.0040 lr: 0.02\n",
            "iteration: 284670 loss: 0.0029 lr: 0.02\n",
            "iteration: 284680 loss: 0.0045 lr: 0.02\n",
            "iteration: 284690 loss: 0.0040 lr: 0.02\n",
            "iteration: 284700 loss: 0.0035 lr: 0.02\n",
            "iteration: 284710 loss: 0.0036 lr: 0.02\n",
            "iteration: 284720 loss: 0.0046 lr: 0.02\n",
            "iteration: 284730 loss: 0.0041 lr: 0.02\n",
            "iteration: 284740 loss: 0.0032 lr: 0.02\n",
            "iteration: 284750 loss: 0.0040 lr: 0.02\n",
            "iteration: 284760 loss: 0.0026 lr: 0.02\n",
            "iteration: 284770 loss: 0.0042 lr: 0.02\n",
            "iteration: 284780 loss: 0.0053 lr: 0.02\n",
            "iteration: 284790 loss: 0.0034 lr: 0.02\n",
            "iteration: 284800 loss: 0.0038 lr: 0.02\n",
            "iteration: 284810 loss: 0.0029 lr: 0.02\n",
            "iteration: 284820 loss: 0.0035 lr: 0.02\n",
            "iteration: 284830 loss: 0.0033 lr: 0.02\n",
            "iteration: 284840 loss: 0.0039 lr: 0.02\n",
            "iteration: 284850 loss: 0.0029 lr: 0.02\n",
            "iteration: 284860 loss: 0.0031 lr: 0.02\n",
            "iteration: 284870 loss: 0.0034 lr: 0.02\n",
            "iteration: 284880 loss: 0.0031 lr: 0.02\n",
            "iteration: 284890 loss: 0.0037 lr: 0.02\n",
            "iteration: 284900 loss: 0.0029 lr: 0.02\n",
            "iteration: 284910 loss: 0.0031 lr: 0.02\n",
            "iteration: 284920 loss: 0.0029 lr: 0.02\n",
            "iteration: 284930 loss: 0.0044 lr: 0.02\n",
            "iteration: 284940 loss: 0.0043 lr: 0.02\n",
            "iteration: 284950 loss: 0.0049 lr: 0.02\n",
            "iteration: 284960 loss: 0.0035 lr: 0.02\n",
            "iteration: 284970 loss: 0.0044 lr: 0.02\n",
            "iteration: 284980 loss: 0.0030 lr: 0.02\n",
            "iteration: 284990 loss: 0.0046 lr: 0.02\n",
            "iteration: 285000 loss: 0.0031 lr: 0.02\n",
            "iteration: 285010 loss: 0.0030 lr: 0.02\n",
            "iteration: 285020 loss: 0.0030 lr: 0.02\n",
            "iteration: 285030 loss: 0.0040 lr: 0.02\n",
            "iteration: 285040 loss: 0.0031 lr: 0.02\n",
            "iteration: 285050 loss: 0.0040 lr: 0.02\n",
            "iteration: 285060 loss: 0.0037 lr: 0.02\n",
            "iteration: 285070 loss: 0.0028 lr: 0.02\n",
            "iteration: 285080 loss: 0.0037 lr: 0.02\n",
            "iteration: 285090 loss: 0.0044 lr: 0.02\n",
            "iteration: 285100 loss: 0.0028 lr: 0.02\n",
            "iteration: 285110 loss: 0.0032 lr: 0.02\n",
            "iteration: 285120 loss: 0.0030 lr: 0.02\n",
            "iteration: 285130 loss: 0.0032 lr: 0.02\n",
            "iteration: 285140 loss: 0.0038 lr: 0.02\n",
            "iteration: 285150 loss: 0.0040 lr: 0.02\n",
            "iteration: 285160 loss: 0.0035 lr: 0.02\n",
            "iteration: 285170 loss: 0.0036 lr: 0.02\n",
            "iteration: 285180 loss: 0.0030 lr: 0.02\n",
            "iteration: 285190 loss: 0.0036 lr: 0.02\n",
            "iteration: 285200 loss: 0.0039 lr: 0.02\n",
            "iteration: 285210 loss: 0.0035 lr: 0.02\n",
            "iteration: 285220 loss: 0.0039 lr: 0.02\n",
            "iteration: 285230 loss: 0.0037 lr: 0.02\n",
            "iteration: 285240 loss: 0.0032 lr: 0.02\n",
            "iteration: 285250 loss: 0.0037 lr: 0.02\n",
            "iteration: 285260 loss: 0.0032 lr: 0.02\n",
            "iteration: 285270 loss: 0.0035 lr: 0.02\n",
            "iteration: 285280 loss: 0.0029 lr: 0.02\n",
            "iteration: 285290 loss: 0.0046 lr: 0.02\n",
            "iteration: 285300 loss: 0.0037 lr: 0.02\n",
            "iteration: 285310 loss: 0.0057 lr: 0.02\n",
            "iteration: 285320 loss: 0.0036 lr: 0.02\n",
            "iteration: 285330 loss: 0.0038 lr: 0.02\n",
            "iteration: 285340 loss: 0.0027 lr: 0.02\n",
            "iteration: 285350 loss: 0.0034 lr: 0.02\n",
            "iteration: 285360 loss: 0.0048 lr: 0.02\n",
            "iteration: 285370 loss: 0.0035 lr: 0.02\n",
            "iteration: 285380 loss: 0.0022 lr: 0.02\n",
            "iteration: 285390 loss: 0.0031 lr: 0.02\n",
            "iteration: 285400 loss: 0.0036 lr: 0.02\n",
            "iteration: 285410 loss: 0.0030 lr: 0.02\n",
            "iteration: 285420 loss: 0.0033 lr: 0.02\n",
            "iteration: 285430 loss: 0.0028 lr: 0.02\n",
            "iteration: 285440 loss: 0.0042 lr: 0.02\n",
            "iteration: 285450 loss: 0.0034 lr: 0.02\n",
            "iteration: 285460 loss: 0.0034 lr: 0.02\n",
            "iteration: 285470 loss: 0.0038 lr: 0.02\n",
            "iteration: 285480 loss: 0.0047 lr: 0.02\n",
            "iteration: 285490 loss: 0.0045 lr: 0.02\n",
            "iteration: 285500 loss: 0.0034 lr: 0.02\n",
            "iteration: 285510 loss: 0.0030 lr: 0.02\n",
            "iteration: 285520 loss: 0.0036 lr: 0.02\n",
            "iteration: 285530 loss: 0.0047 lr: 0.02\n",
            "iteration: 285540 loss: 0.0036 lr: 0.02\n",
            "iteration: 285550 loss: 0.0032 lr: 0.02\n",
            "iteration: 285560 loss: 0.0036 lr: 0.02\n",
            "iteration: 285570 loss: 0.0035 lr: 0.02\n",
            "iteration: 285580 loss: 0.0029 lr: 0.02\n",
            "iteration: 285590 loss: 0.0044 lr: 0.02\n",
            "iteration: 285600 loss: 0.0033 lr: 0.02\n",
            "iteration: 285610 loss: 0.0036 lr: 0.02\n",
            "iteration: 285620 loss: 0.0026 lr: 0.02\n",
            "iteration: 285630 loss: 0.0032 lr: 0.02\n",
            "iteration: 285640 loss: 0.0034 lr: 0.02\n",
            "iteration: 285650 loss: 0.0043 lr: 0.02\n",
            "iteration: 285660 loss: 0.0031 lr: 0.02\n",
            "iteration: 285670 loss: 0.0030 lr: 0.02\n",
            "iteration: 285680 loss: 0.0033 lr: 0.02\n",
            "iteration: 285690 loss: 0.0044 lr: 0.02\n",
            "iteration: 285700 loss: 0.0032 lr: 0.02\n",
            "iteration: 285710 loss: 0.0039 lr: 0.02\n",
            "iteration: 285720 loss: 0.0038 lr: 0.02\n",
            "iteration: 285730 loss: 0.0035 lr: 0.02\n",
            "iteration: 285740 loss: 0.0030 lr: 0.02\n",
            "iteration: 285750 loss: 0.0039 lr: 0.02\n",
            "iteration: 285760 loss: 0.0041 lr: 0.02\n",
            "iteration: 285770 loss: 0.0039 lr: 0.02\n",
            "iteration: 285780 loss: 0.0032 lr: 0.02\n",
            "iteration: 285790 loss: 0.0036 lr: 0.02\n",
            "iteration: 285800 loss: 0.0041 lr: 0.02\n",
            "iteration: 285810 loss: 0.0032 lr: 0.02\n",
            "iteration: 285820 loss: 0.0032 lr: 0.02\n",
            "iteration: 285830 loss: 0.0031 lr: 0.02\n",
            "iteration: 285840 loss: 0.0030 lr: 0.02\n",
            "iteration: 285850 loss: 0.0026 lr: 0.02\n",
            "iteration: 285860 loss: 0.0028 lr: 0.02\n",
            "iteration: 285870 loss: 0.0033 lr: 0.02\n",
            "iteration: 285880 loss: 0.0035 lr: 0.02\n",
            "iteration: 285890 loss: 0.0042 lr: 0.02\n",
            "iteration: 285900 loss: 0.0060 lr: 0.02\n",
            "iteration: 285910 loss: 0.0031 lr: 0.02\n",
            "iteration: 285920 loss: 0.0039 lr: 0.02\n",
            "iteration: 285930 loss: 0.0047 lr: 0.02\n",
            "iteration: 285940 loss: 0.0042 lr: 0.02\n",
            "iteration: 285950 loss: 0.0034 lr: 0.02\n",
            "iteration: 285960 loss: 0.0043 lr: 0.02\n",
            "iteration: 285970 loss: 0.0040 lr: 0.02\n",
            "iteration: 285980 loss: 0.0035 lr: 0.02\n",
            "iteration: 285990 loss: 0.0035 lr: 0.02\n",
            "iteration: 286000 loss: 0.0036 lr: 0.02\n",
            "iteration: 286010 loss: 0.0045 lr: 0.02\n",
            "iteration: 286020 loss: 0.0032 lr: 0.02\n",
            "iteration: 286030 loss: 0.0036 lr: 0.02\n",
            "iteration: 286040 loss: 0.0031 lr: 0.02\n",
            "iteration: 286050 loss: 0.0047 lr: 0.02\n",
            "iteration: 286060 loss: 0.0047 lr: 0.02\n",
            "iteration: 286070 loss: 0.0030 lr: 0.02\n",
            "iteration: 286080 loss: 0.0037 lr: 0.02\n",
            "iteration: 286090 loss: 0.0034 lr: 0.02\n",
            "iteration: 286100 loss: 0.0026 lr: 0.02\n",
            "iteration: 286110 loss: 0.0027 lr: 0.02\n",
            "iteration: 286120 loss: 0.0035 lr: 0.02\n",
            "iteration: 286130 loss: 0.0041 lr: 0.02\n",
            "iteration: 286140 loss: 0.0030 lr: 0.02\n",
            "iteration: 286150 loss: 0.0032 lr: 0.02\n",
            "iteration: 286160 loss: 0.0033 lr: 0.02\n",
            "iteration: 286170 loss: 0.0033 lr: 0.02\n",
            "iteration: 286180 loss: 0.0042 lr: 0.02\n",
            "iteration: 286190 loss: 0.0036 lr: 0.02\n",
            "iteration: 286200 loss: 0.0035 lr: 0.02\n",
            "iteration: 286210 loss: 0.0041 lr: 0.02\n",
            "iteration: 286220 loss: 0.0033 lr: 0.02\n",
            "iteration: 286230 loss: 0.0029 lr: 0.02\n",
            "iteration: 286240 loss: 0.0036 lr: 0.02\n",
            "iteration: 286250 loss: 0.0043 lr: 0.02\n",
            "iteration: 286260 loss: 0.0043 lr: 0.02\n",
            "iteration: 286270 loss: 0.0029 lr: 0.02\n",
            "iteration: 286280 loss: 0.0025 lr: 0.02\n",
            "iteration: 286290 loss: 0.0036 lr: 0.02\n",
            "iteration: 286300 loss: 0.0034 lr: 0.02\n",
            "iteration: 286310 loss: 0.0027 lr: 0.02\n",
            "iteration: 286320 loss: 0.0039 lr: 0.02\n",
            "iteration: 286330 loss: 0.0035 lr: 0.02\n",
            "iteration: 286340 loss: 0.0041 lr: 0.02\n",
            "iteration: 286350 loss: 0.0035 lr: 0.02\n",
            "iteration: 286360 loss: 0.0035 lr: 0.02\n",
            "iteration: 286370 loss: 0.0029 lr: 0.02\n",
            "iteration: 286380 loss: 0.0037 lr: 0.02\n",
            "iteration: 286390 loss: 0.0038 lr: 0.02\n",
            "iteration: 286400 loss: 0.0038 lr: 0.02\n",
            "iteration: 286410 loss: 0.0030 lr: 0.02\n",
            "iteration: 286420 loss: 0.0037 lr: 0.02\n",
            "iteration: 286430 loss: 0.0044 lr: 0.02\n",
            "iteration: 286440 loss: 0.0035 lr: 0.02\n",
            "iteration: 286450 loss: 0.0039 lr: 0.02\n",
            "iteration: 286460 loss: 0.0033 lr: 0.02\n",
            "iteration: 286470 loss: 0.0038 lr: 0.02\n",
            "iteration: 286480 loss: 0.0029 lr: 0.02\n",
            "iteration: 286490 loss: 0.0044 lr: 0.02\n",
            "iteration: 286500 loss: 0.0037 lr: 0.02\n",
            "iteration: 286510 loss: 0.0037 lr: 0.02\n",
            "iteration: 286520 loss: 0.0036 lr: 0.02\n",
            "iteration: 286530 loss: 0.0038 lr: 0.02\n",
            "iteration: 286540 loss: 0.0041 lr: 0.02\n",
            "iteration: 286550 loss: 0.0032 lr: 0.02\n",
            "iteration: 286560 loss: 0.0037 lr: 0.02\n",
            "iteration: 286570 loss: 0.0037 lr: 0.02\n",
            "iteration: 286580 loss: 0.0037 lr: 0.02\n",
            "iteration: 286590 loss: 0.0030 lr: 0.02\n",
            "iteration: 286600 loss: 0.0032 lr: 0.02\n",
            "iteration: 286610 loss: 0.0038 lr: 0.02\n",
            "iteration: 286620 loss: 0.0036 lr: 0.02\n",
            "iteration: 286630 loss: 0.0025 lr: 0.02\n",
            "iteration: 286640 loss: 0.0029 lr: 0.02\n",
            "iteration: 286650 loss: 0.0033 lr: 0.02\n",
            "iteration: 286660 loss: 0.0040 lr: 0.02\n",
            "iteration: 286670 loss: 0.0038 lr: 0.02\n",
            "iteration: 286680 loss: 0.0029 lr: 0.02\n",
            "iteration: 286690 loss: 0.0034 lr: 0.02\n",
            "iteration: 286700 loss: 0.0031 lr: 0.02\n",
            "iteration: 286710 loss: 0.0037 lr: 0.02\n",
            "iteration: 286720 loss: 0.0035 lr: 0.02\n",
            "iteration: 286730 loss: 0.0041 lr: 0.02\n",
            "iteration: 286740 loss: 0.0033 lr: 0.02\n",
            "iteration: 286750 loss: 0.0052 lr: 0.02\n",
            "iteration: 286760 loss: 0.0046 lr: 0.02\n",
            "iteration: 286770 loss: 0.0033 lr: 0.02\n",
            "iteration: 286780 loss: 0.0041 lr: 0.02\n",
            "iteration: 286790 loss: 0.0038 lr: 0.02\n",
            "iteration: 286800 loss: 0.0038 lr: 0.02\n",
            "iteration: 286810 loss: 0.0045 lr: 0.02\n",
            "iteration: 286820 loss: 0.0040 lr: 0.02\n",
            "iteration: 286830 loss: 0.0029 lr: 0.02\n",
            "iteration: 286840 loss: 0.0034 lr: 0.02\n",
            "iteration: 286850 loss: 0.0036 lr: 0.02\n",
            "iteration: 286860 loss: 0.0034 lr: 0.02\n",
            "iteration: 286870 loss: 0.0029 lr: 0.02\n",
            "iteration: 286880 loss: 0.0044 lr: 0.02\n",
            "iteration: 286890 loss: 0.0036 lr: 0.02\n",
            "iteration: 286900 loss: 0.0032 lr: 0.02\n",
            "iteration: 286910 loss: 0.0030 lr: 0.02\n",
            "iteration: 286920 loss: 0.0038 lr: 0.02\n",
            "iteration: 286930 loss: 0.0037 lr: 0.02\n",
            "iteration: 286940 loss: 0.0040 lr: 0.02\n",
            "iteration: 286950 loss: 0.0031 lr: 0.02\n",
            "iteration: 286960 loss: 0.0041 lr: 0.02\n",
            "iteration: 286970 loss: 0.0034 lr: 0.02\n",
            "iteration: 286980 loss: 0.0035 lr: 0.02\n",
            "iteration: 286990 loss: 0.0024 lr: 0.02\n",
            "iteration: 287000 loss: 0.0033 lr: 0.02\n",
            "iteration: 287010 loss: 0.0040 lr: 0.02\n",
            "iteration: 287020 loss: 0.0030 lr: 0.02\n",
            "iteration: 287030 loss: 0.0035 lr: 0.02\n",
            "iteration: 287040 loss: 0.0034 lr: 0.02\n",
            "iteration: 287050 loss: 0.0053 lr: 0.02\n",
            "iteration: 287060 loss: 0.0037 lr: 0.02\n",
            "iteration: 287070 loss: 0.0024 lr: 0.02\n",
            "iteration: 287080 loss: 0.0030 lr: 0.02\n",
            "iteration: 287090 loss: 0.0041 lr: 0.02\n",
            "iteration: 287100 loss: 0.0027 lr: 0.02\n",
            "iteration: 287110 loss: 0.0036 lr: 0.02\n",
            "iteration: 287120 loss: 0.0037 lr: 0.02\n",
            "iteration: 287130 loss: 0.0035 lr: 0.02\n",
            "iteration: 287140 loss: 0.0032 lr: 0.02\n",
            "iteration: 287150 loss: 0.0028 lr: 0.02\n",
            "iteration: 287160 loss: 0.0030 lr: 0.02\n",
            "iteration: 287170 loss: 0.0028 lr: 0.02\n",
            "iteration: 287180 loss: 0.0041 lr: 0.02\n",
            "iteration: 287190 loss: 0.0037 lr: 0.02\n",
            "iteration: 287200 loss: 0.0054 lr: 0.02\n",
            "iteration: 287210 loss: 0.0043 lr: 0.02\n",
            "iteration: 287220 loss: 0.0036 lr: 0.02\n",
            "iteration: 287230 loss: 0.0030 lr: 0.02\n",
            "iteration: 287240 loss: 0.0034 lr: 0.02\n",
            "iteration: 287250 loss: 0.0028 lr: 0.02\n",
            "iteration: 287260 loss: 0.0033 lr: 0.02\n",
            "iteration: 287270 loss: 0.0030 lr: 0.02\n",
            "iteration: 287280 loss: 0.0038 lr: 0.02\n",
            "iteration: 287290 loss: 0.0036 lr: 0.02\n",
            "iteration: 287300 loss: 0.0036 lr: 0.02\n",
            "iteration: 287310 loss: 0.0039 lr: 0.02\n",
            "iteration: 287320 loss: 0.0034 lr: 0.02\n",
            "iteration: 287330 loss: 0.0038 lr: 0.02\n",
            "iteration: 287340 loss: 0.0039 lr: 0.02\n",
            "iteration: 287350 loss: 0.0040 lr: 0.02\n",
            "iteration: 287360 loss: 0.0028 lr: 0.02\n",
            "iteration: 287370 loss: 0.0038 lr: 0.02\n",
            "iteration: 287380 loss: 0.0032 lr: 0.02\n",
            "iteration: 287390 loss: 0.0030 lr: 0.02\n",
            "iteration: 287400 loss: 0.0037 lr: 0.02\n",
            "iteration: 287410 loss: 0.0068 lr: 0.02\n",
            "iteration: 287420 loss: 0.0044 lr: 0.02\n",
            "iteration: 287430 loss: 0.0038 lr: 0.02\n",
            "iteration: 287440 loss: 0.0043 lr: 0.02\n",
            "iteration: 287450 loss: 0.0032 lr: 0.02\n",
            "iteration: 287460 loss: 0.0032 lr: 0.02\n",
            "iteration: 287470 loss: 0.0038 lr: 0.02\n",
            "iteration: 287480 loss: 0.0031 lr: 0.02\n",
            "iteration: 287490 loss: 0.0048 lr: 0.02\n",
            "iteration: 287500 loss: 0.0026 lr: 0.02\n",
            "iteration: 287510 loss: 0.0037 lr: 0.02\n",
            "iteration: 287520 loss: 0.0033 lr: 0.02\n",
            "iteration: 287530 loss: 0.0032 lr: 0.02\n",
            "iteration: 287540 loss: 0.0042 lr: 0.02\n",
            "iteration: 287550 loss: 0.0037 lr: 0.02\n",
            "iteration: 287560 loss: 0.0033 lr: 0.02\n",
            "iteration: 287570 loss: 0.0043 lr: 0.02\n",
            "iteration: 287580 loss: 0.0038 lr: 0.02\n",
            "iteration: 287590 loss: 0.0049 lr: 0.02\n",
            "iteration: 287600 loss: 0.0040 lr: 0.02\n",
            "iteration: 287610 loss: 0.0042 lr: 0.02\n",
            "iteration: 287620 loss: 0.0034 lr: 0.02\n",
            "iteration: 287630 loss: 0.0035 lr: 0.02\n",
            "iteration: 287640 loss: 0.0038 lr: 0.02\n",
            "iteration: 287650 loss: 0.0035 lr: 0.02\n",
            "iteration: 287660 loss: 0.0032 lr: 0.02\n",
            "iteration: 287670 loss: 0.0036 lr: 0.02\n",
            "iteration: 287680 loss: 0.0032 lr: 0.02\n",
            "iteration: 287690 loss: 0.0032 lr: 0.02\n",
            "iteration: 287700 loss: 0.0039 lr: 0.02\n",
            "iteration: 287710 loss: 0.0032 lr: 0.02\n",
            "iteration: 287720 loss: 0.0029 lr: 0.02\n",
            "iteration: 287730 loss: 0.0037 lr: 0.02\n",
            "iteration: 287740 loss: 0.0035 lr: 0.02\n",
            "iteration: 287750 loss: 0.0036 lr: 0.02\n",
            "iteration: 287760 loss: 0.0034 lr: 0.02\n",
            "iteration: 287770 loss: 0.0038 lr: 0.02\n",
            "iteration: 287780 loss: 0.0040 lr: 0.02\n",
            "iteration: 287790 loss: 0.0036 lr: 0.02\n",
            "iteration: 287800 loss: 0.0037 lr: 0.02\n",
            "iteration: 287810 loss: 0.0026 lr: 0.02\n",
            "iteration: 287820 loss: 0.0030 lr: 0.02\n",
            "iteration: 287830 loss: 0.0036 lr: 0.02\n",
            "iteration: 287840 loss: 0.0029 lr: 0.02\n",
            "iteration: 287850 loss: 0.0042 lr: 0.02\n",
            "iteration: 287860 loss: 0.0032 lr: 0.02\n",
            "iteration: 287870 loss: 0.0025 lr: 0.02\n",
            "iteration: 287880 loss: 0.0036 lr: 0.02\n",
            "iteration: 287890 loss: 0.0036 lr: 0.02\n",
            "iteration: 287900 loss: 0.0040 lr: 0.02\n",
            "iteration: 287910 loss: 0.0028 lr: 0.02\n",
            "iteration: 287920 loss: 0.0035 lr: 0.02\n",
            "iteration: 287930 loss: 0.0035 lr: 0.02\n",
            "iteration: 287940 loss: 0.0033 lr: 0.02\n",
            "iteration: 287950 loss: 0.0036 lr: 0.02\n",
            "iteration: 287960 loss: 0.0045 lr: 0.02\n",
            "iteration: 287970 loss: 0.0025 lr: 0.02\n",
            "iteration: 287980 loss: 0.0036 lr: 0.02\n",
            "iteration: 287990 loss: 0.0045 lr: 0.02\n",
            "iteration: 288000 loss: 0.0052 lr: 0.02\n",
            "iteration: 288010 loss: 0.0027 lr: 0.02\n",
            "iteration: 288020 loss: 0.0029 lr: 0.02\n",
            "iteration: 288030 loss: 0.0039 lr: 0.02\n",
            "iteration: 288040 loss: 0.0031 lr: 0.02\n",
            "iteration: 288050 loss: 0.0032 lr: 0.02\n",
            "iteration: 288060 loss: 0.0036 lr: 0.02\n",
            "iteration: 288070 loss: 0.0038 lr: 0.02\n",
            "iteration: 288080 loss: 0.0031 lr: 0.02\n",
            "iteration: 288090 loss: 0.0040 lr: 0.02\n",
            "iteration: 288100 loss: 0.0040 lr: 0.02\n",
            "iteration: 288110 loss: 0.0040 lr: 0.02\n",
            "iteration: 288120 loss: 0.0030 lr: 0.02\n",
            "iteration: 288130 loss: 0.0052 lr: 0.02\n",
            "iteration: 288140 loss: 0.0031 lr: 0.02\n",
            "iteration: 288150 loss: 0.0027 lr: 0.02\n",
            "iteration: 288160 loss: 0.0032 lr: 0.02\n",
            "iteration: 288170 loss: 0.0036 lr: 0.02\n",
            "iteration: 288180 loss: 0.0043 lr: 0.02\n",
            "iteration: 288190 loss: 0.0039 lr: 0.02\n",
            "iteration: 288200 loss: 0.0038 lr: 0.02\n",
            "iteration: 288210 loss: 0.0035 lr: 0.02\n",
            "iteration: 288220 loss: 0.0034 lr: 0.02\n",
            "iteration: 288230 loss: 0.0034 lr: 0.02\n",
            "iteration: 288240 loss: 0.0036 lr: 0.02\n",
            "iteration: 288250 loss: 0.0024 lr: 0.02\n",
            "iteration: 288260 loss: 0.0026 lr: 0.02\n",
            "iteration: 288270 loss: 0.0033 lr: 0.02\n",
            "iteration: 288280 loss: 0.0034 lr: 0.02\n",
            "iteration: 288290 loss: 0.0040 lr: 0.02\n",
            "iteration: 288300 loss: 0.0032 lr: 0.02\n",
            "iteration: 288310 loss: 0.0043 lr: 0.02\n",
            "iteration: 288320 loss: 0.0035 lr: 0.02\n",
            "iteration: 288330 loss: 0.0037 lr: 0.02\n",
            "iteration: 288340 loss: 0.0023 lr: 0.02\n",
            "iteration: 288350 loss: 0.0032 lr: 0.02\n",
            "iteration: 288360 loss: 0.0032 lr: 0.02\n",
            "iteration: 288370 loss: 0.0029 lr: 0.02\n",
            "iteration: 288380 loss: 0.0035 lr: 0.02\n",
            "iteration: 288390 loss: 0.0041 lr: 0.02\n",
            "iteration: 288400 loss: 0.0040 lr: 0.02\n",
            "iteration: 288410 loss: 0.0035 lr: 0.02\n",
            "iteration: 288420 loss: 0.0039 lr: 0.02\n",
            "iteration: 288430 loss: 0.0037 lr: 0.02\n",
            "iteration: 288440 loss: 0.0037 lr: 0.02\n",
            "iteration: 288450 loss: 0.0027 lr: 0.02\n",
            "iteration: 288460 loss: 0.0039 lr: 0.02\n",
            "iteration: 288470 loss: 0.0044 lr: 0.02\n",
            "iteration: 288480 loss: 0.0040 lr: 0.02\n",
            "iteration: 288490 loss: 0.0029 lr: 0.02\n",
            "iteration: 288500 loss: 0.0030 lr: 0.02\n",
            "iteration: 288510 loss: 0.0043 lr: 0.02\n",
            "iteration: 288520 loss: 0.0033 lr: 0.02\n",
            "iteration: 288530 loss: 0.0031 lr: 0.02\n",
            "iteration: 288540 loss: 0.0038 lr: 0.02\n",
            "iteration: 288550 loss: 0.0041 lr: 0.02\n",
            "iteration: 288560 loss: 0.0036 lr: 0.02\n",
            "iteration: 288570 loss: 0.0039 lr: 0.02\n",
            "iteration: 288580 loss: 0.0033 lr: 0.02\n",
            "iteration: 288590 loss: 0.0042 lr: 0.02\n",
            "iteration: 288600 loss: 0.0043 lr: 0.02\n",
            "iteration: 288610 loss: 0.0040 lr: 0.02\n",
            "iteration: 288620 loss: 0.0038 lr: 0.02\n",
            "iteration: 288630 loss: 0.0036 lr: 0.02\n",
            "iteration: 288640 loss: 0.0043 lr: 0.02\n",
            "iteration: 288650 loss: 0.0033 lr: 0.02\n",
            "iteration: 288660 loss: 0.0032 lr: 0.02\n",
            "iteration: 288670 loss: 0.0034 lr: 0.02\n",
            "iteration: 288680 loss: 0.0034 lr: 0.02\n",
            "iteration: 288690 loss: 0.0036 lr: 0.02\n",
            "iteration: 288700 loss: 0.0029 lr: 0.02\n",
            "iteration: 288710 loss: 0.0039 lr: 0.02\n",
            "iteration: 288720 loss: 0.0029 lr: 0.02\n",
            "iteration: 288730 loss: 0.0039 lr: 0.02\n",
            "iteration: 288740 loss: 0.0034 lr: 0.02\n",
            "iteration: 288750 loss: 0.0042 lr: 0.02\n",
            "iteration: 288760 loss: 0.0032 lr: 0.02\n",
            "iteration: 288770 loss: 0.0039 lr: 0.02\n",
            "iteration: 288780 loss: 0.0039 lr: 0.02\n",
            "iteration: 288790 loss: 0.0046 lr: 0.02\n",
            "iteration: 288800 loss: 0.0043 lr: 0.02\n",
            "iteration: 288810 loss: 0.0029 lr: 0.02\n",
            "iteration: 288820 loss: 0.0031 lr: 0.02\n",
            "iteration: 288830 loss: 0.0026 lr: 0.02\n",
            "iteration: 288840 loss: 0.0035 lr: 0.02\n",
            "iteration: 288850 loss: 0.0040 lr: 0.02\n",
            "iteration: 288860 loss: 0.0033 lr: 0.02\n",
            "iteration: 288870 loss: 0.0026 lr: 0.02\n",
            "iteration: 288880 loss: 0.0040 lr: 0.02\n",
            "iteration: 288890 loss: 0.0029 lr: 0.02\n",
            "iteration: 288900 loss: 0.0027 lr: 0.02\n",
            "iteration: 288910 loss: 0.0035 lr: 0.02\n",
            "iteration: 288920 loss: 0.0024 lr: 0.02\n",
            "iteration: 288930 loss: 0.0043 lr: 0.02\n",
            "iteration: 288940 loss: 0.0030 lr: 0.02\n",
            "iteration: 288950 loss: 0.0039 lr: 0.02\n",
            "iteration: 288960 loss: 0.0032 lr: 0.02\n",
            "iteration: 288970 loss: 0.0030 lr: 0.02\n",
            "iteration: 288980 loss: 0.0033 lr: 0.02\n",
            "iteration: 288990 loss: 0.0033 lr: 0.02\n",
            "iteration: 289000 loss: 0.0036 lr: 0.02\n",
            "iteration: 289010 loss: 0.0030 lr: 0.02\n",
            "iteration: 289020 loss: 0.0043 lr: 0.02\n",
            "iteration: 289030 loss: 0.0043 lr: 0.02\n",
            "iteration: 289040 loss: 0.0030 lr: 0.02\n",
            "iteration: 289050 loss: 0.0036 lr: 0.02\n",
            "iteration: 289060 loss: 0.0032 lr: 0.02\n",
            "iteration: 289070 loss: 0.0027 lr: 0.02\n",
            "iteration: 289080 loss: 0.0049 lr: 0.02\n",
            "iteration: 289090 loss: 0.0028 lr: 0.02\n",
            "iteration: 289100 loss: 0.0043 lr: 0.02\n",
            "iteration: 289110 loss: 0.0038 lr: 0.02\n",
            "iteration: 289120 loss: 0.0049 lr: 0.02\n",
            "iteration: 289130 loss: 0.0032 lr: 0.02\n",
            "iteration: 289140 loss: 0.0031 lr: 0.02\n",
            "iteration: 289150 loss: 0.0035 lr: 0.02\n",
            "iteration: 289160 loss: 0.0030 lr: 0.02\n",
            "iteration: 289170 loss: 0.0026 lr: 0.02\n",
            "iteration: 289180 loss: 0.0034 lr: 0.02\n",
            "iteration: 289190 loss: 0.0031 lr: 0.02\n",
            "iteration: 289200 loss: 0.0035 lr: 0.02\n",
            "iteration: 289210 loss: 0.0039 lr: 0.02\n",
            "iteration: 289220 loss: 0.0038 lr: 0.02\n",
            "iteration: 289230 loss: 0.0038 lr: 0.02\n",
            "iteration: 289240 loss: 0.0037 lr: 0.02\n",
            "iteration: 289250 loss: 0.0033 lr: 0.02\n",
            "iteration: 289260 loss: 0.0028 lr: 0.02\n",
            "iteration: 289270 loss: 0.0050 lr: 0.02\n",
            "iteration: 289280 loss: 0.0026 lr: 0.02\n",
            "iteration: 289290 loss: 0.0034 lr: 0.02\n",
            "iteration: 289300 loss: 0.0035 lr: 0.02\n",
            "iteration: 289310 loss: 0.0033 lr: 0.02\n",
            "iteration: 289320 loss: 0.0038 lr: 0.02\n",
            "iteration: 289330 loss: 0.0031 lr: 0.02\n",
            "iteration: 289340 loss: 0.0029 lr: 0.02\n",
            "iteration: 289350 loss: 0.0037 lr: 0.02\n",
            "iteration: 289360 loss: 0.0042 lr: 0.02\n",
            "iteration: 289370 loss: 0.0033 lr: 0.02\n",
            "iteration: 289380 loss: 0.0035 lr: 0.02\n",
            "iteration: 289390 loss: 0.0044 lr: 0.02\n",
            "iteration: 289400 loss: 0.0037 lr: 0.02\n",
            "iteration: 289410 loss: 0.0045 lr: 0.02\n",
            "iteration: 289420 loss: 0.0043 lr: 0.02\n",
            "iteration: 289430 loss: 0.0025 lr: 0.02\n",
            "iteration: 289440 loss: 0.0030 lr: 0.02\n",
            "iteration: 289450 loss: 0.0045 lr: 0.02\n",
            "iteration: 289460 loss: 0.0041 lr: 0.02\n",
            "iteration: 289470 loss: 0.0037 lr: 0.02\n",
            "iteration: 289480 loss: 0.0042 lr: 0.02\n",
            "iteration: 289490 loss: 0.0042 lr: 0.02\n",
            "iteration: 289500 loss: 0.0044 lr: 0.02\n",
            "iteration: 289510 loss: 0.0036 lr: 0.02\n",
            "iteration: 289520 loss: 0.0033 lr: 0.02\n",
            "iteration: 289530 loss: 0.0041 lr: 0.02\n",
            "iteration: 289540 loss: 0.0041 lr: 0.02\n",
            "iteration: 289550 loss: 0.0034 lr: 0.02\n",
            "iteration: 289560 loss: 0.0027 lr: 0.02\n",
            "iteration: 289570 loss: 0.0023 lr: 0.02\n",
            "iteration: 289580 loss: 0.0035 lr: 0.02\n",
            "iteration: 289590 loss: 0.0034 lr: 0.02\n",
            "iteration: 289600 loss: 0.0036 lr: 0.02\n",
            "iteration: 289610 loss: 0.0027 lr: 0.02\n",
            "iteration: 289620 loss: 0.0029 lr: 0.02\n",
            "iteration: 289630 loss: 0.0044 lr: 0.02\n",
            "iteration: 289640 loss: 0.0033 lr: 0.02\n",
            "iteration: 289650 loss: 0.0036 lr: 0.02\n",
            "iteration: 289660 loss: 0.0038 lr: 0.02\n",
            "iteration: 289670 loss: 0.0030 lr: 0.02\n",
            "iteration: 289680 loss: 0.0036 lr: 0.02\n",
            "iteration: 289690 loss: 0.0029 lr: 0.02\n",
            "iteration: 289700 loss: 0.0039 lr: 0.02\n",
            "iteration: 289710 loss: 0.0043 lr: 0.02\n",
            "iteration: 289720 loss: 0.0029 lr: 0.02\n",
            "iteration: 289730 loss: 0.0034 lr: 0.02\n",
            "iteration: 289740 loss: 0.0033 lr: 0.02\n",
            "iteration: 289750 loss: 0.0028 lr: 0.02\n",
            "iteration: 289760 loss: 0.0035 lr: 0.02\n",
            "iteration: 289770 loss: 0.0033 lr: 0.02\n",
            "iteration: 289780 loss: 0.0036 lr: 0.02\n",
            "iteration: 289790 loss: 0.0033 lr: 0.02\n",
            "iteration: 289800 loss: 0.0051 lr: 0.02\n",
            "iteration: 289810 loss: 0.0034 lr: 0.02\n",
            "iteration: 289820 loss: 0.0044 lr: 0.02\n",
            "iteration: 289830 loss: 0.0050 lr: 0.02\n",
            "iteration: 289840 loss: 0.0040 lr: 0.02\n",
            "iteration: 289850 loss: 0.0044 lr: 0.02\n",
            "iteration: 289860 loss: 0.0038 lr: 0.02\n",
            "iteration: 289870 loss: 0.0039 lr: 0.02\n",
            "iteration: 289880 loss: 0.0030 lr: 0.02\n",
            "iteration: 289890 loss: 0.0024 lr: 0.02\n",
            "iteration: 289900 loss: 0.0032 lr: 0.02\n",
            "iteration: 289910 loss: 0.0032 lr: 0.02\n",
            "iteration: 289920 loss: 0.0036 lr: 0.02\n",
            "iteration: 289930 loss: 0.0031 lr: 0.02\n",
            "iteration: 289940 loss: 0.0035 lr: 0.02\n",
            "iteration: 289950 loss: 0.0034 lr: 0.02\n",
            "iteration: 289960 loss: 0.0032 lr: 0.02\n",
            "iteration: 289970 loss: 0.0037 lr: 0.02\n",
            "iteration: 289980 loss: 0.0039 lr: 0.02\n",
            "iteration: 289990 loss: 0.0037 lr: 0.02\n",
            "iteration: 290000 loss: 0.0028 lr: 0.02\n",
            "iteration: 290010 loss: 0.0029 lr: 0.02\n",
            "iteration: 290020 loss: 0.0034 lr: 0.02\n",
            "iteration: 290030 loss: 0.0035 lr: 0.02\n",
            "iteration: 290040 loss: 0.0028 lr: 0.02\n",
            "iteration: 290050 loss: 0.0048 lr: 0.02\n",
            "iteration: 290060 loss: 0.0034 lr: 0.02\n",
            "iteration: 290070 loss: 0.0032 lr: 0.02\n",
            "iteration: 290080 loss: 0.0036 lr: 0.02\n",
            "iteration: 290090 loss: 0.0035 lr: 0.02\n",
            "iteration: 290100 loss: 0.0028 lr: 0.02\n",
            "iteration: 290110 loss: 0.0032 lr: 0.02\n",
            "iteration: 290120 loss: 0.0034 lr: 0.02\n",
            "iteration: 290130 loss: 0.0035 lr: 0.02\n",
            "iteration: 290140 loss: 0.0035 lr: 0.02\n",
            "iteration: 290150 loss: 0.0034 lr: 0.02\n",
            "iteration: 290160 loss: 0.0048 lr: 0.02\n",
            "iteration: 290170 loss: 0.0022 lr: 0.02\n",
            "iteration: 290180 loss: 0.0034 lr: 0.02\n",
            "iteration: 290190 loss: 0.0039 lr: 0.02\n",
            "iteration: 290200 loss: 0.0035 lr: 0.02\n",
            "iteration: 290210 loss: 0.0052 lr: 0.02\n",
            "iteration: 290220 loss: 0.0036 lr: 0.02\n",
            "iteration: 290230 loss: 0.0034 lr: 0.02\n",
            "iteration: 290240 loss: 0.0038 lr: 0.02\n",
            "iteration: 290250 loss: 0.0037 lr: 0.02\n",
            "iteration: 290260 loss: 0.0031 lr: 0.02\n",
            "iteration: 290270 loss: 0.0039 lr: 0.02\n",
            "iteration: 290280 loss: 0.0030 lr: 0.02\n",
            "iteration: 290290 loss: 0.0042 lr: 0.02\n",
            "iteration: 290300 loss: 0.0032 lr: 0.02\n",
            "iteration: 290310 loss: 0.0053 lr: 0.02\n",
            "iteration: 290320 loss: 0.0034 lr: 0.02\n",
            "iteration: 290330 loss: 0.0037 lr: 0.02\n",
            "iteration: 290340 loss: 0.0046 lr: 0.02\n",
            "iteration: 290350 loss: 0.0036 lr: 0.02\n",
            "iteration: 290360 loss: 0.0037 lr: 0.02\n",
            "iteration: 290370 loss: 0.0031 lr: 0.02\n",
            "iteration: 290380 loss: 0.0027 lr: 0.02\n",
            "iteration: 290390 loss: 0.0042 lr: 0.02\n",
            "iteration: 290400 loss: 0.0031 lr: 0.02\n",
            "iteration: 290410 loss: 0.0052 lr: 0.02\n",
            "iteration: 290420 loss: 0.0031 lr: 0.02\n",
            "iteration: 290430 loss: 0.0034 lr: 0.02\n",
            "iteration: 290440 loss: 0.0026 lr: 0.02\n",
            "iteration: 290450 loss: 0.0037 lr: 0.02\n",
            "iteration: 290460 loss: 0.0036 lr: 0.02\n",
            "iteration: 290470 loss: 0.0038 lr: 0.02\n",
            "iteration: 290480 loss: 0.0033 lr: 0.02\n",
            "iteration: 290490 loss: 0.0051 lr: 0.02\n",
            "iteration: 290500 loss: 0.0034 lr: 0.02\n",
            "iteration: 290510 loss: 0.0042 lr: 0.02\n",
            "iteration: 290520 loss: 0.0037 lr: 0.02\n",
            "iteration: 290530 loss: 0.0022 lr: 0.02\n",
            "iteration: 290540 loss: 0.0035 lr: 0.02\n",
            "iteration: 290550 loss: 0.0034 lr: 0.02\n",
            "iteration: 290560 loss: 0.0034 lr: 0.02\n",
            "iteration: 290570 loss: 0.0034 lr: 0.02\n",
            "iteration: 290580 loss: 0.0033 lr: 0.02\n",
            "iteration: 290590 loss: 0.0043 lr: 0.02\n",
            "iteration: 290600 loss: 0.0044 lr: 0.02\n",
            "iteration: 290610 loss: 0.0034 lr: 0.02\n",
            "iteration: 290620 loss: 0.0037 lr: 0.02\n",
            "iteration: 290630 loss: 0.0033 lr: 0.02\n",
            "iteration: 290640 loss: 0.0033 lr: 0.02\n",
            "iteration: 290650 loss: 0.0035 lr: 0.02\n",
            "iteration: 290660 loss: 0.0030 lr: 0.02\n",
            "iteration: 290670 loss: 0.0049 lr: 0.02\n",
            "iteration: 290680 loss: 0.0054 lr: 0.02\n",
            "iteration: 290690 loss: 0.0039 lr: 0.02\n",
            "iteration: 290700 loss: 0.0033 lr: 0.02\n",
            "iteration: 290710 loss: 0.0039 lr: 0.02\n",
            "iteration: 290720 loss: 0.0042 lr: 0.02\n",
            "iteration: 290730 loss: 0.0050 lr: 0.02\n",
            "iteration: 290740 loss: 0.0035 lr: 0.02\n",
            "iteration: 290750 loss: 0.0037 lr: 0.02\n",
            "iteration: 290760 loss: 0.0036 lr: 0.02\n",
            "iteration: 290770 loss: 0.0026 lr: 0.02\n",
            "iteration: 290780 loss: 0.0043 lr: 0.02\n",
            "iteration: 290790 loss: 0.0039 lr: 0.02\n",
            "iteration: 290800 loss: 0.0030 lr: 0.02\n",
            "iteration: 290810 loss: 0.0031 lr: 0.02\n",
            "iteration: 290820 loss: 0.0032 lr: 0.02\n",
            "iteration: 290830 loss: 0.0032 lr: 0.02\n",
            "iteration: 290840 loss: 0.0029 lr: 0.02\n",
            "iteration: 290850 loss: 0.0028 lr: 0.02\n",
            "iteration: 290860 loss: 0.0034 lr: 0.02\n",
            "iteration: 290870 loss: 0.0035 lr: 0.02\n",
            "iteration: 290880 loss: 0.0032 lr: 0.02\n",
            "iteration: 290890 loss: 0.0035 lr: 0.02\n",
            "iteration: 290900 loss: 0.0041 lr: 0.02\n",
            "iteration: 290910 loss: 0.0039 lr: 0.02\n",
            "iteration: 290920 loss: 0.0029 lr: 0.02\n",
            "iteration: 290930 loss: 0.0034 lr: 0.02\n",
            "iteration: 290940 loss: 0.0042 lr: 0.02\n",
            "iteration: 290950 loss: 0.0031 lr: 0.02\n",
            "iteration: 290960 loss: 0.0031 lr: 0.02\n",
            "iteration: 290970 loss: 0.0044 lr: 0.02\n",
            "iteration: 290980 loss: 0.0036 lr: 0.02\n",
            "iteration: 290990 loss: 0.0038 lr: 0.02\n",
            "iteration: 291000 loss: 0.0031 lr: 0.02\n",
            "iteration: 291010 loss: 0.0037 lr: 0.02\n",
            "iteration: 291020 loss: 0.0041 lr: 0.02\n",
            "iteration: 291030 loss: 0.0039 lr: 0.02\n",
            "iteration: 291040 loss: 0.0048 lr: 0.02\n",
            "iteration: 291050 loss: 0.0031 lr: 0.02\n",
            "iteration: 291060 loss: 0.0025 lr: 0.02\n",
            "iteration: 291070 loss: 0.0039 lr: 0.02\n",
            "iteration: 291080 loss: 0.0029 lr: 0.02\n",
            "iteration: 291090 loss: 0.0030 lr: 0.02\n",
            "iteration: 291100 loss: 0.0036 lr: 0.02\n",
            "iteration: 291110 loss: 0.0036 lr: 0.02\n",
            "iteration: 291120 loss: 0.0028 lr: 0.02\n",
            "iteration: 291130 loss: 0.0049 lr: 0.02\n",
            "iteration: 291140 loss: 0.0038 lr: 0.02\n",
            "iteration: 291150 loss: 0.0029 lr: 0.02\n",
            "iteration: 291160 loss: 0.0040 lr: 0.02\n",
            "iteration: 291170 loss: 0.0035 lr: 0.02\n",
            "iteration: 291180 loss: 0.0034 lr: 0.02\n",
            "iteration: 291190 loss: 0.0035 lr: 0.02\n",
            "iteration: 291200 loss: 0.0030 lr: 0.02\n",
            "iteration: 291210 loss: 0.0043 lr: 0.02\n",
            "iteration: 291220 loss: 0.0030 lr: 0.02\n",
            "iteration: 291230 loss: 0.0045 lr: 0.02\n",
            "iteration: 291240 loss: 0.0036 lr: 0.02\n",
            "iteration: 291250 loss: 0.0036 lr: 0.02\n",
            "iteration: 291260 loss: 0.0044 lr: 0.02\n",
            "iteration: 291270 loss: 0.0026 lr: 0.02\n",
            "iteration: 291280 loss: 0.0044 lr: 0.02\n",
            "iteration: 291290 loss: 0.0040 lr: 0.02\n",
            "iteration: 291300 loss: 0.0034 lr: 0.02\n",
            "iteration: 291310 loss: 0.0035 lr: 0.02\n",
            "iteration: 291320 loss: 0.0040 lr: 0.02\n",
            "iteration: 291330 loss: 0.0041 lr: 0.02\n",
            "iteration: 291340 loss: 0.0037 lr: 0.02\n",
            "iteration: 291350 loss: 0.0041 lr: 0.02\n",
            "iteration: 291360 loss: 0.0045 lr: 0.02\n",
            "iteration: 291370 loss: 0.0051 lr: 0.02\n",
            "iteration: 291380 loss: 0.0038 lr: 0.02\n",
            "iteration: 291390 loss: 0.0038 lr: 0.02\n",
            "iteration: 291400 loss: 0.0032 lr: 0.02\n",
            "iteration: 291410 loss: 0.0038 lr: 0.02\n",
            "iteration: 291420 loss: 0.0033 lr: 0.02\n",
            "iteration: 291430 loss: 0.0031 lr: 0.02\n",
            "iteration: 291440 loss: 0.0035 lr: 0.02\n",
            "iteration: 291450 loss: 0.0033 lr: 0.02\n",
            "iteration: 291460 loss: 0.0037 lr: 0.02\n",
            "iteration: 291470 loss: 0.0032 lr: 0.02\n",
            "iteration: 291480 loss: 0.0045 lr: 0.02\n",
            "iteration: 291490 loss: 0.0043 lr: 0.02\n",
            "iteration: 291500 loss: 0.0027 lr: 0.02\n",
            "iteration: 291510 loss: 0.0034 lr: 0.02\n",
            "iteration: 291520 loss: 0.0034 lr: 0.02\n",
            "iteration: 291530 loss: 0.0043 lr: 0.02\n",
            "iteration: 291540 loss: 0.0036 lr: 0.02\n",
            "iteration: 291550 loss: 0.0033 lr: 0.02\n",
            "iteration: 291560 loss: 0.0027 lr: 0.02\n",
            "iteration: 291570 loss: 0.0036 lr: 0.02\n",
            "iteration: 291580 loss: 0.0039 lr: 0.02\n",
            "iteration: 291590 loss: 0.0025 lr: 0.02\n",
            "iteration: 291600 loss: 0.0025 lr: 0.02\n",
            "iteration: 291610 loss: 0.0035 lr: 0.02\n",
            "iteration: 291620 loss: 0.0044 lr: 0.02\n",
            "iteration: 291630 loss: 0.0040 lr: 0.02\n",
            "iteration: 291640 loss: 0.0036 lr: 0.02\n",
            "iteration: 291650 loss: 0.0036 lr: 0.02\n",
            "iteration: 291660 loss: 0.0038 lr: 0.02\n",
            "iteration: 291670 loss: 0.0036 lr: 0.02\n",
            "iteration: 291680 loss: 0.0034 lr: 0.02\n",
            "iteration: 291690 loss: 0.0035 lr: 0.02\n",
            "iteration: 291700 loss: 0.0031 lr: 0.02\n",
            "iteration: 291710 loss: 0.0028 lr: 0.02\n",
            "iteration: 291720 loss: 0.0035 lr: 0.02\n",
            "iteration: 291730 loss: 0.0034 lr: 0.02\n",
            "iteration: 291740 loss: 0.0032 lr: 0.02\n",
            "iteration: 291750 loss: 0.0029 lr: 0.02\n",
            "iteration: 291760 loss: 0.0039 lr: 0.02\n",
            "iteration: 291770 loss: 0.0029 lr: 0.02\n",
            "iteration: 291780 loss: 0.0049 lr: 0.02\n",
            "iteration: 291790 loss: 0.0029 lr: 0.02\n",
            "iteration: 291800 loss: 0.0032 lr: 0.02\n",
            "iteration: 291810 loss: 0.0022 lr: 0.02\n",
            "iteration: 291820 loss: 0.0029 lr: 0.02\n",
            "iteration: 291830 loss: 0.0047 lr: 0.02\n",
            "iteration: 291840 loss: 0.0043 lr: 0.02\n",
            "iteration: 291850 loss: 0.0037 lr: 0.02\n",
            "iteration: 291860 loss: 0.0039 lr: 0.02\n",
            "iteration: 291870 loss: 0.0053 lr: 0.02\n",
            "iteration: 291880 loss: 0.0031 lr: 0.02\n",
            "iteration: 291890 loss: 0.0034 lr: 0.02\n",
            "iteration: 291900 loss: 0.0041 lr: 0.02\n",
            "iteration: 291910 loss: 0.0034 lr: 0.02\n",
            "iteration: 291920 loss: 0.0034 lr: 0.02\n",
            "iteration: 291930 loss: 0.0041 lr: 0.02\n",
            "iteration: 291940 loss: 0.0033 lr: 0.02\n",
            "iteration: 291950 loss: 0.0034 lr: 0.02\n",
            "iteration: 291960 loss: 0.0032 lr: 0.02\n",
            "iteration: 291970 loss: 0.0043 lr: 0.02\n",
            "iteration: 291980 loss: 0.0032 lr: 0.02\n",
            "iteration: 291990 loss: 0.0025 lr: 0.02\n",
            "iteration: 292000 loss: 0.0035 lr: 0.02\n",
            "iteration: 292010 loss: 0.0031 lr: 0.02\n",
            "iteration: 292020 loss: 0.0037 lr: 0.02\n",
            "iteration: 292030 loss: 0.0039 lr: 0.02\n",
            "iteration: 292040 loss: 0.0039 lr: 0.02\n",
            "iteration: 292050 loss: 0.0038 lr: 0.02\n",
            "iteration: 292060 loss: 0.0041 lr: 0.02\n",
            "iteration: 292070 loss: 0.0026 lr: 0.02\n",
            "iteration: 292080 loss: 0.0036 lr: 0.02\n",
            "iteration: 292090 loss: 0.0032 lr: 0.02\n",
            "iteration: 292100 loss: 0.0030 lr: 0.02\n",
            "iteration: 292110 loss: 0.0037 lr: 0.02\n",
            "iteration: 292120 loss: 0.0048 lr: 0.02\n",
            "iteration: 292130 loss: 0.0039 lr: 0.02\n",
            "iteration: 292140 loss: 0.0038 lr: 0.02\n",
            "iteration: 292150 loss: 0.0033 lr: 0.02\n",
            "iteration: 292160 loss: 0.0044 lr: 0.02\n",
            "iteration: 292170 loss: 0.0041 lr: 0.02\n",
            "iteration: 292180 loss: 0.0030 lr: 0.02\n",
            "iteration: 292190 loss: 0.0036 lr: 0.02\n",
            "iteration: 292200 loss: 0.0031 lr: 0.02\n",
            "iteration: 292210 loss: 0.0030 lr: 0.02\n",
            "iteration: 292220 loss: 0.0035 lr: 0.02\n",
            "iteration: 292230 loss: 0.0040 lr: 0.02\n",
            "iteration: 292240 loss: 0.0042 lr: 0.02\n",
            "iteration: 292250 loss: 0.0031 lr: 0.02\n",
            "iteration: 292260 loss: 0.0029 lr: 0.02\n",
            "iteration: 292270 loss: 0.0034 lr: 0.02\n",
            "iteration: 292280 loss: 0.0034 lr: 0.02\n",
            "iteration: 292290 loss: 0.0030 lr: 0.02\n",
            "iteration: 292300 loss: 0.0029 lr: 0.02\n",
            "iteration: 292310 loss: 0.0050 lr: 0.02\n",
            "iteration: 292320 loss: 0.0026 lr: 0.02\n",
            "iteration: 292330 loss: 0.0039 lr: 0.02\n",
            "iteration: 292340 loss: 0.0037 lr: 0.02\n",
            "iteration: 292350 loss: 0.0041 lr: 0.02\n",
            "iteration: 292360 loss: 0.0029 lr: 0.02\n",
            "iteration: 292370 loss: 0.0029 lr: 0.02\n",
            "iteration: 292380 loss: 0.0031 lr: 0.02\n",
            "iteration: 292390 loss: 0.0033 lr: 0.02\n",
            "iteration: 292400 loss: 0.0027 lr: 0.02\n",
            "iteration: 292410 loss: 0.0034 lr: 0.02\n",
            "iteration: 292420 loss: 0.0028 lr: 0.02\n",
            "iteration: 292430 loss: 0.0050 lr: 0.02\n",
            "iteration: 292440 loss: 0.0051 lr: 0.02\n",
            "iteration: 292450 loss: 0.0031 lr: 0.02\n",
            "iteration: 292460 loss: 0.0035 lr: 0.02\n",
            "iteration: 292470 loss: 0.0034 lr: 0.02\n",
            "iteration: 292480 loss: 0.0032 lr: 0.02\n",
            "iteration: 292490 loss: 0.0033 lr: 0.02\n",
            "iteration: 292500 loss: 0.0042 lr: 0.02\n",
            "iteration: 292510 loss: 0.0047 lr: 0.02\n",
            "iteration: 292520 loss: 0.0038 lr: 0.02\n",
            "iteration: 292530 loss: 0.0037 lr: 0.02\n",
            "iteration: 292540 loss: 0.0040 lr: 0.02\n",
            "iteration: 292550 loss: 0.0037 lr: 0.02\n",
            "iteration: 292560 loss: 0.0029 lr: 0.02\n",
            "iteration: 292570 loss: 0.0038 lr: 0.02\n",
            "iteration: 292580 loss: 0.0033 lr: 0.02\n",
            "iteration: 292590 loss: 0.0034 lr: 0.02\n",
            "iteration: 292600 loss: 0.0040 lr: 0.02\n",
            "iteration: 292610 loss: 0.0033 lr: 0.02\n",
            "iteration: 292620 loss: 0.0041 lr: 0.02\n",
            "iteration: 292630 loss: 0.0031 lr: 0.02\n",
            "iteration: 292640 loss: 0.0046 lr: 0.02\n",
            "iteration: 292650 loss: 0.0034 lr: 0.02\n",
            "iteration: 292660 loss: 0.0030 lr: 0.02\n",
            "iteration: 292670 loss: 0.0039 lr: 0.02\n",
            "iteration: 292680 loss: 0.0042 lr: 0.02\n",
            "iteration: 292690 loss: 0.0040 lr: 0.02\n",
            "iteration: 292700 loss: 0.0047 lr: 0.02\n",
            "iteration: 292710 loss: 0.0037 lr: 0.02\n",
            "iteration: 292720 loss: 0.0039 lr: 0.02\n",
            "iteration: 292730 loss: 0.0034 lr: 0.02\n",
            "iteration: 292740 loss: 0.0035 lr: 0.02\n",
            "iteration: 292750 loss: 0.0038 lr: 0.02\n",
            "iteration: 292760 loss: 0.0024 lr: 0.02\n",
            "iteration: 292770 loss: 0.0041 lr: 0.02\n",
            "iteration: 292780 loss: 0.0038 lr: 0.02\n",
            "iteration: 292790 loss: 0.0030 lr: 0.02\n",
            "iteration: 292800 loss: 0.0030 lr: 0.02\n",
            "iteration: 292810 loss: 0.0031 lr: 0.02\n",
            "iteration: 292820 loss: 0.0034 lr: 0.02\n",
            "iteration: 292830 loss: 0.0039 lr: 0.02\n",
            "iteration: 292840 loss: 0.0027 lr: 0.02\n",
            "iteration: 292850 loss: 0.0032 lr: 0.02\n",
            "iteration: 292860 loss: 0.0035 lr: 0.02\n",
            "iteration: 292870 loss: 0.0033 lr: 0.02\n",
            "iteration: 292880 loss: 0.0030 lr: 0.02\n",
            "iteration: 292890 loss: 0.0044 lr: 0.02\n",
            "iteration: 292900 loss: 0.0034 lr: 0.02\n",
            "iteration: 292910 loss: 0.0034 lr: 0.02\n",
            "iteration: 292920 loss: 0.0034 lr: 0.02\n",
            "iteration: 292930 loss: 0.0038 lr: 0.02\n",
            "iteration: 292940 loss: 0.0031 lr: 0.02\n",
            "iteration: 292950 loss: 0.0039 lr: 0.02\n",
            "iteration: 292960 loss: 0.0033 lr: 0.02\n",
            "iteration: 292970 loss: 0.0033 lr: 0.02\n",
            "iteration: 292980 loss: 0.0031 lr: 0.02\n",
            "iteration: 292990 loss: 0.0027 lr: 0.02\n",
            "iteration: 293000 loss: 0.0030 lr: 0.02\n",
            "iteration: 293010 loss: 0.0030 lr: 0.02\n",
            "iteration: 293020 loss: 0.0041 lr: 0.02\n",
            "iteration: 293030 loss: 0.0032 lr: 0.02\n",
            "iteration: 293040 loss: 0.0026 lr: 0.02\n",
            "iteration: 293050 loss: 0.0036 lr: 0.02\n",
            "iteration: 293060 loss: 0.0034 lr: 0.02\n",
            "iteration: 293070 loss: 0.0028 lr: 0.02\n",
            "iteration: 293080 loss: 0.0036 lr: 0.02\n",
            "iteration: 293090 loss: 0.0036 lr: 0.02\n",
            "iteration: 293100 loss: 0.0028 lr: 0.02\n",
            "iteration: 293110 loss: 0.0043 lr: 0.02\n",
            "iteration: 293120 loss: 0.0035 lr: 0.02\n",
            "iteration: 293130 loss: 0.0027 lr: 0.02\n",
            "iteration: 293140 loss: 0.0022 lr: 0.02\n",
            "iteration: 293150 loss: 0.0045 lr: 0.02\n",
            "iteration: 293160 loss: 0.0038 lr: 0.02\n",
            "iteration: 293170 loss: 0.0037 lr: 0.02\n",
            "iteration: 293180 loss: 0.0043 lr: 0.02\n",
            "iteration: 293190 loss: 0.0030 lr: 0.02\n",
            "iteration: 293200 loss: 0.0044 lr: 0.02\n",
            "iteration: 293210 loss: 0.0037 lr: 0.02\n",
            "iteration: 293220 loss: 0.0042 lr: 0.02\n",
            "iteration: 293230 loss: 0.0040 lr: 0.02\n",
            "iteration: 293240 loss: 0.0033 lr: 0.02\n",
            "iteration: 293250 loss: 0.0038 lr: 0.02\n",
            "iteration: 293260 loss: 0.0036 lr: 0.02\n",
            "iteration: 293270 loss: 0.0028 lr: 0.02\n",
            "iteration: 293280 loss: 0.0031 lr: 0.02\n",
            "iteration: 293290 loss: 0.0027 lr: 0.02\n",
            "iteration: 293300 loss: 0.0034 lr: 0.02\n",
            "iteration: 293310 loss: 0.0029 lr: 0.02\n",
            "iteration: 293320 loss: 0.0039 lr: 0.02\n",
            "iteration: 293330 loss: 0.0040 lr: 0.02\n",
            "iteration: 293340 loss: 0.0034 lr: 0.02\n",
            "iteration: 293350 loss: 0.0038 lr: 0.02\n",
            "iteration: 293360 loss: 0.0035 lr: 0.02\n",
            "iteration: 293370 loss: 0.0033 lr: 0.02\n",
            "iteration: 293380 loss: 0.0035 lr: 0.02\n",
            "iteration: 293390 loss: 0.0040 lr: 0.02\n",
            "iteration: 293400 loss: 0.0035 lr: 0.02\n",
            "iteration: 293410 loss: 0.0035 lr: 0.02\n",
            "iteration: 293420 loss: 0.0037 lr: 0.02\n",
            "iteration: 293430 loss: 0.0033 lr: 0.02\n",
            "iteration: 293440 loss: 0.0042 lr: 0.02\n",
            "iteration: 293450 loss: 0.0036 lr: 0.02\n",
            "iteration: 293460 loss: 0.0028 lr: 0.02\n",
            "iteration: 293470 loss: 0.0034 lr: 0.02\n",
            "iteration: 293480 loss: 0.0040 lr: 0.02\n",
            "iteration: 293490 loss: 0.0044 lr: 0.02\n",
            "iteration: 293500 loss: 0.0031 lr: 0.02\n",
            "iteration: 293510 loss: 0.0034 lr: 0.02\n",
            "iteration: 293520 loss: 0.0043 lr: 0.02\n",
            "iteration: 293530 loss: 0.0043 lr: 0.02\n",
            "iteration: 293540 loss: 0.0033 lr: 0.02\n",
            "iteration: 293550 loss: 0.0037 lr: 0.02\n",
            "iteration: 293560 loss: 0.0032 lr: 0.02\n",
            "iteration: 293570 loss: 0.0042 lr: 0.02\n",
            "iteration: 293580 loss: 0.0044 lr: 0.02\n",
            "iteration: 293590 loss: 0.0036 lr: 0.02\n",
            "iteration: 293600 loss: 0.0036 lr: 0.02\n",
            "iteration: 293610 loss: 0.0037 lr: 0.02\n",
            "iteration: 293620 loss: 0.0026 lr: 0.02\n",
            "iteration: 293630 loss: 0.0039 lr: 0.02\n",
            "iteration: 293640 loss: 0.0037 lr: 0.02\n",
            "iteration: 293650 loss: 0.0034 lr: 0.02\n",
            "iteration: 293660 loss: 0.0035 lr: 0.02\n",
            "iteration: 293670 loss: 0.0026 lr: 0.02\n",
            "iteration: 293680 loss: 0.0034 lr: 0.02\n",
            "iteration: 293690 loss: 0.0035 lr: 0.02\n",
            "iteration: 293700 loss: 0.0038 lr: 0.02\n",
            "iteration: 293710 loss: 0.0042 lr: 0.02\n",
            "iteration: 293720 loss: 0.0037 lr: 0.02\n",
            "iteration: 293730 loss: 0.0035 lr: 0.02\n",
            "iteration: 293740 loss: 0.0041 lr: 0.02\n",
            "iteration: 293750 loss: 0.0035 lr: 0.02\n",
            "iteration: 293760 loss: 0.0032 lr: 0.02\n",
            "iteration: 293770 loss: 0.0033 lr: 0.02\n",
            "iteration: 293780 loss: 0.0030 lr: 0.02\n",
            "iteration: 293790 loss: 0.0029 lr: 0.02\n",
            "iteration: 293800 loss: 0.0036 lr: 0.02\n",
            "iteration: 293810 loss: 0.0052 lr: 0.02\n",
            "iteration: 293820 loss: 0.0050 lr: 0.02\n",
            "iteration: 293830 loss: 0.0051 lr: 0.02\n",
            "iteration: 293840 loss: 0.0049 lr: 0.02\n",
            "iteration: 293850 loss: 0.0037 lr: 0.02\n",
            "iteration: 293860 loss: 0.0060 lr: 0.02\n",
            "iteration: 293870 loss: 0.0048 lr: 0.02\n",
            "iteration: 293880 loss: 0.0033 lr: 0.02\n",
            "iteration: 293890 loss: 0.0033 lr: 0.02\n",
            "iteration: 293900 loss: 0.0039 lr: 0.02\n",
            "iteration: 293910 loss: 0.0033 lr: 0.02\n",
            "iteration: 293920 loss: 0.0033 lr: 0.02\n",
            "iteration: 293930 loss: 0.0039 lr: 0.02\n",
            "iteration: 293940 loss: 0.0031 lr: 0.02\n",
            "iteration: 293950 loss: 0.0042 lr: 0.02\n",
            "iteration: 293960 loss: 0.0030 lr: 0.02\n",
            "iteration: 293970 loss: 0.0032 lr: 0.02\n",
            "iteration: 293980 loss: 0.0038 lr: 0.02\n",
            "iteration: 293990 loss: 0.0043 lr: 0.02\n",
            "iteration: 294000 loss: 0.0024 lr: 0.02\n",
            "iteration: 294010 loss: 0.0033 lr: 0.02\n",
            "iteration: 294020 loss: 0.0037 lr: 0.02\n",
            "iteration: 294030 loss: 0.0038 lr: 0.02\n",
            "iteration: 294040 loss: 0.0038 lr: 0.02\n",
            "iteration: 294050 loss: 0.0043 lr: 0.02\n",
            "iteration: 294060 loss: 0.0047 lr: 0.02\n",
            "iteration: 294070 loss: 0.0039 lr: 0.02\n",
            "iteration: 294080 loss: 0.0029 lr: 0.02\n",
            "iteration: 294090 loss: 0.0039 lr: 0.02\n",
            "iteration: 294100 loss: 0.0034 lr: 0.02\n",
            "iteration: 294110 loss: 0.0033 lr: 0.02\n",
            "iteration: 294120 loss: 0.0038 lr: 0.02\n",
            "iteration: 294130 loss: 0.0039 lr: 0.02\n",
            "iteration: 294140 loss: 0.0035 lr: 0.02\n",
            "iteration: 294150 loss: 0.0032 lr: 0.02\n",
            "iteration: 294160 loss: 0.0033 lr: 0.02\n",
            "iteration: 294170 loss: 0.0033 lr: 0.02\n",
            "iteration: 294180 loss: 0.0035 lr: 0.02\n",
            "iteration: 294190 loss: 0.0036 lr: 0.02\n",
            "iteration: 294200 loss: 0.0029 lr: 0.02\n",
            "iteration: 294210 loss: 0.0031 lr: 0.02\n",
            "iteration: 294220 loss: 0.0033 lr: 0.02\n",
            "iteration: 294230 loss: 0.0048 lr: 0.02\n",
            "iteration: 294240 loss: 0.0041 lr: 0.02\n",
            "iteration: 294250 loss: 0.0029 lr: 0.02\n",
            "iteration: 294260 loss: 0.0043 lr: 0.02\n",
            "iteration: 294270 loss: 0.0045 lr: 0.02\n",
            "iteration: 294280 loss: 0.0043 lr: 0.02\n",
            "iteration: 294290 loss: 0.0037 lr: 0.02\n",
            "iteration: 294300 loss: 0.0044 lr: 0.02\n",
            "iteration: 294310 loss: 0.0047 lr: 0.02\n",
            "iteration: 294320 loss: 0.0034 lr: 0.02\n",
            "iteration: 294330 loss: 0.0038 lr: 0.02\n",
            "iteration: 294340 loss: 0.0028 lr: 0.02\n",
            "iteration: 294350 loss: 0.0043 lr: 0.02\n",
            "iteration: 294360 loss: 0.0035 lr: 0.02\n",
            "iteration: 294370 loss: 0.0035 lr: 0.02\n",
            "iteration: 294380 loss: 0.0035 lr: 0.02\n",
            "iteration: 294390 loss: 0.0034 lr: 0.02\n",
            "iteration: 294400 loss: 0.0035 lr: 0.02\n",
            "iteration: 294410 loss: 0.0040 lr: 0.02\n",
            "iteration: 294420 loss: 0.0028 lr: 0.02\n",
            "iteration: 294430 loss: 0.0031 lr: 0.02\n",
            "iteration: 294440 loss: 0.0031 lr: 0.02\n",
            "iteration: 294450 loss: 0.0038 lr: 0.02\n",
            "iteration: 294460 loss: 0.0038 lr: 0.02\n",
            "iteration: 294470 loss: 0.0036 lr: 0.02\n",
            "iteration: 294480 loss: 0.0034 lr: 0.02\n",
            "iteration: 294490 loss: 0.0036 lr: 0.02\n",
            "iteration: 294500 loss: 0.0037 lr: 0.02\n",
            "iteration: 294510 loss: 0.0036 lr: 0.02\n",
            "iteration: 294520 loss: 0.0047 lr: 0.02\n",
            "iteration: 294530 loss: 0.0032 lr: 0.02\n",
            "iteration: 294540 loss: 0.0034 lr: 0.02\n",
            "iteration: 294550 loss: 0.0035 lr: 0.02\n",
            "iteration: 294560 loss: 0.0039 lr: 0.02\n",
            "iteration: 294570 loss: 0.0037 lr: 0.02\n",
            "iteration: 294580 loss: 0.0037 lr: 0.02\n",
            "iteration: 294590 loss: 0.0036 lr: 0.02\n",
            "iteration: 294600 loss: 0.0037 lr: 0.02\n",
            "iteration: 294610 loss: 0.0032 lr: 0.02\n",
            "iteration: 294620 loss: 0.0045 lr: 0.02\n",
            "iteration: 294630 loss: 0.0031 lr: 0.02\n",
            "iteration: 294640 loss: 0.0037 lr: 0.02\n",
            "iteration: 294650 loss: 0.0025 lr: 0.02\n",
            "iteration: 294660 loss: 0.0045 lr: 0.02\n",
            "iteration: 294670 loss: 0.0043 lr: 0.02\n",
            "iteration: 294680 loss: 0.0040 lr: 0.02\n",
            "iteration: 294690 loss: 0.0028 lr: 0.02\n",
            "iteration: 294700 loss: 0.0040 lr: 0.02\n",
            "iteration: 294710 loss: 0.0033 lr: 0.02\n",
            "iteration: 294720 loss: 0.0033 lr: 0.02\n",
            "iteration: 294730 loss: 0.0048 lr: 0.02\n",
            "iteration: 294740 loss: 0.0038 lr: 0.02\n",
            "iteration: 294750 loss: 0.0038 lr: 0.02\n",
            "iteration: 294760 loss: 0.0027 lr: 0.02\n",
            "iteration: 294770 loss: 0.0024 lr: 0.02\n",
            "iteration: 294780 loss: 0.0041 lr: 0.02\n",
            "iteration: 294790 loss: 0.0037 lr: 0.02\n",
            "iteration: 294800 loss: 0.0031 lr: 0.02\n",
            "iteration: 294810 loss: 0.0031 lr: 0.02\n",
            "iteration: 294820 loss: 0.0040 lr: 0.02\n",
            "iteration: 294830 loss: 0.0033 lr: 0.02\n",
            "iteration: 294840 loss: 0.0049 lr: 0.02\n",
            "iteration: 294850 loss: 0.0041 lr: 0.02\n",
            "iteration: 294860 loss: 0.0027 lr: 0.02\n",
            "iteration: 294870 loss: 0.0042 lr: 0.02\n",
            "iteration: 294880 loss: 0.0028 lr: 0.02\n",
            "iteration: 294890 loss: 0.0037 lr: 0.02\n",
            "iteration: 294900 loss: 0.0031 lr: 0.02\n",
            "iteration: 294910 loss: 0.0031 lr: 0.02\n",
            "iteration: 294920 loss: 0.0028 lr: 0.02\n",
            "iteration: 294930 loss: 0.0042 lr: 0.02\n",
            "iteration: 294940 loss: 0.0033 lr: 0.02\n",
            "iteration: 294950 loss: 0.0039 lr: 0.02\n",
            "iteration: 294960 loss: 0.0037 lr: 0.02\n",
            "iteration: 294970 loss: 0.0058 lr: 0.02\n",
            "iteration: 294980 loss: 0.0065 lr: 0.02\n",
            "iteration: 294990 loss: 0.0050 lr: 0.02\n",
            "iteration: 295000 loss: 0.0056 lr: 0.02\n",
            "iteration: 295010 loss: 0.0050 lr: 0.02\n",
            "iteration: 295020 loss: 0.0038 lr: 0.02\n",
            "iteration: 295030 loss: 0.0036 lr: 0.02\n",
            "iteration: 295040 loss: 0.0040 lr: 0.02\n",
            "iteration: 295050 loss: 0.0031 lr: 0.02\n",
            "iteration: 295060 loss: 0.0027 lr: 0.02\n",
            "iteration: 295070 loss: 0.0034 lr: 0.02\n",
            "iteration: 295080 loss: 0.0037 lr: 0.02\n",
            "iteration: 295090 loss: 0.0029 lr: 0.02\n",
            "iteration: 295100 loss: 0.0040 lr: 0.02\n",
            "iteration: 295110 loss: 0.0038 lr: 0.02\n",
            "iteration: 295120 loss: 0.0041 lr: 0.02\n",
            "iteration: 295130 loss: 0.0029 lr: 0.02\n",
            "iteration: 295140 loss: 0.0044 lr: 0.02\n",
            "iteration: 295150 loss: 0.0035 lr: 0.02\n",
            "iteration: 295160 loss: 0.0035 lr: 0.02\n",
            "iteration: 295170 loss: 0.0039 lr: 0.02\n",
            "iteration: 295180 loss: 0.0044 lr: 0.02\n",
            "iteration: 295190 loss: 0.0042 lr: 0.02\n",
            "iteration: 295200 loss: 0.0031 lr: 0.02\n",
            "iteration: 295210 loss: 0.0028 lr: 0.02\n",
            "iteration: 295220 loss: 0.0035 lr: 0.02\n",
            "iteration: 295230 loss: 0.0037 lr: 0.02\n",
            "iteration: 295240 loss: 0.0036 lr: 0.02\n",
            "iteration: 295250 loss: 0.0026 lr: 0.02\n",
            "iteration: 295260 loss: 0.0031 lr: 0.02\n",
            "iteration: 295270 loss: 0.0030 lr: 0.02\n",
            "iteration: 295280 loss: 0.0044 lr: 0.02\n",
            "iteration: 295290 loss: 0.0032 lr: 0.02\n",
            "iteration: 295300 loss: 0.0022 lr: 0.02\n",
            "iteration: 295310 loss: 0.0034 lr: 0.02\n",
            "iteration: 295320 loss: 0.0032 lr: 0.02\n",
            "iteration: 295330 loss: 0.0044 lr: 0.02\n",
            "iteration: 295340 loss: 0.0036 lr: 0.02\n",
            "iteration: 295350 loss: 0.0040 lr: 0.02\n",
            "iteration: 295360 loss: 0.0032 lr: 0.02\n",
            "iteration: 295370 loss: 0.0031 lr: 0.02\n",
            "iteration: 295380 loss: 0.0027 lr: 0.02\n",
            "iteration: 295390 loss: 0.0036 lr: 0.02\n",
            "iteration: 295400 loss: 0.0037 lr: 0.02\n",
            "iteration: 295410 loss: 0.0033 lr: 0.02\n",
            "iteration: 295420 loss: 0.0042 lr: 0.02\n",
            "iteration: 295430 loss: 0.0044 lr: 0.02\n",
            "iteration: 295440 loss: 0.0040 lr: 0.02\n",
            "iteration: 295450 loss: 0.0034 lr: 0.02\n",
            "iteration: 295460 loss: 0.0043 lr: 0.02\n",
            "iteration: 295470 loss: 0.0029 lr: 0.02\n",
            "iteration: 295480 loss: 0.0030 lr: 0.02\n",
            "iteration: 295490 loss: 0.0042 lr: 0.02\n",
            "iteration: 295500 loss: 0.0034 lr: 0.02\n",
            "iteration: 295510 loss: 0.0046 lr: 0.02\n",
            "iteration: 295520 loss: 0.0033 lr: 0.02\n",
            "iteration: 295530 loss: 0.0036 lr: 0.02\n",
            "iteration: 295540 loss: 0.0040 lr: 0.02\n",
            "iteration: 295550 loss: 0.0041 lr: 0.02\n",
            "iteration: 295560 loss: 0.0028 lr: 0.02\n",
            "iteration: 295570 loss: 0.0036 lr: 0.02\n",
            "iteration: 295580 loss: 0.0023 lr: 0.02\n",
            "iteration: 295590 loss: 0.0033 lr: 0.02\n",
            "iteration: 295600 loss: 0.0030 lr: 0.02\n",
            "iteration: 295610 loss: 0.0027 lr: 0.02\n",
            "iteration: 295620 loss: 0.0036 lr: 0.02\n",
            "iteration: 295630 loss: 0.0036 lr: 0.02\n",
            "iteration: 295640 loss: 0.0044 lr: 0.02\n",
            "iteration: 295650 loss: 0.0034 lr: 0.02\n",
            "iteration: 295660 loss: 0.0025 lr: 0.02\n",
            "iteration: 295670 loss: 0.0032 lr: 0.02\n",
            "iteration: 295680 loss: 0.0033 lr: 0.02\n",
            "iteration: 295690 loss: 0.0029 lr: 0.02\n",
            "iteration: 295700 loss: 0.0050 lr: 0.02\n",
            "iteration: 295710 loss: 0.0044 lr: 0.02\n",
            "iteration: 295720 loss: 0.0038 lr: 0.02\n",
            "iteration: 295730 loss: 0.0032 lr: 0.02\n",
            "iteration: 295740 loss: 0.0036 lr: 0.02\n",
            "iteration: 295750 loss: 0.0033 lr: 0.02\n",
            "iteration: 295760 loss: 0.0044 lr: 0.02\n",
            "iteration: 295770 loss: 0.0039 lr: 0.02\n",
            "iteration: 295780 loss: 0.0037 lr: 0.02\n",
            "iteration: 295790 loss: 0.0028 lr: 0.02\n",
            "iteration: 295800 loss: 0.0034 lr: 0.02\n",
            "iteration: 295810 loss: 0.0023 lr: 0.02\n",
            "iteration: 295820 loss: 0.0031 lr: 0.02\n",
            "iteration: 295830 loss: 0.0044 lr: 0.02\n",
            "iteration: 295840 loss: 0.0048 lr: 0.02\n",
            "iteration: 295850 loss: 0.0031 lr: 0.02\n",
            "iteration: 295860 loss: 0.0032 lr: 0.02\n",
            "iteration: 295870 loss: 0.0032 lr: 0.02\n",
            "iteration: 295880 loss: 0.0040 lr: 0.02\n",
            "iteration: 295890 loss: 0.0042 lr: 0.02\n",
            "iteration: 295900 loss: 0.0033 lr: 0.02\n",
            "iteration: 295910 loss: 0.0029 lr: 0.02\n",
            "iteration: 295920 loss: 0.0032 lr: 0.02\n",
            "iteration: 295930 loss: 0.0030 lr: 0.02\n",
            "iteration: 295940 loss: 0.0028 lr: 0.02\n",
            "iteration: 295950 loss: 0.0027 lr: 0.02\n",
            "iteration: 295960 loss: 0.0031 lr: 0.02\n",
            "iteration: 295970 loss: 0.0029 lr: 0.02\n",
            "iteration: 295980 loss: 0.0033 lr: 0.02\n",
            "iteration: 295990 loss: 0.0027 lr: 0.02\n",
            "iteration: 296000 loss: 0.0039 lr: 0.02\n",
            "iteration: 296010 loss: 0.0025 lr: 0.02\n",
            "iteration: 296020 loss: 0.0026 lr: 0.02\n",
            "iteration: 296030 loss: 0.0046 lr: 0.02\n",
            "iteration: 296040 loss: 0.0043 lr: 0.02\n",
            "iteration: 296050 loss: 0.0057 lr: 0.02\n",
            "iteration: 296060 loss: 0.0045 lr: 0.02\n",
            "iteration: 296070 loss: 0.0052 lr: 0.02\n",
            "iteration: 296080 loss: 0.0034 lr: 0.02\n",
            "iteration: 296090 loss: 0.0039 lr: 0.02\n",
            "iteration: 296100 loss: 0.0052 lr: 0.02\n",
            "iteration: 296110 loss: 0.0042 lr: 0.02\n",
            "iteration: 296120 loss: 0.0041 lr: 0.02\n",
            "iteration: 296130 loss: 0.0029 lr: 0.02\n",
            "iteration: 296140 loss: 0.0042 lr: 0.02\n",
            "iteration: 296150 loss: 0.0054 lr: 0.02\n",
            "iteration: 296160 loss: 0.0043 lr: 0.02\n",
            "iteration: 296170 loss: 0.0034 lr: 0.02\n",
            "iteration: 296180 loss: 0.0037 lr: 0.02\n",
            "iteration: 296190 loss: 0.0046 lr: 0.02\n",
            "iteration: 296200 loss: 0.0041 lr: 0.02\n",
            "iteration: 296210 loss: 0.0072 lr: 0.02\n",
            "iteration: 296220 loss: 0.0054 lr: 0.02\n",
            "iteration: 296230 loss: 0.0082 lr: 0.02\n",
            "iteration: 296240 loss: 0.0043 lr: 0.02\n",
            "iteration: 296250 loss: 0.0050 lr: 0.02\n",
            "iteration: 296260 loss: 0.0037 lr: 0.02\n",
            "iteration: 296270 loss: 0.0033 lr: 0.02\n",
            "iteration: 296280 loss: 0.0027 lr: 0.02\n",
            "iteration: 296290 loss: 0.0037 lr: 0.02\n",
            "iteration: 296300 loss: 0.0045 lr: 0.02\n",
            "iteration: 296310 loss: 0.0030 lr: 0.02\n",
            "iteration: 296320 loss: 0.0035 lr: 0.02\n",
            "iteration: 296330 loss: 0.0048 lr: 0.02\n",
            "iteration: 296340 loss: 0.0051 lr: 0.02\n",
            "iteration: 296350 loss: 0.0037 lr: 0.02\n",
            "iteration: 296360 loss: 0.0040 lr: 0.02\n",
            "iteration: 296370 loss: 0.0046 lr: 0.02\n",
            "iteration: 296380 loss: 0.0050 lr: 0.02\n",
            "iteration: 296390 loss: 0.0044 lr: 0.02\n",
            "iteration: 296400 loss: 0.0034 lr: 0.02\n",
            "iteration: 296410 loss: 0.0031 lr: 0.02\n",
            "iteration: 296420 loss: 0.0031 lr: 0.02\n",
            "iteration: 296430 loss: 0.0031 lr: 0.02\n",
            "iteration: 296440 loss: 0.0038 lr: 0.02\n",
            "iteration: 296450 loss: 0.0035 lr: 0.02\n",
            "iteration: 296460 loss: 0.0045 lr: 0.02\n",
            "iteration: 296470 loss: 0.0035 lr: 0.02\n",
            "iteration: 296480 loss: 0.0035 lr: 0.02\n",
            "iteration: 296490 loss: 0.0040 lr: 0.02\n",
            "iteration: 296500 loss: 0.0039 lr: 0.02\n",
            "iteration: 296510 loss: 0.0035 lr: 0.02\n",
            "iteration: 296520 loss: 0.0043 lr: 0.02\n",
            "iteration: 296530 loss: 0.0029 lr: 0.02\n",
            "iteration: 296540 loss: 0.0030 lr: 0.02\n",
            "iteration: 296550 loss: 0.0037 lr: 0.02\n",
            "iteration: 296560 loss: 0.0037 lr: 0.02\n",
            "iteration: 296570 loss: 0.0031 lr: 0.02\n",
            "iteration: 296580 loss: 0.0036 lr: 0.02\n",
            "iteration: 296590 loss: 0.0039 lr: 0.02\n",
            "iteration: 296600 loss: 0.0050 lr: 0.02\n",
            "iteration: 296610 loss: 0.0036 lr: 0.02\n",
            "iteration: 296620 loss: 0.0027 lr: 0.02\n",
            "iteration: 296630 loss: 0.0034 lr: 0.02\n",
            "iteration: 296640 loss: 0.0032 lr: 0.02\n",
            "iteration: 296650 loss: 0.0033 lr: 0.02\n",
            "iteration: 296660 loss: 0.0041 lr: 0.02\n",
            "iteration: 296670 loss: 0.0028 lr: 0.02\n",
            "iteration: 296680 loss: 0.0044 lr: 0.02\n",
            "iteration: 296690 loss: 0.0029 lr: 0.02\n",
            "iteration: 296700 loss: 0.0042 lr: 0.02\n",
            "iteration: 296710 loss: 0.0046 lr: 0.02\n",
            "iteration: 296720 loss: 0.0046 lr: 0.02\n",
            "iteration: 296730 loss: 0.0041 lr: 0.02\n",
            "iteration: 296740 loss: 0.0039 lr: 0.02\n",
            "iteration: 296750 loss: 0.0030 lr: 0.02\n",
            "iteration: 296760 loss: 0.0034 lr: 0.02\n",
            "iteration: 296770 loss: 0.0035 lr: 0.02\n",
            "iteration: 296780 loss: 0.0045 lr: 0.02\n",
            "iteration: 296790 loss: 0.0033 lr: 0.02\n",
            "iteration: 296800 loss: 0.0027 lr: 0.02\n",
            "iteration: 296810 loss: 0.0034 lr: 0.02\n",
            "iteration: 296820 loss: 0.0048 lr: 0.02\n",
            "iteration: 296830 loss: 0.0027 lr: 0.02\n",
            "iteration: 296840 loss: 0.0049 lr: 0.02\n",
            "iteration: 296850 loss: 0.0042 lr: 0.02\n",
            "iteration: 296860 loss: 0.0034 lr: 0.02\n",
            "iteration: 296870 loss: 0.0039 lr: 0.02\n",
            "iteration: 296880 loss: 0.0047 lr: 0.02\n",
            "iteration: 296890 loss: 0.0039 lr: 0.02\n",
            "iteration: 296900 loss: 0.0038 lr: 0.02\n",
            "iteration: 296910 loss: 0.0028 lr: 0.02\n",
            "iteration: 296920 loss: 0.0036 lr: 0.02\n",
            "iteration: 296930 loss: 0.0025 lr: 0.02\n",
            "iteration: 296940 loss: 0.0023 lr: 0.02\n",
            "iteration: 296950 loss: 0.0035 lr: 0.02\n",
            "iteration: 296960 loss: 0.0037 lr: 0.02\n",
            "iteration: 296970 loss: 0.0044 lr: 0.02\n",
            "iteration: 296980 loss: 0.0055 lr: 0.02\n",
            "iteration: 296990 loss: 0.0031 lr: 0.02\n",
            "iteration: 297000 loss: 0.0033 lr: 0.02\n",
            "iteration: 297010 loss: 0.0039 lr: 0.02\n",
            "iteration: 297020 loss: 0.0036 lr: 0.02\n",
            "iteration: 297030 loss: 0.0028 lr: 0.02\n",
            "iteration: 297040 loss: 0.0032 lr: 0.02\n",
            "iteration: 297050 loss: 0.0025 lr: 0.02\n",
            "iteration: 297060 loss: 0.0028 lr: 0.02\n",
            "iteration: 297070 loss: 0.0040 lr: 0.02\n",
            "iteration: 297080 loss: 0.0032 lr: 0.02\n",
            "iteration: 297090 loss: 0.0048 lr: 0.02\n",
            "iteration: 297100 loss: 0.0032 lr: 0.02\n",
            "iteration: 297110 loss: 0.0035 lr: 0.02\n",
            "iteration: 297120 loss: 0.0042 lr: 0.02\n",
            "iteration: 297130 loss: 0.0036 lr: 0.02\n",
            "iteration: 297140 loss: 0.0028 lr: 0.02\n",
            "iteration: 297150 loss: 0.0032 lr: 0.02\n",
            "iteration: 297160 loss: 0.0050 lr: 0.02\n",
            "iteration: 297170 loss: 0.0047 lr: 0.02\n",
            "iteration: 297180 loss: 0.0028 lr: 0.02\n",
            "iteration: 297190 loss: 0.0034 lr: 0.02\n",
            "iteration: 297200 loss: 0.0031 lr: 0.02\n",
            "iteration: 297210 loss: 0.0035 lr: 0.02\n",
            "iteration: 297220 loss: 0.0034 lr: 0.02\n",
            "iteration: 297230 loss: 0.0030 lr: 0.02\n",
            "iteration: 297240 loss: 0.0033 lr: 0.02\n",
            "iteration: 297250 loss: 0.0022 lr: 0.02\n",
            "iteration: 297260 loss: 0.0031 lr: 0.02\n",
            "iteration: 297270 loss: 0.0040 lr: 0.02\n",
            "iteration: 297280 loss: 0.0037 lr: 0.02\n",
            "iteration: 297290 loss: 0.0030 lr: 0.02\n",
            "iteration: 297300 loss: 0.0038 lr: 0.02\n",
            "iteration: 297310 loss: 0.0037 lr: 0.02\n",
            "iteration: 297320 loss: 0.0032 lr: 0.02\n",
            "iteration: 297330 loss: 0.0040 lr: 0.02\n",
            "iteration: 297340 loss: 0.0039 lr: 0.02\n",
            "iteration: 297350 loss: 0.0044 lr: 0.02\n",
            "iteration: 297360 loss: 0.0036 lr: 0.02\n",
            "iteration: 297370 loss: 0.0038 lr: 0.02\n",
            "iteration: 297380 loss: 0.0033 lr: 0.02\n",
            "iteration: 297390 loss: 0.0037 lr: 0.02\n",
            "iteration: 297400 loss: 0.0032 lr: 0.02\n",
            "iteration: 297410 loss: 0.0033 lr: 0.02\n",
            "iteration: 297420 loss: 0.0038 lr: 0.02\n",
            "iteration: 297430 loss: 0.0040 lr: 0.02\n",
            "iteration: 297440 loss: 0.0027 lr: 0.02\n",
            "iteration: 297450 loss: 0.0035 lr: 0.02\n",
            "iteration: 297460 loss: 0.0036 lr: 0.02\n",
            "iteration: 297470 loss: 0.0038 lr: 0.02\n",
            "iteration: 297480 loss: 0.0029 lr: 0.02\n",
            "iteration: 297490 loss: 0.0031 lr: 0.02\n",
            "iteration: 297500 loss: 0.0035 lr: 0.02\n",
            "iteration: 297510 loss: 0.0031 lr: 0.02\n",
            "iteration: 297520 loss: 0.0032 lr: 0.02\n",
            "iteration: 297530 loss: 0.0043 lr: 0.02\n",
            "iteration: 297540 loss: 0.0030 lr: 0.02\n",
            "iteration: 297550 loss: 0.0029 lr: 0.02\n",
            "iteration: 297560 loss: 0.0029 lr: 0.02\n",
            "iteration: 297570 loss: 0.0038 lr: 0.02\n",
            "iteration: 297580 loss: 0.0038 lr: 0.02\n",
            "iteration: 297590 loss: 0.0036 lr: 0.02\n",
            "iteration: 297600 loss: 0.0037 lr: 0.02\n",
            "iteration: 297610 loss: 0.0036 lr: 0.02\n",
            "iteration: 297620 loss: 0.0051 lr: 0.02\n",
            "iteration: 297630 loss: 0.0027 lr: 0.02\n",
            "iteration: 297640 loss: 0.0046 lr: 0.02\n",
            "iteration: 297650 loss: 0.0038 lr: 0.02\n",
            "iteration: 297660 loss: 0.0031 lr: 0.02\n",
            "iteration: 297670 loss: 0.0038 lr: 0.02\n",
            "iteration: 297680 loss: 0.0040 lr: 0.02\n",
            "iteration: 297690 loss: 0.0041 lr: 0.02\n",
            "iteration: 297700 loss: 0.0037 lr: 0.02\n",
            "iteration: 297710 loss: 0.0043 lr: 0.02\n",
            "iteration: 297720 loss: 0.0028 lr: 0.02\n",
            "iteration: 297730 loss: 0.0029 lr: 0.02\n",
            "iteration: 297740 loss: 0.0051 lr: 0.02\n",
            "iteration: 297750 loss: 0.0034 lr: 0.02\n",
            "iteration: 297760 loss: 0.0045 lr: 0.02\n",
            "iteration: 297770 loss: 0.0045 lr: 0.02\n",
            "iteration: 297780 loss: 0.0037 lr: 0.02\n",
            "iteration: 297790 loss: 0.0031 lr: 0.02\n",
            "iteration: 297800 loss: 0.0027 lr: 0.02\n",
            "iteration: 297810 loss: 0.0037 lr: 0.02\n",
            "iteration: 297820 loss: 0.0037 lr: 0.02\n",
            "iteration: 297830 loss: 0.0033 lr: 0.02\n",
            "iteration: 297840 loss: 0.0033 lr: 0.02\n",
            "iteration: 297850 loss: 0.0041 lr: 0.02\n",
            "iteration: 297860 loss: 0.0034 lr: 0.02\n",
            "iteration: 297870 loss: 0.0037 lr: 0.02\n",
            "iteration: 297880 loss: 0.0038 lr: 0.02\n",
            "iteration: 297890 loss: 0.0041 lr: 0.02\n",
            "iteration: 297900 loss: 0.0034 lr: 0.02\n",
            "iteration: 297910 loss: 0.0040 lr: 0.02\n",
            "iteration: 297920 loss: 0.0044 lr: 0.02\n",
            "iteration: 297930 loss: 0.0030 lr: 0.02\n",
            "iteration: 297940 loss: 0.0047 lr: 0.02\n",
            "iteration: 297950 loss: 0.0027 lr: 0.02\n",
            "iteration: 297960 loss: 0.0034 lr: 0.02\n",
            "iteration: 297970 loss: 0.0022 lr: 0.02\n",
            "iteration: 297980 loss: 0.0031 lr: 0.02\n",
            "iteration: 297990 loss: 0.0031 lr: 0.02\n",
            "iteration: 298000 loss: 0.0029 lr: 0.02\n",
            "iteration: 298010 loss: 0.0032 lr: 0.02\n",
            "iteration: 298020 loss: 0.0031 lr: 0.02\n",
            "iteration: 298030 loss: 0.0031 lr: 0.02\n",
            "iteration: 298040 loss: 0.0037 lr: 0.02\n",
            "iteration: 298050 loss: 0.0043 lr: 0.02\n",
            "iteration: 298060 loss: 0.0026 lr: 0.02\n",
            "iteration: 298070 loss: 0.0035 lr: 0.02\n",
            "iteration: 298080 loss: 0.0036 lr: 0.02\n",
            "iteration: 298090 loss: 0.0035 lr: 0.02\n",
            "iteration: 298100 loss: 0.0046 lr: 0.02\n",
            "iteration: 298110 loss: 0.0039 lr: 0.02\n",
            "iteration: 298120 loss: 0.0041 lr: 0.02\n",
            "iteration: 298130 loss: 0.0036 lr: 0.02\n",
            "iteration: 298140 loss: 0.0034 lr: 0.02\n",
            "iteration: 298150 loss: 0.0034 lr: 0.02\n",
            "iteration: 298160 loss: 0.0039 lr: 0.02\n",
            "iteration: 298170 loss: 0.0037 lr: 0.02\n",
            "iteration: 298180 loss: 0.0033 lr: 0.02\n",
            "iteration: 298190 loss: 0.0043 lr: 0.02\n",
            "iteration: 298200 loss: 0.0036 lr: 0.02\n",
            "iteration: 298210 loss: 0.0040 lr: 0.02\n",
            "iteration: 298220 loss: 0.0030 lr: 0.02\n",
            "iteration: 298230 loss: 0.0037 lr: 0.02\n",
            "iteration: 298240 loss: 0.0035 lr: 0.02\n",
            "iteration: 298250 loss: 0.0053 lr: 0.02\n",
            "iteration: 298260 loss: 0.0037 lr: 0.02\n",
            "iteration: 298270 loss: 0.0046 lr: 0.02\n",
            "iteration: 298280 loss: 0.0035 lr: 0.02\n",
            "iteration: 298290 loss: 0.0043 lr: 0.02\n",
            "iteration: 298300 loss: 0.0034 lr: 0.02\n",
            "iteration: 298310 loss: 0.0044 lr: 0.02\n",
            "iteration: 298320 loss: 0.0031 lr: 0.02\n",
            "iteration: 298330 loss: 0.0033 lr: 0.02\n",
            "iteration: 298340 loss: 0.0031 lr: 0.02\n",
            "iteration: 298350 loss: 0.0041 lr: 0.02\n",
            "iteration: 298360 loss: 0.0038 lr: 0.02\n",
            "iteration: 298370 loss: 0.0035 lr: 0.02\n",
            "iteration: 298380 loss: 0.0029 lr: 0.02\n",
            "iteration: 298390 loss: 0.0039 lr: 0.02\n",
            "iteration: 298400 loss: 0.0039 lr: 0.02\n",
            "iteration: 298410 loss: 0.0044 lr: 0.02\n",
            "iteration: 298420 loss: 0.0023 lr: 0.02\n",
            "iteration: 298430 loss: 0.0033 lr: 0.02\n",
            "iteration: 298440 loss: 0.0043 lr: 0.02\n",
            "iteration: 298450 loss: 0.0043 lr: 0.02\n",
            "iteration: 298460 loss: 0.0030 lr: 0.02\n",
            "iteration: 298470 loss: 0.0036 lr: 0.02\n",
            "iteration: 298480 loss: 0.0039 lr: 0.02\n",
            "iteration: 298490 loss: 0.0036 lr: 0.02\n",
            "iteration: 298500 loss: 0.0029 lr: 0.02\n",
            "iteration: 298510 loss: 0.0032 lr: 0.02\n",
            "iteration: 298520 loss: 0.0049 lr: 0.02\n",
            "iteration: 298530 loss: 0.0034 lr: 0.02\n",
            "iteration: 298540 loss: 0.0027 lr: 0.02\n",
            "iteration: 298550 loss: 0.0034 lr: 0.02\n",
            "iteration: 298560 loss: 0.0035 lr: 0.02\n",
            "iteration: 298570 loss: 0.0034 lr: 0.02\n",
            "iteration: 298580 loss: 0.0030 lr: 0.02\n",
            "iteration: 298590 loss: 0.0029 lr: 0.02\n",
            "iteration: 298600 loss: 0.0032 lr: 0.02\n",
            "iteration: 298610 loss: 0.0034 lr: 0.02\n",
            "iteration: 298620 loss: 0.0035 lr: 0.02\n",
            "iteration: 298630 loss: 0.0031 lr: 0.02\n",
            "iteration: 298640 loss: 0.0038 lr: 0.02\n",
            "iteration: 298650 loss: 0.0028 lr: 0.02\n",
            "iteration: 298660 loss: 0.0025 lr: 0.02\n",
            "iteration: 298670 loss: 0.0053 lr: 0.02\n",
            "iteration: 298680 loss: 0.0034 lr: 0.02\n",
            "iteration: 298690 loss: 0.0044 lr: 0.02\n",
            "iteration: 298700 loss: 0.0044 lr: 0.02\n",
            "iteration: 298710 loss: 0.0033 lr: 0.02\n",
            "iteration: 298720 loss: 0.0036 lr: 0.02\n",
            "iteration: 298730 loss: 0.0030 lr: 0.02\n",
            "iteration: 298740 loss: 0.0031 lr: 0.02\n",
            "iteration: 298750 loss: 0.0026 lr: 0.02\n",
            "iteration: 298760 loss: 0.0028 lr: 0.02\n",
            "iteration: 298770 loss: 0.0025 lr: 0.02\n",
            "iteration: 298780 loss: 0.0023 lr: 0.02\n",
            "iteration: 298790 loss: 0.0053 lr: 0.02\n",
            "iteration: 298800 loss: 0.0049 lr: 0.02\n",
            "iteration: 298810 loss: 0.0038 lr: 0.02\n",
            "iteration: 298820 loss: 0.0025 lr: 0.02\n",
            "iteration: 298830 loss: 0.0035 lr: 0.02\n",
            "iteration: 298840 loss: 0.0041 lr: 0.02\n",
            "iteration: 298850 loss: 0.0028 lr: 0.02\n",
            "iteration: 298860 loss: 0.0032 lr: 0.02\n",
            "iteration: 298870 loss: 0.0045 lr: 0.02\n",
            "iteration: 298880 loss: 0.0029 lr: 0.02\n",
            "iteration: 298890 loss: 0.0032 lr: 0.02\n",
            "iteration: 298900 loss: 0.0032 lr: 0.02\n",
            "iteration: 298910 loss: 0.0031 lr: 0.02\n",
            "iteration: 298920 loss: 0.0031 lr: 0.02\n",
            "iteration: 298930 loss: 0.0044 lr: 0.02\n",
            "iteration: 298940 loss: 0.0022 lr: 0.02\n",
            "iteration: 298950 loss: 0.0047 lr: 0.02\n",
            "iteration: 298960 loss: 0.0035 lr: 0.02\n",
            "iteration: 298970 loss: 0.0038 lr: 0.02\n",
            "iteration: 298980 loss: 0.0029 lr: 0.02\n",
            "iteration: 298990 loss: 0.0032 lr: 0.02\n",
            "iteration: 299000 loss: 0.0047 lr: 0.02\n",
            "iteration: 299010 loss: 0.0041 lr: 0.02\n",
            "iteration: 299020 loss: 0.0044 lr: 0.02\n",
            "iteration: 299030 loss: 0.0036 lr: 0.02\n",
            "iteration: 299040 loss: 0.0034 lr: 0.02\n",
            "iteration: 299050 loss: 0.0031 lr: 0.02\n",
            "iteration: 299060 loss: 0.0027 lr: 0.02\n",
            "iteration: 299070 loss: 0.0035 lr: 0.02\n",
            "iteration: 299080 loss: 0.0028 lr: 0.02\n",
            "iteration: 299090 loss: 0.0040 lr: 0.02\n",
            "iteration: 299100 loss: 0.0037 lr: 0.02\n",
            "iteration: 299110 loss: 0.0051 lr: 0.02\n",
            "iteration: 299120 loss: 0.0030 lr: 0.02\n",
            "iteration: 299130 loss: 0.0034 lr: 0.02\n",
            "iteration: 299140 loss: 0.0034 lr: 0.02\n",
            "iteration: 299150 loss: 0.0034 lr: 0.02\n",
            "iteration: 299160 loss: 0.0034 lr: 0.02\n",
            "iteration: 299170 loss: 0.0026 lr: 0.02\n",
            "iteration: 299180 loss: 0.0028 lr: 0.02\n",
            "iteration: 299190 loss: 0.0038 lr: 0.02\n",
            "iteration: 299200 loss: 0.0049 lr: 0.02\n",
            "iteration: 299210 loss: 0.0036 lr: 0.02\n",
            "iteration: 299220 loss: 0.0039 lr: 0.02\n",
            "iteration: 299230 loss: 0.0024 lr: 0.02\n",
            "iteration: 299240 loss: 0.0034 lr: 0.02\n",
            "iteration: 299250 loss: 0.0042 lr: 0.02\n",
            "iteration: 299260 loss: 0.0034 lr: 0.02\n",
            "iteration: 299270 loss: 0.0028 lr: 0.02\n",
            "iteration: 299280 loss: 0.0031 lr: 0.02\n",
            "iteration: 299290 loss: 0.0029 lr: 0.02\n",
            "iteration: 299300 loss: 0.0036 lr: 0.02\n",
            "iteration: 299310 loss: 0.0025 lr: 0.02\n",
            "iteration: 299320 loss: 0.0028 lr: 0.02\n",
            "iteration: 299330 loss: 0.0034 lr: 0.02\n",
            "iteration: 299340 loss: 0.0025 lr: 0.02\n",
            "iteration: 299350 loss: 0.0038 lr: 0.02\n",
            "iteration: 299360 loss: 0.0030 lr: 0.02\n",
            "iteration: 299370 loss: 0.0028 lr: 0.02\n",
            "iteration: 299380 loss: 0.0048 lr: 0.02\n",
            "iteration: 299390 loss: 0.0041 lr: 0.02\n",
            "iteration: 299400 loss: 0.0036 lr: 0.02\n",
            "iteration: 299410 loss: 0.0036 lr: 0.02\n",
            "iteration: 299420 loss: 0.0037 lr: 0.02\n",
            "iteration: 299430 loss: 0.0045 lr: 0.02\n",
            "iteration: 299440 loss: 0.0039 lr: 0.02\n",
            "iteration: 299450 loss: 0.0045 lr: 0.02\n",
            "iteration: 299460 loss: 0.0027 lr: 0.02\n",
            "iteration: 299470 loss: 0.0033 lr: 0.02\n",
            "iteration: 299480 loss: 0.0034 lr: 0.02\n",
            "iteration: 299490 loss: 0.0041 lr: 0.02\n",
            "iteration: 299500 loss: 0.0031 lr: 0.02\n",
            "iteration: 299510 loss: 0.0042 lr: 0.02\n",
            "iteration: 299520 loss: 0.0039 lr: 0.02\n",
            "iteration: 299530 loss: 0.0037 lr: 0.02\n",
            "iteration: 299540 loss: 0.0035 lr: 0.02\n",
            "iteration: 299550 loss: 0.0029 lr: 0.02\n",
            "iteration: 299560 loss: 0.0038 lr: 0.02\n",
            "iteration: 299570 loss: 0.0039 lr: 0.02\n",
            "iteration: 299580 loss: 0.0035 lr: 0.02\n",
            "iteration: 299590 loss: 0.0033 lr: 0.02\n",
            "iteration: 299600 loss: 0.0031 lr: 0.02\n",
            "iteration: 299610 loss: 0.0033 lr: 0.02\n",
            "iteration: 299620 loss: 0.0039 lr: 0.02\n",
            "iteration: 299630 loss: 0.0034 lr: 0.02\n",
            "iteration: 299640 loss: 0.0017 lr: 0.02\n",
            "iteration: 299650 loss: 0.0034 lr: 0.02\n",
            "iteration: 299660 loss: 0.0025 lr: 0.02\n",
            "iteration: 299670 loss: 0.0034 lr: 0.02\n",
            "iteration: 299680 loss: 0.0027 lr: 0.02\n",
            "iteration: 299690 loss: 0.0031 lr: 0.02\n",
            "iteration: 299700 loss: 0.0030 lr: 0.02\n",
            "iteration: 299710 loss: 0.0046 lr: 0.02\n",
            "iteration: 299720 loss: 0.0033 lr: 0.02\n",
            "iteration: 299730 loss: 0.0034 lr: 0.02\n",
            "iteration: 299740 loss: 0.0036 lr: 0.02\n",
            "iteration: 299750 loss: 0.0027 lr: 0.02\n",
            "iteration: 299760 loss: 0.0037 lr: 0.02\n",
            "iteration: 299770 loss: 0.0036 lr: 0.02\n",
            "iteration: 299780 loss: 0.0039 lr: 0.02\n",
            "iteration: 299790 loss: 0.0033 lr: 0.02\n",
            "iteration: 299800 loss: 0.0040 lr: 0.02\n",
            "iteration: 299810 loss: 0.0034 lr: 0.02\n",
            "iteration: 299820 loss: 0.0034 lr: 0.02\n",
            "iteration: 299830 loss: 0.0049 lr: 0.02\n",
            "iteration: 299840 loss: 0.0027 lr: 0.02\n",
            "iteration: 299850 loss: 0.0032 lr: 0.02\n",
            "iteration: 299860 loss: 0.0028 lr: 0.02\n",
            "iteration: 299870 loss: 0.0034 lr: 0.02\n",
            "iteration: 299880 loss: 0.0034 lr: 0.02\n",
            "iteration: 299890 loss: 0.0031 lr: 0.02\n",
            "iteration: 299900 loss: 0.0035 lr: 0.02\n",
            "iteration: 299910 loss: 0.0035 lr: 0.02\n",
            "iteration: 299920 loss: 0.0040 lr: 0.02\n",
            "iteration: 299930 loss: 0.0032 lr: 0.02\n",
            "iteration: 299940 loss: 0.0034 lr: 0.02\n",
            "iteration: 299950 loss: 0.0030 lr: 0.02\n",
            "iteration: 299960 loss: 0.0030 lr: 0.02\n",
            "iteration: 299970 loss: 0.0029 lr: 0.02\n",
            "iteration: 299980 loss: 0.0034 lr: 0.02\n",
            "iteration: 299990 loss: 0.0034 lr: 0.02\n",
            "iteration: 300000 loss: 0.0034 lr: 0.02\n",
            "iteration: 300010 loss: 0.0042 lr: 0.02\n",
            "iteration: 300020 loss: 0.0032 lr: 0.02\n",
            "iteration: 300030 loss: 0.0030 lr: 0.02\n",
            "iteration: 300040 loss: 0.0039 lr: 0.02\n",
            "iteration: 300050 loss: 0.0034 lr: 0.02\n",
            "iteration: 300060 loss: 0.0032 lr: 0.02\n",
            "iteration: 300070 loss: 0.0034 lr: 0.02\n",
            "iteration: 300080 loss: 0.0029 lr: 0.02\n",
            "iteration: 300090 loss: 0.0039 lr: 0.02\n",
            "iteration: 300100 loss: 0.0032 lr: 0.02\n",
            "iteration: 300110 loss: 0.0044 lr: 0.02\n",
            "iteration: 300120 loss: 0.0033 lr: 0.02\n",
            "iteration: 300130 loss: 0.0037 lr: 0.02\n",
            "iteration: 300140 loss: 0.0029 lr: 0.02\n",
            "iteration: 300150 loss: 0.0039 lr: 0.02\n",
            "iteration: 300160 loss: 0.0040 lr: 0.02\n",
            "iteration: 300170 loss: 0.0038 lr: 0.02\n",
            "iteration: 300180 loss: 0.0035 lr: 0.02\n",
            "iteration: 300190 loss: 0.0030 lr: 0.02\n",
            "iteration: 300200 loss: 0.0034 lr: 0.02\n",
            "iteration: 300210 loss: 0.0040 lr: 0.02\n",
            "iteration: 300220 loss: 0.0047 lr: 0.02\n",
            "iteration: 300230 loss: 0.0036 lr: 0.02\n",
            "iteration: 300240 loss: 0.0040 lr: 0.02\n",
            "iteration: 300250 loss: 0.0032 lr: 0.02\n",
            "iteration: 300260 loss: 0.0030 lr: 0.02\n",
            "iteration: 300270 loss: 0.0023 lr: 0.02\n",
            "iteration: 300280 loss: 0.0037 lr: 0.02\n",
            "iteration: 300290 loss: 0.0033 lr: 0.02\n",
            "iteration: 300300 loss: 0.0048 lr: 0.02\n",
            "iteration: 300310 loss: 0.0040 lr: 0.02\n",
            "iteration: 300320 loss: 0.0026 lr: 0.02\n",
            "iteration: 300330 loss: 0.0028 lr: 0.02\n",
            "iteration: 300340 loss: 0.0034 lr: 0.02\n",
            "iteration: 300350 loss: 0.0038 lr: 0.02\n",
            "iteration: 300360 loss: 0.0040 lr: 0.02\n",
            "iteration: 300370 loss: 0.0032 lr: 0.02\n",
            "iteration: 300380 loss: 0.0030 lr: 0.02\n",
            "iteration: 300390 loss: 0.0034 lr: 0.02\n",
            "iteration: 300400 loss: 0.0034 lr: 0.02\n",
            "iteration: 300410 loss: 0.0033 lr: 0.02\n",
            "iteration: 300420 loss: 0.0032 lr: 0.02\n",
            "iteration: 300430 loss: 0.0037 lr: 0.02\n",
            "iteration: 300440 loss: 0.0031 lr: 0.02\n",
            "iteration: 300450 loss: 0.0037 lr: 0.02\n",
            "iteration: 300460 loss: 0.0050 lr: 0.02\n",
            "iteration: 300470 loss: 0.0034 lr: 0.02\n",
            "iteration: 300480 loss: 0.0034 lr: 0.02\n",
            "iteration: 300490 loss: 0.0030 lr: 0.02\n",
            "iteration: 300500 loss: 0.0043 lr: 0.02\n",
            "iteration: 300510 loss: 0.0031 lr: 0.02\n",
            "iteration: 300520 loss: 0.0036 lr: 0.02\n",
            "iteration: 300530 loss: 0.0032 lr: 0.02\n",
            "iteration: 300540 loss: 0.0034 lr: 0.02\n",
            "iteration: 300550 loss: 0.0034 lr: 0.02\n",
            "iteration: 300560 loss: 0.0042 lr: 0.02\n",
            "iteration: 300570 loss: 0.0027 lr: 0.02\n",
            "iteration: 300580 loss: 0.0039 lr: 0.02\n",
            "iteration: 300590 loss: 0.0032 lr: 0.02\n",
            "iteration: 300600 loss: 0.0036 lr: 0.02\n",
            "iteration: 300610 loss: 0.0040 lr: 0.02\n",
            "iteration: 300620 loss: 0.0031 lr: 0.02\n",
            "iteration: 300630 loss: 0.0039 lr: 0.02\n",
            "iteration: 300640 loss: 0.0026 lr: 0.02\n",
            "iteration: 300650 loss: 0.0037 lr: 0.02\n",
            "iteration: 300660 loss: 0.0031 lr: 0.02\n",
            "iteration: 300670 loss: 0.0027 lr: 0.02\n",
            "iteration: 300680 loss: 0.0032 lr: 0.02\n",
            "iteration: 300690 loss: 0.0047 lr: 0.02\n",
            "iteration: 300700 loss: 0.0043 lr: 0.02\n",
            "iteration: 300710 loss: 0.0027 lr: 0.02\n",
            "iteration: 300720 loss: 0.0033 lr: 0.02\n",
            "iteration: 300730 loss: 0.0025 lr: 0.02\n",
            "iteration: 300740 loss: 0.0050 lr: 0.02\n",
            "iteration: 300750 loss: 0.0024 lr: 0.02\n",
            "iteration: 300760 loss: 0.0033 lr: 0.02\n",
            "iteration: 300770 loss: 0.0031 lr: 0.02\n",
            "iteration: 300780 loss: 0.0033 lr: 0.02\n",
            "iteration: 300790 loss: 0.0034 lr: 0.02\n",
            "iteration: 300800 loss: 0.0043 lr: 0.02\n",
            "iteration: 300810 loss: 0.0036 lr: 0.02\n",
            "iteration: 300820 loss: 0.0041 lr: 0.02\n",
            "iteration: 300830 loss: 0.0034 lr: 0.02\n",
            "iteration: 300840 loss: 0.0039 lr: 0.02\n",
            "iteration: 300850 loss: 0.0035 lr: 0.02\n",
            "iteration: 300860 loss: 0.0025 lr: 0.02\n",
            "iteration: 300870 loss: 0.0045 lr: 0.02\n",
            "iteration: 300880 loss: 0.0029 lr: 0.02\n",
            "iteration: 300890 loss: 0.0034 lr: 0.02\n",
            "iteration: 300900 loss: 0.0038 lr: 0.02\n",
            "iteration: 300910 loss: 0.0039 lr: 0.02\n",
            "iteration: 300920 loss: 0.0032 lr: 0.02\n",
            "iteration: 300930 loss: 0.0033 lr: 0.02\n",
            "iteration: 300940 loss: 0.0033 lr: 0.02\n",
            "iteration: 300950 loss: 0.0034 lr: 0.02\n",
            "iteration: 300960 loss: 0.0019 lr: 0.02\n",
            "iteration: 300970 loss: 0.0037 lr: 0.02\n",
            "iteration: 300980 loss: 0.0028 lr: 0.02\n",
            "iteration: 300990 loss: 0.0040 lr: 0.02\n",
            "iteration: 301000 loss: 0.0045 lr: 0.02\n",
            "iteration: 301010 loss: 0.0037 lr: 0.02\n",
            "iteration: 301020 loss: 0.0040 lr: 0.02\n",
            "iteration: 301030 loss: 0.0028 lr: 0.02\n",
            "iteration: 301040 loss: 0.0032 lr: 0.02\n",
            "iteration: 301050 loss: 0.0042 lr: 0.02\n",
            "iteration: 301060 loss: 0.0030 lr: 0.02\n",
            "iteration: 301070 loss: 0.0042 lr: 0.02\n",
            "iteration: 301080 loss: 0.0045 lr: 0.02\n",
            "iteration: 301090 loss: 0.0034 lr: 0.02\n",
            "iteration: 301100 loss: 0.0034 lr: 0.02\n",
            "iteration: 301110 loss: 0.0043 lr: 0.02\n",
            "iteration: 301120 loss: 0.0031 lr: 0.02\n",
            "iteration: 301130 loss: 0.0039 lr: 0.02\n",
            "iteration: 301140 loss: 0.0028 lr: 0.02\n",
            "iteration: 301150 loss: 0.0029 lr: 0.02\n",
            "iteration: 301160 loss: 0.0030 lr: 0.02\n",
            "iteration: 301170 loss: 0.0051 lr: 0.02\n",
            "iteration: 301180 loss: 0.0037 lr: 0.02\n",
            "iteration: 301190 loss: 0.0039 lr: 0.02\n",
            "iteration: 301200 loss: 0.0033 lr: 0.02\n",
            "iteration: 301210 loss: 0.0041 lr: 0.02\n",
            "iteration: 301220 loss: 0.0034 lr: 0.02\n",
            "iteration: 301230 loss: 0.0027 lr: 0.02\n",
            "iteration: 301240 loss: 0.0026 lr: 0.02\n",
            "iteration: 301250 loss: 0.0030 lr: 0.02\n",
            "iteration: 301260 loss: 0.0037 lr: 0.02\n",
            "iteration: 301270 loss: 0.0031 lr: 0.02\n",
            "iteration: 301280 loss: 0.0034 lr: 0.02\n",
            "iteration: 301290 loss: 0.0036 lr: 0.02\n",
            "iteration: 301300 loss: 0.0031 lr: 0.02\n",
            "iteration: 301310 loss: 0.0034 lr: 0.02\n",
            "iteration: 301320 loss: 0.0025 lr: 0.02\n",
            "iteration: 301330 loss: 0.0027 lr: 0.02\n",
            "iteration: 301340 loss: 0.0031 lr: 0.02\n",
            "iteration: 301350 loss: 0.0035 lr: 0.02\n",
            "iteration: 301360 loss: 0.0046 lr: 0.02\n",
            "iteration: 301370 loss: 0.0032 lr: 0.02\n",
            "iteration: 301380 loss: 0.0037 lr: 0.02\n",
            "iteration: 301390 loss: 0.0030 lr: 0.02\n",
            "iteration: 301400 loss: 0.0034 lr: 0.02\n",
            "iteration: 301410 loss: 0.0043 lr: 0.02\n",
            "iteration: 301420 loss: 0.0055 lr: 0.02\n",
            "iteration: 301430 loss: 0.0037 lr: 0.02\n",
            "iteration: 301440 loss: 0.0041 lr: 0.02\n",
            "iteration: 301450 loss: 0.0035 lr: 0.02\n",
            "iteration: 301460 loss: 0.0043 lr: 0.02\n",
            "iteration: 301470 loss: 0.0036 lr: 0.02\n",
            "iteration: 301480 loss: 0.0029 lr: 0.02\n",
            "iteration: 301490 loss: 0.0043 lr: 0.02\n",
            "iteration: 301500 loss: 0.0033 lr: 0.02\n",
            "iteration: 301510 loss: 0.0041 lr: 0.02\n",
            "iteration: 301520 loss: 0.0036 lr: 0.02\n",
            "iteration: 301530 loss: 0.0032 lr: 0.02\n",
            "iteration: 301540 loss: 0.0036 lr: 0.02\n",
            "iteration: 301550 loss: 0.0034 lr: 0.02\n",
            "iteration: 301560 loss: 0.0038 lr: 0.02\n",
            "iteration: 301570 loss: 0.0038 lr: 0.02\n",
            "iteration: 301580 loss: 0.0032 lr: 0.02\n",
            "iteration: 301590 loss: 0.0032 lr: 0.02\n",
            "iteration: 301600 loss: 0.0035 lr: 0.02\n",
            "iteration: 301610 loss: 0.0027 lr: 0.02\n",
            "iteration: 301620 loss: 0.0032 lr: 0.02\n",
            "iteration: 301630 loss: 0.0037 lr: 0.02\n",
            "iteration: 301640 loss: 0.0038 lr: 0.02\n",
            "iteration: 301650 loss: 0.0043 lr: 0.02\n",
            "iteration: 301660 loss: 0.0033 lr: 0.02\n",
            "iteration: 301670 loss: 0.0045 lr: 0.02\n",
            "iteration: 301680 loss: 0.0041 lr: 0.02\n",
            "iteration: 301690 loss: 0.0045 lr: 0.02\n",
            "iteration: 301700 loss: 0.0030 lr: 0.02\n",
            "iteration: 301710 loss: 0.0033 lr: 0.02\n",
            "iteration: 301720 loss: 0.0035 lr: 0.02\n",
            "iteration: 301730 loss: 0.0034 lr: 0.02\n",
            "iteration: 301740 loss: 0.0036 lr: 0.02\n",
            "iteration: 301750 loss: 0.0028 lr: 0.02\n",
            "iteration: 301760 loss: 0.0039 lr: 0.02\n",
            "iteration: 301770 loss: 0.0035 lr: 0.02\n",
            "iteration: 301780 loss: 0.0037 lr: 0.02\n",
            "iteration: 301790 loss: 0.0029 lr: 0.02\n",
            "iteration: 301800 loss: 0.0042 lr: 0.02\n",
            "iteration: 301810 loss: 0.0034 lr: 0.02\n",
            "iteration: 301820 loss: 0.0032 lr: 0.02\n",
            "iteration: 301830 loss: 0.0046 lr: 0.02\n",
            "iteration: 301840 loss: 0.0034 lr: 0.02\n",
            "iteration: 301850 loss: 0.0031 lr: 0.02\n",
            "iteration: 301860 loss: 0.0040 lr: 0.02\n",
            "iteration: 301870 loss: 0.0033 lr: 0.02\n",
            "iteration: 301880 loss: 0.0042 lr: 0.02\n",
            "iteration: 301890 loss: 0.0041 lr: 0.02\n",
            "iteration: 301900 loss: 0.0038 lr: 0.02\n",
            "iteration: 301910 loss: 0.0040 lr: 0.02\n",
            "iteration: 301920 loss: 0.0030 lr: 0.02\n",
            "iteration: 301930 loss: 0.0026 lr: 0.02\n",
            "iteration: 301940 loss: 0.0029 lr: 0.02\n",
            "iteration: 301950 loss: 0.0035 lr: 0.02\n",
            "iteration: 301960 loss: 0.0041 lr: 0.02\n",
            "iteration: 301970 loss: 0.0039 lr: 0.02\n",
            "iteration: 301980 loss: 0.0033 lr: 0.02\n",
            "iteration: 301990 loss: 0.0028 lr: 0.02\n",
            "iteration: 302000 loss: 0.0040 lr: 0.02\n",
            "iteration: 302010 loss: 0.0044 lr: 0.02\n",
            "iteration: 302020 loss: 0.0034 lr: 0.02\n",
            "iteration: 302030 loss: 0.0031 lr: 0.02\n",
            "iteration: 302040 loss: 0.0043 lr: 0.02\n",
            "iteration: 302050 loss: 0.0024 lr: 0.02\n",
            "iteration: 302060 loss: 0.0042 lr: 0.02\n",
            "iteration: 302070 loss: 0.0048 lr: 0.02\n",
            "iteration: 302080 loss: 0.0046 lr: 0.02\n",
            "iteration: 302090 loss: 0.0037 lr: 0.02\n",
            "iteration: 302100 loss: 0.0025 lr: 0.02\n",
            "iteration: 302110 loss: 0.0045 lr: 0.02\n",
            "iteration: 302120 loss: 0.0041 lr: 0.02\n",
            "iteration: 302130 loss: 0.0036 lr: 0.02\n",
            "iteration: 302140 loss: 0.0032 lr: 0.02\n",
            "iteration: 302150 loss: 0.0038 lr: 0.02\n",
            "iteration: 302160 loss: 0.0035 lr: 0.02\n",
            "iteration: 302170 loss: 0.0036 lr: 0.02\n",
            "iteration: 302180 loss: 0.0043 lr: 0.02\n",
            "iteration: 302190 loss: 0.0046 lr: 0.02\n",
            "iteration: 302200 loss: 0.0039 lr: 0.02\n",
            "iteration: 302210 loss: 0.0031 lr: 0.02\n",
            "iteration: 302220 loss: 0.0035 lr: 0.02\n",
            "iteration: 302230 loss: 0.0035 lr: 0.02\n",
            "iteration: 302240 loss: 0.0039 lr: 0.02\n",
            "iteration: 302250 loss: 0.0054 lr: 0.02\n",
            "iteration: 302260 loss: 0.0047 lr: 0.02\n",
            "iteration: 302270 loss: 0.0038 lr: 0.02\n",
            "iteration: 302280 loss: 0.0032 lr: 0.02\n",
            "iteration: 302290 loss: 0.0034 lr: 0.02\n",
            "iteration: 302300 loss: 0.0032 lr: 0.02\n",
            "iteration: 302310 loss: 0.0030 lr: 0.02\n",
            "iteration: 302320 loss: 0.0038 lr: 0.02\n",
            "iteration: 302330 loss: 0.0029 lr: 0.02\n",
            "iteration: 302340 loss: 0.0034 lr: 0.02\n",
            "iteration: 302350 loss: 0.0027 lr: 0.02\n",
            "iteration: 302360 loss: 0.0032 lr: 0.02\n",
            "iteration: 302370 loss: 0.0045 lr: 0.02\n",
            "iteration: 302380 loss: 0.0038 lr: 0.02\n",
            "iteration: 302390 loss: 0.0035 lr: 0.02\n",
            "iteration: 302400 loss: 0.0030 lr: 0.02\n",
            "iteration: 302410 loss: 0.0045 lr: 0.02\n",
            "iteration: 302420 loss: 0.0042 lr: 0.02\n",
            "iteration: 302430 loss: 0.0034 lr: 0.02\n",
            "iteration: 302440 loss: 0.0032 lr: 0.02\n",
            "iteration: 302450 loss: 0.0039 lr: 0.02\n",
            "iteration: 302460 loss: 0.0036 lr: 0.02\n",
            "iteration: 302470 loss: 0.0039 lr: 0.02\n",
            "iteration: 302480 loss: 0.0034 lr: 0.02\n",
            "iteration: 302490 loss: 0.0030 lr: 0.02\n",
            "iteration: 302500 loss: 0.0029 lr: 0.02\n",
            "iteration: 302510 loss: 0.0043 lr: 0.02\n",
            "iteration: 302520 loss: 0.0041 lr: 0.02\n",
            "iteration: 302530 loss: 0.0039 lr: 0.02\n",
            "iteration: 302540 loss: 0.0027 lr: 0.02\n",
            "iteration: 302550 loss: 0.0029 lr: 0.02\n",
            "iteration: 302560 loss: 0.0033 lr: 0.02\n",
            "iteration: 302570 loss: 0.0027 lr: 0.02\n",
            "iteration: 302580 loss: 0.0038 lr: 0.02\n",
            "iteration: 302590 loss: 0.0044 lr: 0.02\n",
            "iteration: 302600 loss: 0.0034 lr: 0.02\n",
            "iteration: 302610 loss: 0.0030 lr: 0.02\n",
            "iteration: 302620 loss: 0.0034 lr: 0.02\n",
            "iteration: 302630 loss: 0.0034 lr: 0.02\n",
            "iteration: 302640 loss: 0.0029 lr: 0.02\n",
            "iteration: 302650 loss: 0.0027 lr: 0.02\n",
            "iteration: 302660 loss: 0.0041 lr: 0.02\n",
            "iteration: 302670 loss: 0.0029 lr: 0.02\n",
            "iteration: 302680 loss: 0.0034 lr: 0.02\n",
            "iteration: 302690 loss: 0.0033 lr: 0.02\n",
            "iteration: 302700 loss: 0.0039 lr: 0.02\n",
            "iteration: 302710 loss: 0.0040 lr: 0.02\n",
            "iteration: 302720 loss: 0.0038 lr: 0.02\n",
            "iteration: 302730 loss: 0.0035 lr: 0.02\n",
            "iteration: 302740 loss: 0.0049 lr: 0.02\n",
            "iteration: 302750 loss: 0.0040 lr: 0.02\n",
            "iteration: 302760 loss: 0.0052 lr: 0.02\n",
            "iteration: 302770 loss: 0.0030 lr: 0.02\n",
            "iteration: 302780 loss: 0.0031 lr: 0.02\n",
            "iteration: 302790 loss: 0.0031 lr: 0.02\n",
            "iteration: 302800 loss: 0.0033 lr: 0.02\n",
            "iteration: 302810 loss: 0.0036 lr: 0.02\n",
            "iteration: 302820 loss: 0.0029 lr: 0.02\n",
            "iteration: 302830 loss: 0.0036 lr: 0.02\n",
            "iteration: 302840 loss: 0.0050 lr: 0.02\n",
            "iteration: 302850 loss: 0.0028 lr: 0.02\n",
            "iteration: 302860 loss: 0.0035 lr: 0.02\n",
            "iteration: 302870 loss: 0.0033 lr: 0.02\n",
            "iteration: 302880 loss: 0.0026 lr: 0.02\n",
            "iteration: 302890 loss: 0.0030 lr: 0.02\n",
            "iteration: 302900 loss: 0.0046 lr: 0.02\n",
            "iteration: 302910 loss: 0.0032 lr: 0.02\n",
            "iteration: 302920 loss: 0.0035 lr: 0.02\n",
            "iteration: 302930 loss: 0.0040 lr: 0.02\n",
            "iteration: 302940 loss: 0.0050 lr: 0.02\n",
            "iteration: 302950 loss: 0.0033 lr: 0.02\n",
            "iteration: 302960 loss: 0.0034 lr: 0.02\n",
            "iteration: 302970 loss: 0.0041 lr: 0.02\n",
            "iteration: 302980 loss: 0.0040 lr: 0.02\n",
            "iteration: 302990 loss: 0.0035 lr: 0.02\n",
            "iteration: 303000 loss: 0.0038 lr: 0.02\n",
            "iteration: 303010 loss: 0.0029 lr: 0.02\n",
            "iteration: 303020 loss: 0.0041 lr: 0.02\n",
            "iteration: 303030 loss: 0.0042 lr: 0.02\n",
            "iteration: 303040 loss: 0.0030 lr: 0.02\n",
            "iteration: 303050 loss: 0.0038 lr: 0.02\n",
            "iteration: 303060 loss: 0.0030 lr: 0.02\n",
            "iteration: 303070 loss: 0.0040 lr: 0.02\n",
            "iteration: 303080 loss: 0.0041 lr: 0.02\n",
            "iteration: 303090 loss: 0.0041 lr: 0.02\n",
            "iteration: 303100 loss: 0.0033 lr: 0.02\n",
            "iteration: 303110 loss: 0.0027 lr: 0.02\n",
            "iteration: 303120 loss: 0.0045 lr: 0.02\n",
            "iteration: 303130 loss: 0.0032 lr: 0.02\n",
            "iteration: 303140 loss: 0.0036 lr: 0.02\n",
            "iteration: 303150 loss: 0.0033 lr: 0.02\n",
            "iteration: 303160 loss: 0.0037 lr: 0.02\n",
            "iteration: 303170 loss: 0.0032 lr: 0.02\n",
            "iteration: 303180 loss: 0.0035 lr: 0.02\n",
            "iteration: 303190 loss: 0.0033 lr: 0.02\n",
            "iteration: 303200 loss: 0.0036 lr: 0.02\n",
            "iteration: 303210 loss: 0.0051 lr: 0.02\n",
            "iteration: 303220 loss: 0.0036 lr: 0.02\n",
            "iteration: 303230 loss: 0.0032 lr: 0.02\n",
            "iteration: 303240 loss: 0.0043 lr: 0.02\n",
            "iteration: 303250 loss: 0.0032 lr: 0.02\n",
            "iteration: 303260 loss: 0.0028 lr: 0.02\n",
            "iteration: 303270 loss: 0.0034 lr: 0.02\n",
            "iteration: 303280 loss: 0.0041 lr: 0.02\n",
            "iteration: 303290 loss: 0.0032 lr: 0.02\n",
            "iteration: 303300 loss: 0.0040 lr: 0.02\n",
            "iteration: 303310 loss: 0.0033 lr: 0.02\n",
            "iteration: 303320 loss: 0.0043 lr: 0.02\n",
            "iteration: 303330 loss: 0.0028 lr: 0.02\n",
            "iteration: 303340 loss: 0.0031 lr: 0.02\n",
            "iteration: 303350 loss: 0.0045 lr: 0.02\n",
            "iteration: 303360 loss: 0.0038 lr: 0.02\n",
            "iteration: 303370 loss: 0.0034 lr: 0.02\n",
            "iteration: 303380 loss: 0.0038 lr: 0.02\n",
            "iteration: 303390 loss: 0.0026 lr: 0.02\n",
            "iteration: 303400 loss: 0.0034 lr: 0.02\n",
            "iteration: 303410 loss: 0.0032 lr: 0.02\n",
            "iteration: 303420 loss: 0.0038 lr: 0.02\n",
            "iteration: 303430 loss: 0.0033 lr: 0.02\n",
            "iteration: 303440 loss: 0.0036 lr: 0.02\n",
            "iteration: 303450 loss: 0.0038 lr: 0.02\n",
            "iteration: 303460 loss: 0.0029 lr: 0.02\n",
            "iteration: 303470 loss: 0.0037 lr: 0.02\n",
            "iteration: 303480 loss: 0.0033 lr: 0.02\n",
            "iteration: 303490 loss: 0.0042 lr: 0.02\n",
            "iteration: 303500 loss: 0.0036 lr: 0.02\n",
            "iteration: 303510 loss: 0.0031 lr: 0.02\n",
            "iteration: 303520 loss: 0.0053 lr: 0.02\n",
            "iteration: 303530 loss: 0.0042 lr: 0.02\n",
            "iteration: 303540 loss: 0.0033 lr: 0.02\n",
            "iteration: 303550 loss: 0.0028 lr: 0.02\n",
            "iteration: 303560 loss: 0.0041 lr: 0.02\n",
            "iteration: 303570 loss: 0.0042 lr: 0.02\n",
            "iteration: 303580 loss: 0.0047 lr: 0.02\n",
            "iteration: 303590 loss: 0.0046 lr: 0.02\n",
            "iteration: 303600 loss: 0.0034 lr: 0.02\n",
            "iteration: 303610 loss: 0.0029 lr: 0.02\n",
            "iteration: 303620 loss: 0.0042 lr: 0.02\n",
            "iteration: 303630 loss: 0.0037 lr: 0.02\n",
            "iteration: 303640 loss: 0.0031 lr: 0.02\n",
            "iteration: 303650 loss: 0.0034 lr: 0.02\n",
            "iteration: 303660 loss: 0.0031 lr: 0.02\n",
            "iteration: 303670 loss: 0.0037 lr: 0.02\n",
            "iteration: 303680 loss: 0.0042 lr: 0.02\n",
            "iteration: 303690 loss: 0.0040 lr: 0.02\n",
            "iteration: 303700 loss: 0.0034 lr: 0.02\n",
            "iteration: 303710 loss: 0.0045 lr: 0.02\n",
            "iteration: 303720 loss: 0.0041 lr: 0.02\n",
            "iteration: 303730 loss: 0.0030 lr: 0.02\n",
            "iteration: 303740 loss: 0.0033 lr: 0.02\n",
            "iteration: 303750 loss: 0.0025 lr: 0.02\n",
            "iteration: 303760 loss: 0.0036 lr: 0.02\n",
            "iteration: 303770 loss: 0.0034 lr: 0.02\n",
            "iteration: 303780 loss: 0.0024 lr: 0.02\n",
            "iteration: 303790 loss: 0.0036 lr: 0.02\n",
            "iteration: 303800 loss: 0.0032 lr: 0.02\n",
            "iteration: 303810 loss: 0.0041 lr: 0.02\n",
            "iteration: 303820 loss: 0.0035 lr: 0.02\n",
            "iteration: 303830 loss: 0.0045 lr: 0.02\n",
            "iteration: 303840 loss: 0.0032 lr: 0.02\n",
            "iteration: 303850 loss: 0.0025 lr: 0.02\n",
            "iteration: 303860 loss: 0.0033 lr: 0.02\n",
            "iteration: 303870 loss: 0.0036 lr: 0.02\n",
            "iteration: 303880 loss: 0.0043 lr: 0.02\n",
            "iteration: 303890 loss: 0.0038 lr: 0.02\n",
            "iteration: 303900 loss: 0.0036 lr: 0.02\n",
            "iteration: 303910 loss: 0.0032 lr: 0.02\n",
            "iteration: 303920 loss: 0.0028 lr: 0.02\n",
            "iteration: 303930 loss: 0.0033 lr: 0.02\n",
            "iteration: 303940 loss: 0.0030 lr: 0.02\n",
            "iteration: 303950 loss: 0.0027 lr: 0.02\n",
            "iteration: 303960 loss: 0.0034 lr: 0.02\n",
            "iteration: 303970 loss: 0.0034 lr: 0.02\n",
            "iteration: 303980 loss: 0.0029 lr: 0.02\n",
            "iteration: 303990 loss: 0.0043 lr: 0.02\n",
            "iteration: 304000 loss: 0.0034 lr: 0.02\n",
            "iteration: 304010 loss: 0.0033 lr: 0.02\n",
            "iteration: 304020 loss: 0.0031 lr: 0.02\n",
            "iteration: 304030 loss: 0.0035 lr: 0.02\n",
            "iteration: 304040 loss: 0.0048 lr: 0.02\n",
            "iteration: 304050 loss: 0.0023 lr: 0.02\n",
            "iteration: 304060 loss: 0.0039 lr: 0.02\n",
            "iteration: 304070 loss: 0.0031 lr: 0.02\n",
            "iteration: 304080 loss: 0.0023 lr: 0.02\n",
            "iteration: 304090 loss: 0.0030 lr: 0.02\n",
            "iteration: 304100 loss: 0.0036 lr: 0.02\n",
            "iteration: 304110 loss: 0.0026 lr: 0.02\n",
            "iteration: 304120 loss: 0.0032 lr: 0.02\n",
            "iteration: 304130 loss: 0.0026 lr: 0.02\n",
            "iteration: 304140 loss: 0.0042 lr: 0.02\n",
            "iteration: 304150 loss: 0.0036 lr: 0.02\n",
            "iteration: 304160 loss: 0.0034 lr: 0.02\n",
            "iteration: 304170 loss: 0.0031 lr: 0.02\n",
            "iteration: 304180 loss: 0.0046 lr: 0.02\n",
            "iteration: 304190 loss: 0.0035 lr: 0.02\n",
            "iteration: 304200 loss: 0.0038 lr: 0.02\n",
            "iteration: 304210 loss: 0.0036 lr: 0.02\n",
            "iteration: 304220 loss: 0.0049 lr: 0.02\n",
            "iteration: 304230 loss: 0.0047 lr: 0.02\n",
            "iteration: 304240 loss: 0.0046 lr: 0.02\n",
            "iteration: 304250 loss: 0.0032 lr: 0.02\n",
            "iteration: 304260 loss: 0.0023 lr: 0.02\n",
            "iteration: 304270 loss: 0.0028 lr: 0.02\n",
            "iteration: 304280 loss: 0.0029 lr: 0.02\n",
            "iteration: 304290 loss: 0.0036 lr: 0.02\n",
            "iteration: 304300 loss: 0.0032 lr: 0.02\n",
            "iteration: 304310 loss: 0.0036 lr: 0.02\n",
            "iteration: 304320 loss: 0.0032 lr: 0.02\n",
            "iteration: 304330 loss: 0.0036 lr: 0.02\n",
            "iteration: 304340 loss: 0.0039 lr: 0.02\n",
            "iteration: 304350 loss: 0.0036 lr: 0.02\n",
            "iteration: 304360 loss: 0.0025 lr: 0.02\n",
            "iteration: 304370 loss: 0.0037 lr: 0.02\n",
            "iteration: 304380 loss: 0.0036 lr: 0.02\n",
            "iteration: 304390 loss: 0.0035 lr: 0.02\n",
            "iteration: 304400 loss: 0.0051 lr: 0.02\n",
            "iteration: 304410 loss: 0.0029 lr: 0.02\n",
            "iteration: 304420 loss: 0.0041 lr: 0.02\n",
            "iteration: 304430 loss: 0.0033 lr: 0.02\n",
            "iteration: 304440 loss: 0.0034 lr: 0.02\n",
            "iteration: 304450 loss: 0.0034 lr: 0.02\n",
            "iteration: 304460 loss: 0.0035 lr: 0.02\n",
            "iteration: 304470 loss: 0.0036 lr: 0.02\n",
            "iteration: 304480 loss: 0.0037 lr: 0.02\n",
            "iteration: 304490 loss: 0.0039 lr: 0.02\n",
            "iteration: 304500 loss: 0.0039 lr: 0.02\n",
            "iteration: 304510 loss: 0.0029 lr: 0.02\n",
            "iteration: 304520 loss: 0.0035 lr: 0.02\n",
            "iteration: 304530 loss: 0.0040 lr: 0.02\n",
            "iteration: 304540 loss: 0.0033 lr: 0.02\n",
            "iteration: 304550 loss: 0.0033 lr: 0.02\n",
            "iteration: 304560 loss: 0.0032 lr: 0.02\n",
            "iteration: 304570 loss: 0.0034 lr: 0.02\n",
            "iteration: 304580 loss: 0.0045 lr: 0.02\n",
            "iteration: 304590 loss: 0.0043 lr: 0.02\n",
            "iteration: 304600 loss: 0.0046 lr: 0.02\n",
            "iteration: 304610 loss: 0.0038 lr: 0.02\n",
            "iteration: 304620 loss: 0.0033 lr: 0.02\n",
            "iteration: 304630 loss: 0.0042 lr: 0.02\n",
            "iteration: 304640 loss: 0.0039 lr: 0.02\n",
            "iteration: 304650 loss: 0.0033 lr: 0.02\n",
            "iteration: 304660 loss: 0.0040 lr: 0.02\n",
            "iteration: 304670 loss: 0.0043 lr: 0.02\n",
            "iteration: 304680 loss: 0.0038 lr: 0.02\n",
            "iteration: 304690 loss: 0.0027 lr: 0.02\n",
            "iteration: 304700 loss: 0.0033 lr: 0.02\n",
            "iteration: 304710 loss: 0.0035 lr: 0.02\n",
            "iteration: 304720 loss: 0.0034 lr: 0.02\n",
            "iteration: 304730 loss: 0.0034 lr: 0.02\n",
            "iteration: 304740 loss: 0.0034 lr: 0.02\n",
            "iteration: 304750 loss: 0.0029 lr: 0.02\n",
            "iteration: 304760 loss: 0.0037 lr: 0.02\n",
            "iteration: 304770 loss: 0.0027 lr: 0.02\n",
            "iteration: 304780 loss: 0.0029 lr: 0.02\n",
            "iteration: 304790 loss: 0.0033 lr: 0.02\n",
            "iteration: 304800 loss: 0.0039 lr: 0.02\n",
            "iteration: 304810 loss: 0.0039 lr: 0.02\n",
            "iteration: 304820 loss: 0.0044 lr: 0.02\n",
            "iteration: 304830 loss: 0.0023 lr: 0.02\n",
            "iteration: 304840 loss: 0.0050 lr: 0.02\n",
            "iteration: 304850 loss: 0.0042 lr: 0.02\n",
            "iteration: 304860 loss: 0.0039 lr: 0.02\n",
            "iteration: 304870 loss: 0.0031 lr: 0.02\n",
            "iteration: 304880 loss: 0.0032 lr: 0.02\n",
            "iteration: 304890 loss: 0.0035 lr: 0.02\n",
            "iteration: 304900 loss: 0.0031 lr: 0.02\n",
            "iteration: 304910 loss: 0.0032 lr: 0.02\n",
            "iteration: 304920 loss: 0.0045 lr: 0.02\n",
            "iteration: 304930 loss: 0.0035 lr: 0.02\n",
            "iteration: 304940 loss: 0.0044 lr: 0.02\n",
            "iteration: 304950 loss: 0.0045 lr: 0.02\n",
            "iteration: 304960 loss: 0.0046 lr: 0.02\n",
            "iteration: 304970 loss: 0.0030 lr: 0.02\n",
            "iteration: 304980 loss: 0.0029 lr: 0.02\n",
            "iteration: 304990 loss: 0.0037 lr: 0.02\n",
            "iteration: 305000 loss: 0.0030 lr: 0.02\n",
            "iteration: 305010 loss: 0.0047 lr: 0.02\n",
            "iteration: 305020 loss: 0.0038 lr: 0.02\n",
            "iteration: 305030 loss: 0.0037 lr: 0.02\n",
            "iteration: 305040 loss: 0.0044 lr: 0.02\n",
            "iteration: 305050 loss: 0.0035 lr: 0.02\n",
            "iteration: 305060 loss: 0.0024 lr: 0.02\n",
            "iteration: 305070 loss: 0.0035 lr: 0.02\n",
            "iteration: 305080 loss: 0.0034 lr: 0.02\n",
            "iteration: 305090 loss: 0.0033 lr: 0.02\n",
            "iteration: 305100 loss: 0.0035 lr: 0.02\n",
            "iteration: 305110 loss: 0.0034 lr: 0.02\n",
            "iteration: 305120 loss: 0.0029 lr: 0.02\n",
            "iteration: 305130 loss: 0.0037 lr: 0.02\n",
            "iteration: 305140 loss: 0.0045 lr: 0.02\n",
            "iteration: 305150 loss: 0.0032 lr: 0.02\n",
            "iteration: 305160 loss: 0.0050 lr: 0.02\n",
            "iteration: 305170 loss: 0.0036 lr: 0.02\n",
            "iteration: 305180 loss: 0.0033 lr: 0.02\n",
            "iteration: 305190 loss: 0.0033 lr: 0.02\n",
            "iteration: 305200 loss: 0.0028 lr: 0.02\n",
            "iteration: 305210 loss: 0.0046 lr: 0.02\n",
            "iteration: 305220 loss: 0.0037 lr: 0.02\n",
            "iteration: 305230 loss: 0.0047 lr: 0.02\n",
            "iteration: 305240 loss: 0.0038 lr: 0.02\n",
            "iteration: 305250 loss: 0.0041 lr: 0.02\n",
            "iteration: 305260 loss: 0.0034 lr: 0.02\n",
            "iteration: 305270 loss: 0.0041 lr: 0.02\n",
            "iteration: 305280 loss: 0.0034 lr: 0.02\n",
            "iteration: 305290 loss: 0.0040 lr: 0.02\n",
            "iteration: 305300 loss: 0.0031 lr: 0.02\n",
            "iteration: 305310 loss: 0.0033 lr: 0.02\n",
            "iteration: 305320 loss: 0.0030 lr: 0.02\n",
            "iteration: 305330 loss: 0.0050 lr: 0.02\n",
            "iteration: 305340 loss: 0.0029 lr: 0.02\n",
            "iteration: 305350 loss: 0.0038 lr: 0.02\n",
            "iteration: 305360 loss: 0.0037 lr: 0.02\n",
            "iteration: 305370 loss: 0.0026 lr: 0.02\n",
            "iteration: 305380 loss: 0.0034 lr: 0.02\n",
            "iteration: 305390 loss: 0.0045 lr: 0.02\n",
            "iteration: 305400 loss: 0.0028 lr: 0.02\n",
            "iteration: 305410 loss: 0.0037 lr: 0.02\n",
            "iteration: 305420 loss: 0.0029 lr: 0.02\n",
            "iteration: 305430 loss: 0.0038 lr: 0.02\n",
            "iteration: 305440 loss: 0.0032 lr: 0.02\n",
            "iteration: 305450 loss: 0.0035 lr: 0.02\n",
            "iteration: 305460 loss: 0.0036 lr: 0.02\n",
            "iteration: 305470 loss: 0.0035 lr: 0.02\n",
            "iteration: 305480 loss: 0.0035 lr: 0.02\n",
            "iteration: 305490 loss: 0.0035 lr: 0.02\n",
            "iteration: 305500 loss: 0.0029 lr: 0.02\n",
            "iteration: 305510 loss: 0.0032 lr: 0.02\n",
            "iteration: 305520 loss: 0.0033 lr: 0.02\n",
            "iteration: 305530 loss: 0.0033 lr: 0.02\n",
            "iteration: 305540 loss: 0.0044 lr: 0.02\n",
            "iteration: 305550 loss: 0.0034 lr: 0.02\n",
            "iteration: 305560 loss: 0.0040 lr: 0.02\n",
            "iteration: 305570 loss: 0.0025 lr: 0.02\n",
            "iteration: 305580 loss: 0.0032 lr: 0.02\n",
            "iteration: 305590 loss: 0.0032 lr: 0.02\n",
            "iteration: 305600 loss: 0.0026 lr: 0.02\n",
            "iteration: 305610 loss: 0.0033 lr: 0.02\n",
            "iteration: 305620 loss: 0.0037 lr: 0.02\n",
            "iteration: 305630 loss: 0.0028 lr: 0.02\n",
            "iteration: 305640 loss: 0.0029 lr: 0.02\n",
            "iteration: 305650 loss: 0.0032 lr: 0.02\n",
            "iteration: 305660 loss: 0.0036 lr: 0.02\n",
            "iteration: 305670 loss: 0.0034 lr: 0.02\n",
            "iteration: 305680 loss: 0.0033 lr: 0.02\n",
            "iteration: 305690 loss: 0.0044 lr: 0.02\n",
            "iteration: 305700 loss: 0.0038 lr: 0.02\n",
            "iteration: 305710 loss: 0.0028 lr: 0.02\n",
            "iteration: 305720 loss: 0.0030 lr: 0.02\n",
            "iteration: 305730 loss: 0.0032 lr: 0.02\n",
            "iteration: 305740 loss: 0.0041 lr: 0.02\n",
            "iteration: 305750 loss: 0.0040 lr: 0.02\n",
            "iteration: 305760 loss: 0.0038 lr: 0.02\n",
            "iteration: 305770 loss: 0.0033 lr: 0.02\n",
            "iteration: 305780 loss: 0.0039 lr: 0.02\n",
            "iteration: 305790 loss: 0.0035 lr: 0.02\n",
            "iteration: 305800 loss: 0.0031 lr: 0.02\n",
            "iteration: 305810 loss: 0.0033 lr: 0.02\n",
            "iteration: 305820 loss: 0.0038 lr: 0.02\n",
            "iteration: 305830 loss: 0.0033 lr: 0.02\n",
            "iteration: 305840 loss: 0.0050 lr: 0.02\n",
            "iteration: 305850 loss: 0.0036 lr: 0.02\n",
            "iteration: 305860 loss: 0.0029 lr: 0.02\n",
            "iteration: 305870 loss: 0.0030 lr: 0.02\n",
            "iteration: 305880 loss: 0.0033 lr: 0.02\n",
            "iteration: 305890 loss: 0.0036 lr: 0.02\n",
            "iteration: 305900 loss: 0.0039 lr: 0.02\n",
            "iteration: 305910 loss: 0.0030 lr: 0.02\n",
            "iteration: 305920 loss: 0.0037 lr: 0.02\n",
            "iteration: 305930 loss: 0.0043 lr: 0.02\n",
            "iteration: 305940 loss: 0.0038 lr: 0.02\n",
            "iteration: 305950 loss: 0.0036 lr: 0.02\n",
            "iteration: 305960 loss: 0.0043 lr: 0.02\n",
            "iteration: 305970 loss: 0.0029 lr: 0.02\n",
            "iteration: 305980 loss: 0.0044 lr: 0.02\n",
            "iteration: 305990 loss: 0.0045 lr: 0.02\n",
            "iteration: 306000 loss: 0.0039 lr: 0.02\n",
            "iteration: 306010 loss: 0.0037 lr: 0.02\n",
            "iteration: 306020 loss: 0.0035 lr: 0.02\n",
            "iteration: 306030 loss: 0.0029 lr: 0.02\n",
            "iteration: 306040 loss: 0.0027 lr: 0.02\n",
            "iteration: 306050 loss: 0.0024 lr: 0.02\n",
            "iteration: 306060 loss: 0.0037 lr: 0.02\n",
            "iteration: 306070 loss: 0.0027 lr: 0.02\n",
            "iteration: 306080 loss: 0.0032 lr: 0.02\n",
            "iteration: 306090 loss: 0.0026 lr: 0.02\n",
            "iteration: 306100 loss: 0.0031 lr: 0.02\n",
            "iteration: 306110 loss: 0.0035 lr: 0.02\n",
            "iteration: 306120 loss: 0.0041 lr: 0.02\n",
            "iteration: 306130 loss: 0.0033 lr: 0.02\n",
            "iteration: 306140 loss: 0.0033 lr: 0.02\n",
            "iteration: 306150 loss: 0.0022 lr: 0.02\n",
            "iteration: 306160 loss: 0.0033 lr: 0.02\n",
            "iteration: 306170 loss: 0.0036 lr: 0.02\n",
            "iteration: 306180 loss: 0.0032 lr: 0.02\n",
            "iteration: 306190 loss: 0.0044 lr: 0.02\n",
            "iteration: 306200 loss: 0.0024 lr: 0.02\n",
            "iteration: 306210 loss: 0.0029 lr: 0.02\n",
            "iteration: 306220 loss: 0.0034 lr: 0.02\n",
            "iteration: 306230 loss: 0.0036 lr: 0.02\n",
            "iteration: 306240 loss: 0.0031 lr: 0.02\n",
            "iteration: 306250 loss: 0.0028 lr: 0.02\n",
            "iteration: 306260 loss: 0.0034 lr: 0.02\n",
            "iteration: 306270 loss: 0.0039 lr: 0.02\n",
            "iteration: 306280 loss: 0.0039 lr: 0.02\n",
            "iteration: 306290 loss: 0.0035 lr: 0.02\n",
            "iteration: 306300 loss: 0.0029 lr: 0.02\n",
            "iteration: 306310 loss: 0.0038 lr: 0.02\n",
            "iteration: 306320 loss: 0.0031 lr: 0.02\n",
            "iteration: 306330 loss: 0.0034 lr: 0.02\n",
            "iteration: 306340 loss: 0.0030 lr: 0.02\n",
            "iteration: 306350 loss: 0.0039 lr: 0.02\n",
            "iteration: 306360 loss: 0.0026 lr: 0.02\n",
            "iteration: 306370 loss: 0.0028 lr: 0.02\n",
            "iteration: 306380 loss: 0.0040 lr: 0.02\n",
            "iteration: 306390 loss: 0.0042 lr: 0.02\n",
            "iteration: 306400 loss: 0.0037 lr: 0.02\n",
            "iteration: 306410 loss: 0.0042 lr: 0.02\n",
            "iteration: 306420 loss: 0.0029 lr: 0.02\n",
            "iteration: 306430 loss: 0.0039 lr: 0.02\n",
            "iteration: 306440 loss: 0.0039 lr: 0.02\n",
            "iteration: 306450 loss: 0.0019 lr: 0.02\n",
            "iteration: 306460 loss: 0.0032 lr: 0.02\n",
            "iteration: 306470 loss: 0.0037 lr: 0.02\n",
            "iteration: 306480 loss: 0.0039 lr: 0.02\n",
            "iteration: 306490 loss: 0.0037 lr: 0.02\n",
            "iteration: 306500 loss: 0.0033 lr: 0.02\n",
            "iteration: 306510 loss: 0.0037 lr: 0.02\n",
            "iteration: 306520 loss: 0.0030 lr: 0.02\n",
            "iteration: 306530 loss: 0.0030 lr: 0.02\n",
            "iteration: 306540 loss: 0.0050 lr: 0.02\n",
            "iteration: 306550 loss: 0.0036 lr: 0.02\n",
            "iteration: 306560 loss: 0.0034 lr: 0.02\n",
            "iteration: 306570 loss: 0.0031 lr: 0.02\n",
            "iteration: 306580 loss: 0.0036 lr: 0.02\n",
            "iteration: 306590 loss: 0.0025 lr: 0.02\n",
            "iteration: 306600 loss: 0.0040 lr: 0.02\n",
            "iteration: 306610 loss: 0.0028 lr: 0.02\n",
            "iteration: 306620 loss: 0.0037 lr: 0.02\n",
            "iteration: 306630 loss: 0.0031 lr: 0.02\n",
            "iteration: 306640 loss: 0.0037 lr: 0.02\n",
            "iteration: 306650 loss: 0.0044 lr: 0.02\n",
            "iteration: 306660 loss: 0.0026 lr: 0.02\n",
            "iteration: 306670 loss: 0.0031 lr: 0.02\n",
            "iteration: 306680 loss: 0.0029 lr: 0.02\n",
            "iteration: 306690 loss: 0.0041 lr: 0.02\n",
            "iteration: 306700 loss: 0.0030 lr: 0.02\n",
            "iteration: 306710 loss: 0.0047 lr: 0.02\n",
            "iteration: 306720 loss: 0.0027 lr: 0.02\n",
            "iteration: 306730 loss: 0.0027 lr: 0.02\n",
            "iteration: 306740 loss: 0.0044 lr: 0.02\n",
            "iteration: 306750 loss: 0.0026 lr: 0.02\n",
            "iteration: 306760 loss: 0.0033 lr: 0.02\n",
            "iteration: 306770 loss: 0.0032 lr: 0.02\n",
            "iteration: 306780 loss: 0.0031 lr: 0.02\n",
            "iteration: 306790 loss: 0.0039 lr: 0.02\n",
            "iteration: 306800 loss: 0.0033 lr: 0.02\n",
            "iteration: 306810 loss: 0.0039 lr: 0.02\n",
            "iteration: 306820 loss: 0.0044 lr: 0.02\n",
            "iteration: 306830 loss: 0.0031 lr: 0.02\n",
            "iteration: 306840 loss: 0.0038 lr: 0.02\n",
            "iteration: 306850 loss: 0.0031 lr: 0.02\n",
            "iteration: 306860 loss: 0.0038 lr: 0.02\n",
            "iteration: 306870 loss: 0.0034 lr: 0.02\n",
            "iteration: 306880 loss: 0.0042 lr: 0.02\n",
            "iteration: 306890 loss: 0.0034 lr: 0.02\n",
            "iteration: 306900 loss: 0.0036 lr: 0.02\n",
            "iteration: 306910 loss: 0.0030 lr: 0.02\n",
            "iteration: 306920 loss: 0.0037 lr: 0.02\n",
            "iteration: 306930 loss: 0.0035 lr: 0.02\n",
            "iteration: 306940 loss: 0.0039 lr: 0.02\n",
            "iteration: 306950 loss: 0.0045 lr: 0.02\n",
            "iteration: 306960 loss: 0.0031 lr: 0.02\n",
            "iteration: 306970 loss: 0.0038 lr: 0.02\n",
            "iteration: 306980 loss: 0.0031 lr: 0.02\n",
            "iteration: 306990 loss: 0.0029 lr: 0.02\n",
            "iteration: 307000 loss: 0.0039 lr: 0.02\n",
            "iteration: 307010 loss: 0.0033 lr: 0.02\n",
            "iteration: 307020 loss: 0.0033 lr: 0.02\n",
            "iteration: 307030 loss: 0.0027 lr: 0.02\n",
            "iteration: 307040 loss: 0.0039 lr: 0.02\n",
            "iteration: 307050 loss: 0.0034 lr: 0.02\n",
            "iteration: 307060 loss: 0.0028 lr: 0.02\n",
            "iteration: 307070 loss: 0.0040 lr: 0.02\n",
            "iteration: 307080 loss: 0.0033 lr: 0.02\n",
            "iteration: 307090 loss: 0.0030 lr: 0.02\n",
            "iteration: 307100 loss: 0.0032 lr: 0.02\n",
            "iteration: 307110 loss: 0.0030 lr: 0.02\n",
            "iteration: 307120 loss: 0.0041 lr: 0.02\n",
            "iteration: 307130 loss: 0.0029 lr: 0.02\n",
            "iteration: 307140 loss: 0.0032 lr: 0.02\n",
            "iteration: 307150 loss: 0.0037 lr: 0.02\n",
            "iteration: 307160 loss: 0.0033 lr: 0.02\n",
            "iteration: 307170 loss: 0.0028 lr: 0.02\n",
            "iteration: 307180 loss: 0.0031 lr: 0.02\n",
            "iteration: 307190 loss: 0.0033 lr: 0.02\n",
            "iteration: 307200 loss: 0.0031 lr: 0.02\n",
            "iteration: 307210 loss: 0.0026 lr: 0.02\n",
            "iteration: 307220 loss: 0.0033 lr: 0.02\n",
            "iteration: 307230 loss: 0.0036 lr: 0.02\n",
            "iteration: 307240 loss: 0.0037 lr: 0.02\n",
            "iteration: 307250 loss: 0.0031 lr: 0.02\n",
            "iteration: 307260 loss: 0.0033 lr: 0.02\n",
            "iteration: 307270 loss: 0.0027 lr: 0.02\n",
            "iteration: 307280 loss: 0.0035 lr: 0.02\n",
            "iteration: 307290 loss: 0.0039 lr: 0.02\n",
            "iteration: 307300 loss: 0.0044 lr: 0.02\n",
            "iteration: 307310 loss: 0.0036 lr: 0.02\n",
            "iteration: 307320 loss: 0.0035 lr: 0.02\n",
            "iteration: 307330 loss: 0.0033 lr: 0.02\n",
            "iteration: 307340 loss: 0.0039 lr: 0.02\n",
            "iteration: 307350 loss: 0.0031 lr: 0.02\n",
            "iteration: 307360 loss: 0.0035 lr: 0.02\n",
            "iteration: 307370 loss: 0.0031 lr: 0.02\n",
            "iteration: 307380 loss: 0.0033 lr: 0.02\n",
            "iteration: 307390 loss: 0.0036 lr: 0.02\n",
            "iteration: 307400 loss: 0.0030 lr: 0.02\n",
            "iteration: 307410 loss: 0.0041 lr: 0.02\n",
            "iteration: 307420 loss: 0.0027 lr: 0.02\n",
            "iteration: 307430 loss: 0.0030 lr: 0.02\n",
            "iteration: 307440 loss: 0.0033 lr: 0.02\n",
            "iteration: 307450 loss: 0.0027 lr: 0.02\n",
            "iteration: 307460 loss: 0.0028 lr: 0.02\n",
            "iteration: 307470 loss: 0.0035 lr: 0.02\n",
            "iteration: 307480 loss: 0.0040 lr: 0.02\n",
            "iteration: 307490 loss: 0.0029 lr: 0.02\n",
            "iteration: 307500 loss: 0.0037 lr: 0.02\n",
            "iteration: 307510 loss: 0.0043 lr: 0.02\n",
            "iteration: 307520 loss: 0.0034 lr: 0.02\n",
            "iteration: 307530 loss: 0.0039 lr: 0.02\n",
            "iteration: 307540 loss: 0.0043 lr: 0.02\n",
            "iteration: 307550 loss: 0.0040 lr: 0.02\n",
            "iteration: 307560 loss: 0.0041 lr: 0.02\n",
            "iteration: 307570 loss: 0.0031 lr: 0.02\n",
            "iteration: 307580 loss: 0.0028 lr: 0.02\n",
            "iteration: 307590 loss: 0.0025 lr: 0.02\n",
            "iteration: 307600 loss: 0.0034 lr: 0.02\n",
            "iteration: 307610 loss: 0.0039 lr: 0.02\n",
            "iteration: 307620 loss: 0.0049 lr: 0.02\n",
            "iteration: 307630 loss: 0.0034 lr: 0.02\n",
            "iteration: 307640 loss: 0.0038 lr: 0.02\n",
            "iteration: 307650 loss: 0.0039 lr: 0.02\n",
            "iteration: 307660 loss: 0.0023 lr: 0.02\n",
            "iteration: 307670 loss: 0.0027 lr: 0.02\n",
            "iteration: 307680 loss: 0.0034 lr: 0.02\n",
            "iteration: 307690 loss: 0.0032 lr: 0.02\n",
            "iteration: 307700 loss: 0.0033 lr: 0.02\n",
            "iteration: 307710 loss: 0.0033 lr: 0.02\n",
            "iteration: 307720 loss: 0.0030 lr: 0.02\n",
            "iteration: 307730 loss: 0.0029 lr: 0.02\n",
            "iteration: 307740 loss: 0.0037 lr: 0.02\n",
            "iteration: 307750 loss: 0.0032 lr: 0.02\n",
            "iteration: 307760 loss: 0.0031 lr: 0.02\n",
            "iteration: 307770 loss: 0.0043 lr: 0.02\n",
            "iteration: 307780 loss: 0.0028 lr: 0.02\n",
            "iteration: 307790 loss: 0.0035 lr: 0.02\n",
            "iteration: 307800 loss: 0.0037 lr: 0.02\n",
            "iteration: 307810 loss: 0.0037 lr: 0.02\n",
            "iteration: 307820 loss: 0.0031 lr: 0.02\n",
            "iteration: 307830 loss: 0.0029 lr: 0.02\n",
            "iteration: 307840 loss: 0.0035 lr: 0.02\n",
            "iteration: 307850 loss: 0.0044 lr: 0.02\n",
            "iteration: 307860 loss: 0.0043 lr: 0.02\n",
            "iteration: 307870 loss: 0.0031 lr: 0.02\n",
            "iteration: 307880 loss: 0.0038 lr: 0.02\n",
            "iteration: 307890 loss: 0.0034 lr: 0.02\n",
            "iteration: 307900 loss: 0.0037 lr: 0.02\n",
            "iteration: 307910 loss: 0.0029 lr: 0.02\n",
            "iteration: 307920 loss: 0.0025 lr: 0.02\n",
            "iteration: 307930 loss: 0.0047 lr: 0.02\n",
            "iteration: 307940 loss: 0.0038 lr: 0.02\n",
            "iteration: 307950 loss: 0.0036 lr: 0.02\n",
            "iteration: 307960 loss: 0.0041 lr: 0.02\n",
            "iteration: 307970 loss: 0.0030 lr: 0.02\n",
            "iteration: 307980 loss: 0.0039 lr: 0.02\n",
            "iteration: 307990 loss: 0.0029 lr: 0.02\n",
            "iteration: 308000 loss: 0.0035 lr: 0.02\n",
            "iteration: 308010 loss: 0.0040 lr: 0.02\n",
            "iteration: 308020 loss: 0.0031 lr: 0.02\n",
            "iteration: 308030 loss: 0.0038 lr: 0.02\n",
            "iteration: 308040 loss: 0.0032 lr: 0.02\n",
            "iteration: 308050 loss: 0.0029 lr: 0.02\n",
            "iteration: 308060 loss: 0.0032 lr: 0.02\n",
            "iteration: 308070 loss: 0.0033 lr: 0.02\n",
            "iteration: 308080 loss: 0.0024 lr: 0.02\n",
            "iteration: 308090 loss: 0.0036 lr: 0.02\n",
            "iteration: 308100 loss: 0.0032 lr: 0.02\n",
            "iteration: 308110 loss: 0.0041 lr: 0.02\n",
            "iteration: 308120 loss: 0.0029 lr: 0.02\n",
            "iteration: 308130 loss: 0.0031 lr: 0.02\n",
            "iteration: 308140 loss: 0.0026 lr: 0.02\n",
            "iteration: 308150 loss: 0.0036 lr: 0.02\n",
            "iteration: 308160 loss: 0.0038 lr: 0.02\n",
            "iteration: 308170 loss: 0.0042 lr: 0.02\n",
            "iteration: 308180 loss: 0.0034 lr: 0.02\n",
            "iteration: 308190 loss: 0.0045 lr: 0.02\n",
            "iteration: 308200 loss: 0.0041 lr: 0.02\n",
            "iteration: 308210 loss: 0.0040 lr: 0.02\n",
            "iteration: 308220 loss: 0.0038 lr: 0.02\n",
            "iteration: 308230 loss: 0.0032 lr: 0.02\n",
            "iteration: 308240 loss: 0.0046 lr: 0.02\n",
            "iteration: 308250 loss: 0.0045 lr: 0.02\n",
            "iteration: 308260 loss: 0.0036 lr: 0.02\n",
            "iteration: 308270 loss: 0.0025 lr: 0.02\n",
            "iteration: 308280 loss: 0.0036 lr: 0.02\n",
            "iteration: 308290 loss: 0.0030 lr: 0.02\n",
            "iteration: 308300 loss: 0.0035 lr: 0.02\n",
            "iteration: 308310 loss: 0.0039 lr: 0.02\n",
            "iteration: 308320 loss: 0.0035 lr: 0.02\n",
            "iteration: 308330 loss: 0.0042 lr: 0.02\n",
            "iteration: 308340 loss: 0.0025 lr: 0.02\n",
            "iteration: 308350 loss: 0.0050 lr: 0.02\n",
            "iteration: 308360 loss: 0.0035 lr: 0.02\n",
            "iteration: 308370 loss: 0.0041 lr: 0.02\n",
            "iteration: 308380 loss: 0.0037 lr: 0.02\n",
            "iteration: 308390 loss: 0.0043 lr: 0.02\n",
            "iteration: 308400 loss: 0.0037 lr: 0.02\n",
            "iteration: 308410 loss: 0.0037 lr: 0.02\n",
            "iteration: 308420 loss: 0.0037 lr: 0.02\n",
            "iteration: 308430 loss: 0.0035 lr: 0.02\n",
            "iteration: 308440 loss: 0.0029 lr: 0.02\n",
            "iteration: 308450 loss: 0.0028 lr: 0.02\n",
            "iteration: 308460 loss: 0.0030 lr: 0.02\n",
            "iteration: 308470 loss: 0.0038 lr: 0.02\n",
            "iteration: 308480 loss: 0.0034 lr: 0.02\n",
            "iteration: 308490 loss: 0.0026 lr: 0.02\n",
            "iteration: 308500 loss: 0.0040 lr: 0.02\n",
            "iteration: 308510 loss: 0.0033 lr: 0.02\n",
            "iteration: 308520 loss: 0.0024 lr: 0.02\n",
            "iteration: 308530 loss: 0.0033 lr: 0.02\n",
            "iteration: 308540 loss: 0.0031 lr: 0.02\n",
            "iteration: 308550 loss: 0.0022 lr: 0.02\n",
            "iteration: 308560 loss: 0.0036 lr: 0.02\n",
            "iteration: 308570 loss: 0.0031 lr: 0.02\n",
            "iteration: 308580 loss: 0.0034 lr: 0.02\n",
            "iteration: 308590 loss: 0.0040 lr: 0.02\n",
            "iteration: 308600 loss: 0.0037 lr: 0.02\n",
            "iteration: 308610 loss: 0.0033 lr: 0.02\n",
            "iteration: 308620 loss: 0.0041 lr: 0.02\n",
            "iteration: 308630 loss: 0.0030 lr: 0.02\n",
            "iteration: 308640 loss: 0.0038 lr: 0.02\n",
            "iteration: 308650 loss: 0.0039 lr: 0.02\n",
            "iteration: 308660 loss: 0.0027 lr: 0.02\n",
            "iteration: 308670 loss: 0.0043 lr: 0.02\n",
            "iteration: 308680 loss: 0.0039 lr: 0.02\n",
            "iteration: 308690 loss: 0.0035 lr: 0.02\n",
            "iteration: 308700 loss: 0.0040 lr: 0.02\n",
            "iteration: 308710 loss: 0.0046 lr: 0.02\n",
            "iteration: 308720 loss: 0.0031 lr: 0.02\n",
            "iteration: 308730 loss: 0.0030 lr: 0.02\n",
            "iteration: 308740 loss: 0.0031 lr: 0.02\n",
            "iteration: 308750 loss: 0.0040 lr: 0.02\n",
            "iteration: 308760 loss: 0.0032 lr: 0.02\n",
            "iteration: 308770 loss: 0.0025 lr: 0.02\n",
            "iteration: 308780 loss: 0.0026 lr: 0.02\n",
            "iteration: 308790 loss: 0.0033 lr: 0.02\n",
            "iteration: 308800 loss: 0.0035 lr: 0.02\n",
            "iteration: 308810 loss: 0.0027 lr: 0.02\n",
            "iteration: 308820 loss: 0.0044 lr: 0.02\n",
            "iteration: 308830 loss: 0.0034 lr: 0.02\n",
            "iteration: 308840 loss: 0.0035 lr: 0.02\n",
            "iteration: 308850 loss: 0.0034 lr: 0.02\n",
            "iteration: 308860 loss: 0.0041 lr: 0.02\n",
            "iteration: 308870 loss: 0.0029 lr: 0.02\n",
            "iteration: 308880 loss: 0.0026 lr: 0.02\n",
            "iteration: 308890 loss: 0.0031 lr: 0.02\n",
            "iteration: 308900 loss: 0.0034 lr: 0.02\n",
            "iteration: 308910 loss: 0.0035 lr: 0.02\n",
            "iteration: 308920 loss: 0.0036 lr: 0.02\n",
            "iteration: 308930 loss: 0.0034 lr: 0.02\n",
            "iteration: 308940 loss: 0.0039 lr: 0.02\n",
            "iteration: 308950 loss: 0.0039 lr: 0.02\n",
            "iteration: 308960 loss: 0.0031 lr: 0.02\n",
            "iteration: 308970 loss: 0.0038 lr: 0.02\n",
            "iteration: 308980 loss: 0.0031 lr: 0.02\n",
            "iteration: 308990 loss: 0.0032 lr: 0.02\n",
            "iteration: 309000 loss: 0.0034 lr: 0.02\n",
            "iteration: 309010 loss: 0.0027 lr: 0.02\n",
            "iteration: 309020 loss: 0.0033 lr: 0.02\n",
            "iteration: 309030 loss: 0.0029 lr: 0.02\n",
            "iteration: 309040 loss: 0.0028 lr: 0.02\n",
            "iteration: 309050 loss: 0.0030 lr: 0.02\n",
            "iteration: 309060 loss: 0.0040 lr: 0.02\n",
            "iteration: 309070 loss: 0.0034 lr: 0.02\n",
            "iteration: 309080 loss: 0.0036 lr: 0.02\n",
            "iteration: 309090 loss: 0.0028 lr: 0.02\n",
            "iteration: 309100 loss: 0.0031 lr: 0.02\n",
            "iteration: 309110 loss: 0.0034 lr: 0.02\n",
            "iteration: 309120 loss: 0.0029 lr: 0.02\n",
            "iteration: 309130 loss: 0.0033 lr: 0.02\n",
            "iteration: 309140 loss: 0.0040 lr: 0.02\n",
            "iteration: 309150 loss: 0.0035 lr: 0.02\n",
            "iteration: 309160 loss: 0.0039 lr: 0.02\n",
            "iteration: 309170 loss: 0.0038 lr: 0.02\n",
            "iteration: 309180 loss: 0.0039 lr: 0.02\n",
            "iteration: 309190 loss: 0.0026 lr: 0.02\n",
            "iteration: 309200 loss: 0.0033 lr: 0.02\n",
            "iteration: 309210 loss: 0.0037 lr: 0.02\n",
            "iteration: 309220 loss: 0.0039 lr: 0.02\n",
            "iteration: 309230 loss: 0.0035 lr: 0.02\n",
            "iteration: 309240 loss: 0.0038 lr: 0.02\n",
            "iteration: 309250 loss: 0.0046 lr: 0.02\n",
            "iteration: 309260 loss: 0.0038 lr: 0.02\n",
            "iteration: 309270 loss: 0.0030 lr: 0.02\n",
            "iteration: 309280 loss: 0.0044 lr: 0.02\n",
            "iteration: 309290 loss: 0.0032 lr: 0.02\n",
            "iteration: 309300 loss: 0.0039 lr: 0.02\n",
            "iteration: 309310 loss: 0.0022 lr: 0.02\n",
            "iteration: 309320 loss: 0.0025 lr: 0.02\n",
            "iteration: 309330 loss: 0.0036 lr: 0.02\n",
            "iteration: 309340 loss: 0.0029 lr: 0.02\n",
            "iteration: 309350 loss: 0.0032 lr: 0.02\n",
            "iteration: 309360 loss: 0.0038 lr: 0.02\n",
            "iteration: 309370 loss: 0.0030 lr: 0.02\n",
            "iteration: 309380 loss: 0.0044 lr: 0.02\n",
            "iteration: 309390 loss: 0.0030 lr: 0.02\n",
            "iteration: 309400 loss: 0.0041 lr: 0.02\n",
            "iteration: 309410 loss: 0.0032 lr: 0.02\n",
            "iteration: 309420 loss: 0.0034 lr: 0.02\n",
            "iteration: 309430 loss: 0.0029 lr: 0.02\n",
            "iteration: 309440 loss: 0.0022 lr: 0.02\n",
            "iteration: 309450 loss: 0.0038 lr: 0.02\n",
            "iteration: 309460 loss: 0.0027 lr: 0.02\n",
            "iteration: 309470 loss: 0.0030 lr: 0.02\n",
            "iteration: 309480 loss: 0.0032 lr: 0.02\n",
            "iteration: 309490 loss: 0.0035 lr: 0.02\n",
            "iteration: 309500 loss: 0.0041 lr: 0.02\n",
            "iteration: 309510 loss: 0.0030 lr: 0.02\n",
            "iteration: 309520 loss: 0.0040 lr: 0.02\n",
            "iteration: 309530 loss: 0.0035 lr: 0.02\n",
            "iteration: 309540 loss: 0.0040 lr: 0.02\n",
            "iteration: 309550 loss: 0.0040 lr: 0.02\n",
            "iteration: 309560 loss: 0.0041 lr: 0.02\n",
            "iteration: 309570 loss: 0.0029 lr: 0.02\n",
            "iteration: 309580 loss: 0.0037 lr: 0.02\n",
            "iteration: 309590 loss: 0.0035 lr: 0.02\n",
            "iteration: 309600 loss: 0.0033 lr: 0.02\n",
            "iteration: 309610 loss: 0.0029 lr: 0.02\n",
            "iteration: 309620 loss: 0.0032 lr: 0.02\n",
            "iteration: 309630 loss: 0.0037 lr: 0.02\n",
            "iteration: 309640 loss: 0.0032 lr: 0.02\n",
            "iteration: 309650 loss: 0.0030 lr: 0.02\n",
            "iteration: 309660 loss: 0.0033 lr: 0.02\n",
            "iteration: 309670 loss: 0.0028 lr: 0.02\n",
            "iteration: 309680 loss: 0.0050 lr: 0.02\n",
            "iteration: 309690 loss: 0.0035 lr: 0.02\n",
            "iteration: 309700 loss: 0.0029 lr: 0.02\n",
            "iteration: 309710 loss: 0.0043 lr: 0.02\n",
            "iteration: 309720 loss: 0.0040 lr: 0.02\n",
            "iteration: 309730 loss: 0.0028 lr: 0.02\n",
            "iteration: 309740 loss: 0.0021 lr: 0.02\n",
            "iteration: 309750 loss: 0.0036 lr: 0.02\n",
            "iteration: 309760 loss: 0.0036 lr: 0.02\n",
            "iteration: 309770 loss: 0.0032 lr: 0.02\n",
            "iteration: 309780 loss: 0.0048 lr: 0.02\n",
            "iteration: 309790 loss: 0.0035 lr: 0.02\n",
            "iteration: 309800 loss: 0.0039 lr: 0.02\n",
            "iteration: 309810 loss: 0.0037 lr: 0.02\n",
            "iteration: 309820 loss: 0.0031 lr: 0.02\n",
            "iteration: 309830 loss: 0.0047 lr: 0.02\n",
            "iteration: 309840 loss: 0.0038 lr: 0.02\n",
            "iteration: 309850 loss: 0.0034 lr: 0.02\n",
            "iteration: 309860 loss: 0.0039 lr: 0.02\n",
            "iteration: 309870 loss: 0.0036 lr: 0.02\n",
            "iteration: 309880 loss: 0.0039 lr: 0.02\n",
            "iteration: 309890 loss: 0.0037 lr: 0.02\n",
            "iteration: 309900 loss: 0.0039 lr: 0.02\n",
            "iteration: 309910 loss: 0.0042 lr: 0.02\n",
            "iteration: 309920 loss: 0.0038 lr: 0.02\n",
            "iteration: 309930 loss: 0.0035 lr: 0.02\n",
            "iteration: 309940 loss: 0.0040 lr: 0.02\n",
            "iteration: 309950 loss: 0.0030 lr: 0.02\n",
            "iteration: 309960 loss: 0.0037 lr: 0.02\n",
            "iteration: 309970 loss: 0.0040 lr: 0.02\n",
            "iteration: 309980 loss: 0.0033 lr: 0.02\n",
            "iteration: 309990 loss: 0.0030 lr: 0.02\n",
            "iteration: 310000 loss: 0.0035 lr: 0.02\n",
            "iteration: 310010 loss: 0.0027 lr: 0.02\n",
            "iteration: 310020 loss: 0.0032 lr: 0.02\n",
            "iteration: 310030 loss: 0.0038 lr: 0.02\n",
            "iteration: 310040 loss: 0.0044 lr: 0.02\n",
            "iteration: 310050 loss: 0.0034 lr: 0.02\n",
            "iteration: 310060 loss: 0.0034 lr: 0.02\n",
            "iteration: 310070 loss: 0.0040 lr: 0.02\n",
            "iteration: 310080 loss: 0.0036 lr: 0.02\n",
            "iteration: 310090 loss: 0.0030 lr: 0.02\n",
            "iteration: 310100 loss: 0.0033 lr: 0.02\n",
            "iteration: 310110 loss: 0.0050 lr: 0.02\n",
            "iteration: 310120 loss: 0.0030 lr: 0.02\n",
            "iteration: 310130 loss: 0.0029 lr: 0.02\n",
            "iteration: 310140 loss: 0.0033 lr: 0.02\n",
            "iteration: 310150 loss: 0.0036 lr: 0.02\n",
            "iteration: 310160 loss: 0.0036 lr: 0.02\n",
            "iteration: 310170 loss: 0.0042 lr: 0.02\n",
            "iteration: 310180 loss: 0.0031 lr: 0.02\n",
            "iteration: 310190 loss: 0.0050 lr: 0.02\n",
            "iteration: 310200 loss: 0.0034 lr: 0.02\n",
            "iteration: 310210 loss: 0.0033 lr: 0.02\n",
            "iteration: 310220 loss: 0.0033 lr: 0.02\n",
            "iteration: 310230 loss: 0.0044 lr: 0.02\n",
            "iteration: 310240 loss: 0.0027 lr: 0.02\n",
            "iteration: 310250 loss: 0.0029 lr: 0.02\n",
            "iteration: 310260 loss: 0.0039 lr: 0.02\n",
            "iteration: 310270 loss: 0.0021 lr: 0.02\n",
            "iteration: 310280 loss: 0.0053 lr: 0.02\n",
            "iteration: 310290 loss: 0.0034 lr: 0.02\n",
            "iteration: 310300 loss: 0.0037 lr: 0.02\n",
            "iteration: 310310 loss: 0.0033 lr: 0.02\n",
            "iteration: 310320 loss: 0.0041 lr: 0.02\n",
            "iteration: 310330 loss: 0.0034 lr: 0.02\n",
            "iteration: 310340 loss: 0.0029 lr: 0.02\n",
            "iteration: 310350 loss: 0.0031 lr: 0.02\n",
            "iteration: 310360 loss: 0.0038 lr: 0.02\n",
            "iteration: 310370 loss: 0.0026 lr: 0.02\n",
            "iteration: 310380 loss: 0.0032 lr: 0.02\n",
            "iteration: 310390 loss: 0.0037 lr: 0.02\n",
            "iteration: 310400 loss: 0.0040 lr: 0.02\n",
            "iteration: 310410 loss: 0.0041 lr: 0.02\n",
            "iteration: 310420 loss: 0.0042 lr: 0.02\n",
            "iteration: 310430 loss: 0.0037 lr: 0.02\n",
            "iteration: 310440 loss: 0.0035 lr: 0.02\n",
            "iteration: 310450 loss: 0.0046 lr: 0.02\n",
            "iteration: 310460 loss: 0.0033 lr: 0.02\n",
            "iteration: 310470 loss: 0.0039 lr: 0.02\n",
            "iteration: 310480 loss: 0.0040 lr: 0.02\n",
            "iteration: 310490 loss: 0.0034 lr: 0.02\n",
            "iteration: 310500 loss: 0.0030 lr: 0.02\n",
            "iteration: 310510 loss: 0.0030 lr: 0.02\n",
            "iteration: 310520 loss: 0.0045 lr: 0.02\n",
            "iteration: 310530 loss: 0.0029 lr: 0.02\n",
            "iteration: 310540 loss: 0.0040 lr: 0.02\n",
            "iteration: 310550 loss: 0.0028 lr: 0.02\n",
            "iteration: 310560 loss: 0.0042 lr: 0.02\n",
            "iteration: 310570 loss: 0.0040 lr: 0.02\n",
            "iteration: 310580 loss: 0.0034 lr: 0.02\n",
            "iteration: 310590 loss: 0.0031 lr: 0.02\n",
            "iteration: 310600 loss: 0.0036 lr: 0.02\n",
            "iteration: 310610 loss: 0.0030 lr: 0.02\n",
            "iteration: 310620 loss: 0.0034 lr: 0.02\n",
            "iteration: 310630 loss: 0.0030 lr: 0.02\n",
            "iteration: 310640 loss: 0.0042 lr: 0.02\n",
            "iteration: 310650 loss: 0.0033 lr: 0.02\n",
            "iteration: 310660 loss: 0.0032 lr: 0.02\n",
            "iteration: 310670 loss: 0.0034 lr: 0.02\n",
            "iteration: 310680 loss: 0.0032 lr: 0.02\n",
            "iteration: 310690 loss: 0.0034 lr: 0.02\n",
            "iteration: 310700 loss: 0.0031 lr: 0.02\n",
            "iteration: 310710 loss: 0.0043 lr: 0.02\n",
            "iteration: 310720 loss: 0.0037 lr: 0.02\n",
            "iteration: 310730 loss: 0.0037 lr: 0.02\n",
            "iteration: 310740 loss: 0.0035 lr: 0.02\n",
            "iteration: 310750 loss: 0.0033 lr: 0.02\n",
            "iteration: 310760 loss: 0.0035 lr: 0.02\n",
            "iteration: 310770 loss: 0.0029 lr: 0.02\n",
            "iteration: 310780 loss: 0.0027 lr: 0.02\n",
            "iteration: 310790 loss: 0.0040 lr: 0.02\n",
            "iteration: 310800 loss: 0.0031 lr: 0.02\n",
            "iteration: 310810 loss: 0.0035 lr: 0.02\n",
            "iteration: 310820 loss: 0.0032 lr: 0.02\n",
            "iteration: 310830 loss: 0.0029 lr: 0.02\n",
            "iteration: 310840 loss: 0.0032 lr: 0.02\n",
            "iteration: 310850 loss: 0.0039 lr: 0.02\n",
            "iteration: 310860 loss: 0.0030 lr: 0.02\n",
            "iteration: 310870 loss: 0.0037 lr: 0.02\n",
            "iteration: 310880 loss: 0.0039 lr: 0.02\n",
            "iteration: 310890 loss: 0.0034 lr: 0.02\n",
            "iteration: 310900 loss: 0.0035 lr: 0.02\n",
            "iteration: 310910 loss: 0.0034 lr: 0.02\n",
            "iteration: 310920 loss: 0.0030 lr: 0.02\n",
            "iteration: 310930 loss: 0.0036 lr: 0.02\n",
            "iteration: 310940 loss: 0.0040 lr: 0.02\n",
            "iteration: 310950 loss: 0.0034 lr: 0.02\n",
            "iteration: 310960 loss: 0.0035 lr: 0.02\n",
            "iteration: 310970 loss: 0.0031 lr: 0.02\n",
            "iteration: 310980 loss: 0.0032 lr: 0.02\n",
            "iteration: 310990 loss: 0.0030 lr: 0.02\n",
            "iteration: 311000 loss: 0.0029 lr: 0.02\n",
            "iteration: 311010 loss: 0.0036 lr: 0.02\n",
            "iteration: 311020 loss: 0.0031 lr: 0.02\n",
            "iteration: 311030 loss: 0.0026 lr: 0.02\n",
            "iteration: 311040 loss: 0.0028 lr: 0.02\n",
            "iteration: 311050 loss: 0.0031 lr: 0.02\n",
            "iteration: 311060 loss: 0.0032 lr: 0.02\n",
            "iteration: 311070 loss: 0.0026 lr: 0.02\n",
            "iteration: 311080 loss: 0.0032 lr: 0.02\n",
            "iteration: 311090 loss: 0.0032 lr: 0.02\n",
            "iteration: 311100 loss: 0.0034 lr: 0.02\n",
            "iteration: 311110 loss: 0.0048 lr: 0.02\n",
            "iteration: 311120 loss: 0.0034 lr: 0.02\n",
            "iteration: 311130 loss: 0.0038 lr: 0.02\n",
            "iteration: 311140 loss: 0.0035 lr: 0.02\n",
            "iteration: 311150 loss: 0.0025 lr: 0.02\n",
            "iteration: 311160 loss: 0.0039 lr: 0.02\n",
            "iteration: 311170 loss: 0.0037 lr: 0.02\n",
            "iteration: 311180 loss: 0.0031 lr: 0.02\n",
            "iteration: 311190 loss: 0.0034 lr: 0.02\n",
            "iteration: 311200 loss: 0.0039 lr: 0.02\n",
            "iteration: 311210 loss: 0.0029 lr: 0.02\n",
            "iteration: 311220 loss: 0.0036 lr: 0.02\n",
            "iteration: 311230 loss: 0.0032 lr: 0.02\n",
            "iteration: 311240 loss: 0.0037 lr: 0.02\n",
            "iteration: 311250 loss: 0.0031 lr: 0.02\n",
            "iteration: 311260 loss: 0.0029 lr: 0.02\n",
            "iteration: 311270 loss: 0.0035 lr: 0.02\n",
            "iteration: 311280 loss: 0.0033 lr: 0.02\n",
            "iteration: 311290 loss: 0.0024 lr: 0.02\n",
            "iteration: 311300 loss: 0.0023 lr: 0.02\n",
            "iteration: 311310 loss: 0.0028 lr: 0.02\n",
            "iteration: 311320 loss: 0.0032 lr: 0.02\n",
            "iteration: 311330 loss: 0.0037 lr: 0.02\n",
            "iteration: 311340 loss: 0.0035 lr: 0.02\n",
            "iteration: 311350 loss: 0.0031 lr: 0.02\n",
            "iteration: 311360 loss: 0.0032 lr: 0.02\n",
            "iteration: 311370 loss: 0.0029 lr: 0.02\n",
            "iteration: 311380 loss: 0.0030 lr: 0.02\n",
            "iteration: 311390 loss: 0.0041 lr: 0.02\n",
            "iteration: 311400 loss: 0.0044 lr: 0.02\n",
            "iteration: 311410 loss: 0.0037 lr: 0.02\n",
            "iteration: 311420 loss: 0.0040 lr: 0.02\n",
            "iteration: 311430 loss: 0.0039 lr: 0.02\n",
            "iteration: 311440 loss: 0.0037 lr: 0.02\n",
            "iteration: 311450 loss: 0.0040 lr: 0.02\n",
            "iteration: 311460 loss: 0.0032 lr: 0.02\n",
            "iteration: 311470 loss: 0.0028 lr: 0.02\n",
            "iteration: 311480 loss: 0.0040 lr: 0.02\n",
            "iteration: 311490 loss: 0.0035 lr: 0.02\n",
            "iteration: 311500 loss: 0.0029 lr: 0.02\n",
            "iteration: 311510 loss: 0.0040 lr: 0.02\n",
            "iteration: 311520 loss: 0.0030 lr: 0.02\n",
            "iteration: 311530 loss: 0.0033 lr: 0.02\n",
            "iteration: 311540 loss: 0.0036 lr: 0.02\n",
            "iteration: 311550 loss: 0.0042 lr: 0.02\n",
            "iteration: 311560 loss: 0.0037 lr: 0.02\n",
            "iteration: 311570 loss: 0.0053 lr: 0.02\n",
            "iteration: 311580 loss: 0.0035 lr: 0.02\n",
            "iteration: 311590 loss: 0.0042 lr: 0.02\n",
            "iteration: 311600 loss: 0.0035 lr: 0.02\n",
            "iteration: 311610 loss: 0.0034 lr: 0.02\n",
            "iteration: 311620 loss: 0.0040 lr: 0.02\n",
            "iteration: 311630 loss: 0.0043 lr: 0.02\n",
            "iteration: 311640 loss: 0.0041 lr: 0.02\n",
            "iteration: 311650 loss: 0.0037 lr: 0.02\n",
            "iteration: 311660 loss: 0.0029 lr: 0.02\n",
            "iteration: 311670 loss: 0.0028 lr: 0.02\n",
            "iteration: 311680 loss: 0.0033 lr: 0.02\n",
            "iteration: 311690 loss: 0.0030 lr: 0.02\n",
            "iteration: 311700 loss: 0.0030 lr: 0.02\n",
            "iteration: 311710 loss: 0.0033 lr: 0.02\n",
            "iteration: 311720 loss: 0.0028 lr: 0.02\n",
            "iteration: 311730 loss: 0.0034 lr: 0.02\n",
            "iteration: 311740 loss: 0.0028 lr: 0.02\n",
            "iteration: 311750 loss: 0.0036 lr: 0.02\n",
            "iteration: 311760 loss: 0.0035 lr: 0.02\n",
            "iteration: 311770 loss: 0.0029 lr: 0.02\n",
            "iteration: 311780 loss: 0.0043 lr: 0.02\n",
            "iteration: 311790 loss: 0.0039 lr: 0.02\n",
            "iteration: 311800 loss: 0.0038 lr: 0.02\n",
            "iteration: 311810 loss: 0.0032 lr: 0.02\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwBXkB93kFCX"
      },
      "source": [
        "### Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 247
        },
        "id": "F7zABfgecmIz",
        "outputId": "0cd31c6b-4020-40d8-a93d-9974ccec36f7"
      },
      "source": [
        "%matplotlib notebook\n",
        "deeplabcut.evaluate_network(path_config_file,plotting=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running  DLC_resnet50_final_trackerOct20shuffle1_312000  with # of trainingiterations: 312000\n",
            "Analyzing data...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "345it [06:34,  1.14s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Done and results stored for snapshot:  snapshot-312000\n",
            "Results for 312000  training iterations: 95 1 train error: 3.44 pixels. Test error: 5.48  pixels.\n",
            "With pcutoff of 0.6  train error: 2.75 pixels. Test error: 4.74 pixels\n",
            "Thereby, the errors are given by the average distances between the labels by DLC and the scorer.\n",
            "Plotting...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "application/javascript": [
              "/* Put everything inside the global mpl namespace */\n",
              "window.mpl = {};\n",
              "\n",
              "\n",
              "mpl.get_websocket_type = function() {\n",
              "    if (typeof(WebSocket) !== 'undefined') {\n",
              "        return WebSocket;\n",
              "    } else if (typeof(MozWebSocket) !== 'undefined') {\n",
              "        return MozWebSocket;\n",
              "    } else {\n",
              "        alert('Your browser does not have WebSocket support. ' +\n",
              "              'Please try Chrome, Safari or Firefox ≥ 6. ' +\n",
              "              'Firefox 4 and 5 are also supported but you ' +\n",
              "              'have to enable WebSockets in about:config.');\n",
              "    };\n",
              "}\n",
              "\n",
              "mpl.figure = function(figure_id, websocket, ondownload, parent_element) {\n",
              "    this.id = figure_id;\n",
              "\n",
              "    this.ws = websocket;\n",
              "\n",
              "    this.supports_binary = (this.ws.binaryType != undefined);\n",
              "\n",
              "    if (!this.supports_binary) {\n",
              "        var warnings = document.getElementById(\"mpl-warnings\");\n",
              "        if (warnings) {\n",
              "            warnings.style.display = 'block';\n",
              "            warnings.textContent = (\n",
              "                \"This browser does not support binary websocket messages. \" +\n",
              "                    \"Performance may be slow.\");\n",
              "        }\n",
              "    }\n",
              "\n",
              "    this.imageObj = new Image();\n",
              "\n",
              "    this.context = undefined;\n",
              "    this.message = undefined;\n",
              "    this.canvas = undefined;\n",
              "    this.rubberband_canvas = undefined;\n",
              "    this.rubberband_context = undefined;\n",
              "    this.format_dropdown = undefined;\n",
              "\n",
              "    this.image_mode = 'full';\n",
              "\n",
              "    this.root = $('<div/>');\n",
              "    this._root_extra_style(this.root)\n",
              "    this.root.attr('style', 'display: inline-block');\n",
              "\n",
              "    $(parent_element).append(this.root);\n",
              "\n",
              "    this._init_header(this);\n",
              "    this._init_canvas(this);\n",
              "    this._init_toolbar(this);\n",
              "\n",
              "    var fig = this;\n",
              "\n",
              "    this.waiting = false;\n",
              "\n",
              "    this.ws.onopen =  function () {\n",
              "            fig.send_message(\"supports_binary\", {value: fig.supports_binary});\n",
              "            fig.send_message(\"send_image_mode\", {});\n",
              "            if (mpl.ratio != 1) {\n",
              "                fig.send_message(\"set_dpi_ratio\", {'dpi_ratio': mpl.ratio});\n",
              "            }\n",
              "            fig.send_message(\"refresh\", {});\n",
              "        }\n",
              "\n",
              "    this.imageObj.onload = function() {\n",
              "            if (fig.image_mode == 'full') {\n",
              "                // Full images could contain transparency (where diff images\n",
              "                // almost always do), so we need to clear the canvas so that\n",
              "                // there is no ghosting.\n",
              "                fig.context.clearRect(0, 0, fig.canvas.width, fig.canvas.height);\n",
              "            }\n",
              "            fig.context.drawImage(fig.imageObj, 0, 0);\n",
              "        };\n",
              "\n",
              "    this.imageObj.onunload = function() {\n",
              "        fig.ws.close();\n",
              "    }\n",
              "\n",
              "    this.ws.onmessage = this._make_on_message_function(this);\n",
              "\n",
              "    this.ondownload = ondownload;\n",
              "}\n",
              "\n",
              "mpl.figure.prototype._init_header = function() {\n",
              "    var titlebar = $(\n",
              "        '<div class=\"ui-dialog-titlebar ui-widget-header ui-corner-all ' +\n",
              "        'ui-helper-clearfix\"/>');\n",
              "    var titletext = $(\n",
              "        '<div class=\"ui-dialog-title\" style=\"width: 100%; ' +\n",
              "        'text-align: center; padding: 3px;\"/>');\n",
              "    titlebar.append(titletext)\n",
              "    this.root.append(titlebar);\n",
              "    this.header = titletext[0];\n",
              "}\n",
              "\n",
              "\n",
              "\n",
              "mpl.figure.prototype._canvas_extra_style = function(canvas_div) {\n",
              "\n",
              "}\n",
              "\n",
              "\n",
              "mpl.figure.prototype._root_extra_style = function(canvas_div) {\n",
              "\n",
              "}\n",
              "\n",
              "mpl.figure.prototype._init_canvas = function() {\n",
              "    var fig = this;\n",
              "\n",
              "    var canvas_div = $('<div/>');\n",
              "\n",
              "    canvas_div.attr('style', 'position: relative; clear: both; outline: 0');\n",
              "\n",
              "    function canvas_keyboard_event(event) {\n",
              "        return fig.key_event(event, event['data']);\n",
              "    }\n",
              "\n",
              "    canvas_div.keydown('key_press', canvas_keyboard_event);\n",
              "    canvas_div.keyup('key_release', canvas_keyboard_event);\n",
              "    this.canvas_div = canvas_div\n",
              "    this._canvas_extra_style(canvas_div)\n",
              "    this.root.append(canvas_div);\n",
              "\n",
              "    var canvas = $('<canvas/>');\n",
              "    canvas.addClass('mpl-canvas');\n",
              "    canvas.attr('style', \"left: 0; top: 0; z-index: 0; outline: 0\")\n",
              "\n",
              "    this.canvas = canvas[0];\n",
              "    this.context = canvas[0].getContext(\"2d\");\n",
              "\n",
              "    var backingStore = this.context.backingStorePixelRatio ||\n",
              "\tthis.context.webkitBackingStorePixelRatio ||\n",
              "\tthis.context.mozBackingStorePixelRatio ||\n",
              "\tthis.context.msBackingStorePixelRatio ||\n",
              "\tthis.context.oBackingStorePixelRatio ||\n",
              "\tthis.context.backingStorePixelRatio || 1;\n",
              "\n",
              "    mpl.ratio = (window.devicePixelRatio || 1) / backingStore;\n",
              "\n",
              "    var rubberband = $('<canvas/>');\n",
              "    rubberband.attr('style', \"position: absolute; left: 0; top: 0; z-index: 1;\")\n",
              "\n",
              "    var pass_mouse_events = true;\n",
              "\n",
              "    canvas_div.resizable({\n",
              "        start: function(event, ui) {\n",
              "            pass_mouse_events = false;\n",
              "        },\n",
              "        resize: function(event, ui) {\n",
              "            fig.request_resize(ui.size.width, ui.size.height);\n",
              "        },\n",
              "        stop: function(event, ui) {\n",
              "            pass_mouse_events = true;\n",
              "            fig.request_resize(ui.size.width, ui.size.height);\n",
              "        },\n",
              "    });\n",
              "\n",
              "    function mouse_event_fn(event) {\n",
              "        if (pass_mouse_events)\n",
              "            return fig.mouse_event(event, event['data']);\n",
              "    }\n",
              "\n",
              "    rubberband.mousedown('button_press', mouse_event_fn);\n",
              "    rubberband.mouseup('button_release', mouse_event_fn);\n",
              "    // Throttle sequential mouse events to 1 every 20ms.\n",
              "    rubberband.mousemove('motion_notify', mouse_event_fn);\n",
              "\n",
              "    rubberband.mouseenter('figure_enter', mouse_event_fn);\n",
              "    rubberband.mouseleave('figure_leave', mouse_event_fn);\n",
              "\n",
              "    canvas_div.on(\"wheel\", function (event) {\n",
              "        event = event.originalEvent;\n",
              "        event['data'] = 'scroll'\n",
              "        if (event.deltaY < 0) {\n",
              "            event.step = 1;\n",
              "        } else {\n",
              "            event.step = -1;\n",
              "        }\n",
              "        mouse_event_fn(event);\n",
              "    });\n",
              "\n",
              "    canvas_div.append(canvas);\n",
              "    canvas_div.append(rubberband);\n",
              "\n",
              "    this.rubberband = rubberband;\n",
              "    this.rubberband_canvas = rubberband[0];\n",
              "    this.rubberband_context = rubberband[0].getContext(\"2d\");\n",
              "    this.rubberband_context.strokeStyle = \"#000000\";\n",
              "\n",
              "    this._resize_canvas = function(width, height) {\n",
              "        // Keep the size of the canvas, canvas container, and rubber band\n",
              "        // canvas in synch.\n",
              "        canvas_div.css('width', width)\n",
              "        canvas_div.css('height', height)\n",
              "\n",
              "        canvas.attr('width', width * mpl.ratio);\n",
              "        canvas.attr('height', height * mpl.ratio);\n",
              "        canvas.attr('style', 'width: ' + width + 'px; height: ' + height + 'px;');\n",
              "\n",
              "        rubberband.attr('width', width);\n",
              "        rubberband.attr('height', height);\n",
              "    }\n",
              "\n",
              "    // Set the figure to an initial 600x600px, this will subsequently be updated\n",
              "    // upon first draw.\n",
              "    this._resize_canvas(600, 600);\n",
              "\n",
              "    // Disable right mouse context menu.\n",
              "    $(this.rubberband_canvas).bind(\"contextmenu\",function(e){\n",
              "        return false;\n",
              "    });\n",
              "\n",
              "    function set_focus () {\n",
              "        canvas.focus();\n",
              "        canvas_div.focus();\n",
              "    }\n",
              "\n",
              "    window.setTimeout(set_focus, 100);\n",
              "}\n",
              "\n",
              "mpl.figure.prototype._init_toolbar = function() {\n",
              "    var fig = this;\n",
              "\n",
              "    var nav_element = $('<div/>');\n",
              "    nav_element.attr('style', 'width: 100%');\n",
              "    this.root.append(nav_element);\n",
              "\n",
              "    // Define a callback function for later on.\n",
              "    function toolbar_event(event) {\n",
              "        return fig.toolbar_button_onclick(event['data']);\n",
              "    }\n",
              "    function toolbar_mouse_event(event) {\n",
              "        return fig.toolbar_button_onmouseover(event['data']);\n",
              "    }\n",
              "\n",
              "    for(var toolbar_ind in mpl.toolbar_items) {\n",
              "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
              "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
              "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
              "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
              "\n",
              "        if (!name) {\n",
              "            // put a spacer in here.\n",
              "            continue;\n",
              "        }\n",
              "        var button = $('<button/>');\n",
              "        button.addClass('ui-button ui-widget ui-state-default ui-corner-all ' +\n",
              "                        'ui-button-icon-only');\n",
              "        button.attr('role', 'button');\n",
              "        button.attr('aria-disabled', 'false');\n",
              "        button.click(method_name, toolbar_event);\n",
              "        button.mouseover(tooltip, toolbar_mouse_event);\n",
              "\n",
              "        var icon_img = $('<span/>');\n",
              "        icon_img.addClass('ui-button-icon-primary ui-icon');\n",
              "        icon_img.addClass(image);\n",
              "        icon_img.addClass('ui-corner-all');\n",
              "\n",
              "        var tooltip_span = $('<span/>');\n",
              "        tooltip_span.addClass('ui-button-text');\n",
              "        tooltip_span.html(tooltip);\n",
              "\n",
              "        button.append(icon_img);\n",
              "        button.append(tooltip_span);\n",
              "\n",
              "        nav_element.append(button);\n",
              "    }\n",
              "\n",
              "    var fmt_picker_span = $('<span/>');\n",
              "\n",
              "    var fmt_picker = $('<select/>');\n",
              "    fmt_picker.addClass('mpl-toolbar-option ui-widget ui-widget-content');\n",
              "    fmt_picker_span.append(fmt_picker);\n",
              "    nav_element.append(fmt_picker_span);\n",
              "    this.format_dropdown = fmt_picker[0];\n",
              "\n",
              "    for (var ind in mpl.extensions) {\n",
              "        var fmt = mpl.extensions[ind];\n",
              "        var option = $(\n",
              "            '<option/>', {selected: fmt === mpl.default_extension}).html(fmt);\n",
              "        fmt_picker.append(option);\n",
              "    }\n",
              "\n",
              "    // Add hover states to the ui-buttons\n",
              "    $( \".ui-button\" ).hover(\n",
              "        function() { $(this).addClass(\"ui-state-hover\");},\n",
              "        function() { $(this).removeClass(\"ui-state-hover\");}\n",
              "    );\n",
              "\n",
              "    var status_bar = $('<span class=\"mpl-message\"/>');\n",
              "    nav_element.append(status_bar);\n",
              "    this.message = status_bar[0];\n",
              "}\n",
              "\n",
              "mpl.figure.prototype.request_resize = function(x_pixels, y_pixels) {\n",
              "    // Request matplotlib to resize the figure. Matplotlib will then trigger a resize in the client,\n",
              "    // which will in turn request a refresh of the image.\n",
              "    this.send_message('resize', {'width': x_pixels, 'height': y_pixels});\n",
              "}\n",
              "\n",
              "mpl.figure.prototype.send_message = function(type, properties) {\n",
              "    properties['type'] = type;\n",
              "    properties['figure_id'] = this.id;\n",
              "    this.ws.send(JSON.stringify(properties));\n",
              "}\n",
              "\n",
              "mpl.figure.prototype.send_draw_message = function() {\n",
              "    if (!this.waiting) {\n",
              "        this.waiting = true;\n",
              "        this.ws.send(JSON.stringify({type: \"draw\", figure_id: this.id}));\n",
              "    }\n",
              "}\n",
              "\n",
              "\n",
              "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
              "    var format_dropdown = fig.format_dropdown;\n",
              "    var format = format_dropdown.options[format_dropdown.selectedIndex].value;\n",
              "    fig.ondownload(fig, format);\n",
              "}\n",
              "\n",
              "\n",
              "mpl.figure.prototype.handle_resize = function(fig, msg) {\n",
              "    var size = msg['size'];\n",
              "    if (size[0] != fig.canvas.width || size[1] != fig.canvas.height) {\n",
              "        fig._resize_canvas(size[0], size[1]);\n",
              "        fig.send_message(\"refresh\", {});\n",
              "    };\n",
              "}\n",
              "\n",
              "mpl.figure.prototype.handle_rubberband = function(fig, msg) {\n",
              "    var x0 = msg['x0'] / mpl.ratio;\n",
              "    var y0 = (fig.canvas.height - msg['y0']) / mpl.ratio;\n",
              "    var x1 = msg['x1'] / mpl.ratio;\n",
              "    var y1 = (fig.canvas.height - msg['y1']) / mpl.ratio;\n",
              "    x0 = Math.floor(x0) + 0.5;\n",
              "    y0 = Math.floor(y0) + 0.5;\n",
              "    x1 = Math.floor(x1) + 0.5;\n",
              "    y1 = Math.floor(y1) + 0.5;\n",
              "    var min_x = Math.min(x0, x1);\n",
              "    var min_y = Math.min(y0, y1);\n",
              "    var width = Math.abs(x1 - x0);\n",
              "    var height = Math.abs(y1 - y0);\n",
              "\n",
              "    fig.rubberband_context.clearRect(\n",
              "        0, 0, fig.canvas.width / mpl.ratio, fig.canvas.height / mpl.ratio);\n",
              "\n",
              "    fig.rubberband_context.strokeRect(min_x, min_y, width, height);\n",
              "}\n",
              "\n",
              "mpl.figure.prototype.handle_figure_label = function(fig, msg) {\n",
              "    // Updates the figure title.\n",
              "    fig.header.textContent = msg['label'];\n",
              "}\n",
              "\n",
              "mpl.figure.prototype.handle_cursor = function(fig, msg) {\n",
              "    var cursor = msg['cursor'];\n",
              "    switch(cursor)\n",
              "    {\n",
              "    case 0:\n",
              "        cursor = 'pointer';\n",
              "        break;\n",
              "    case 1:\n",
              "        cursor = 'default';\n",
              "        break;\n",
              "    case 2:\n",
              "        cursor = 'crosshair';\n",
              "        break;\n",
              "    case 3:\n",
              "        cursor = 'move';\n",
              "        break;\n",
              "    }\n",
              "    fig.rubberband_canvas.style.cursor = cursor;\n",
              "}\n",
              "\n",
              "mpl.figure.prototype.handle_message = function(fig, msg) {\n",
              "    fig.message.textContent = msg['message'];\n",
              "}\n",
              "\n",
              "mpl.figure.prototype.handle_draw = function(fig, msg) {\n",
              "    // Request the server to send over a new figure.\n",
              "    fig.send_draw_message();\n",
              "}\n",
              "\n",
              "mpl.figure.prototype.handle_image_mode = function(fig, msg) {\n",
              "    fig.image_mode = msg['mode'];\n",
              "}\n",
              "\n",
              "mpl.figure.prototype.updated_canvas_event = function() {\n",
              "    // Called whenever the canvas gets updated.\n",
              "    this.send_message(\"ack\", {});\n",
              "}\n",
              "\n",
              "// A function to construct a web socket function for onmessage handling.\n",
              "// Called in the figure constructor.\n",
              "mpl.figure.prototype._make_on_message_function = function(fig) {\n",
              "    return function socket_on_message(evt) {\n",
              "        if (evt.data instanceof Blob) {\n",
              "            /* FIXME: We get \"Resource interpreted as Image but\n",
              "             * transferred with MIME type text/plain:\" errors on\n",
              "             * Chrome.  But how to set the MIME type?  It doesn't seem\n",
              "             * to be part of the websocket stream */\n",
              "            evt.data.type = \"image/png\";\n",
              "\n",
              "            /* Free the memory for the previous frames */\n",
              "            if (fig.imageObj.src) {\n",
              "                (window.URL || window.webkitURL).revokeObjectURL(\n",
              "                    fig.imageObj.src);\n",
              "            }\n",
              "\n",
              "            fig.imageObj.src = (window.URL || window.webkitURL).createObjectURL(\n",
              "                evt.data);\n",
              "            fig.updated_canvas_event();\n",
              "            fig.waiting = false;\n",
              "            return;\n",
              "        }\n",
              "        else if (typeof evt.data === 'string' && evt.data.slice(0, 21) == \"data:image/png;base64\") {\n",
              "            fig.imageObj.src = evt.data;\n",
              "            fig.updated_canvas_event();\n",
              "            fig.waiting = false;\n",
              "            return;\n",
              "        }\n",
              "\n",
              "        var msg = JSON.parse(evt.data);\n",
              "        var msg_type = msg['type'];\n",
              "\n",
              "        // Call the  \"handle_{type}\" callback, which takes\n",
              "        // the figure and JSON message as its only arguments.\n",
              "        try {\n",
              "            var callback = fig[\"handle_\" + msg_type];\n",
              "        } catch (e) {\n",
              "            console.log(\"No handler for the '\" + msg_type + \"' message type: \", msg);\n",
              "            return;\n",
              "        }\n",
              "\n",
              "        if (callback) {\n",
              "            try {\n",
              "                // console.log(\"Handling '\" + msg_type + \"' message: \", msg);\n",
              "                callback(fig, msg);\n",
              "            } catch (e) {\n",
              "                console.log(\"Exception inside the 'handler_\" + msg_type + \"' callback:\", e, e.stack, msg);\n",
              "            }\n",
              "        }\n",
              "    };\n",
              "}\n",
              "\n",
              "// from http://stackoverflow.com/questions/1114465/getting-mouse-location-in-canvas\n",
              "mpl.findpos = function(e) {\n",
              "    //this section is from http://www.quirksmode.org/js/events_properties.html\n",
              "    var targ;\n",
              "    if (!e)\n",
              "        e = window.event;\n",
              "    if (e.target)\n",
              "        targ = e.target;\n",
              "    else if (e.srcElement)\n",
              "        targ = e.srcElement;\n",
              "    if (targ.nodeType == 3) // defeat Safari bug\n",
              "        targ = targ.parentNode;\n",
              "\n",
              "    // jQuery normalizes the pageX and pageY\n",
              "    // pageX,Y are the mouse positions relative to the document\n",
              "    // offset() returns the position of the element relative to the document\n",
              "    var x = e.pageX - $(targ).offset().left;\n",
              "    var y = e.pageY - $(targ).offset().top;\n",
              "\n",
              "    return {\"x\": x, \"y\": y};\n",
              "};\n",
              "\n",
              "/*\n",
              " * return a copy of an object with only non-object keys\n",
              " * we need this to avoid circular references\n",
              " * http://stackoverflow.com/a/24161582/3208463\n",
              " */\n",
              "function simpleKeys (original) {\n",
              "  return Object.keys(original).reduce(function (obj, key) {\n",
              "    if (typeof original[key] !== 'object')\n",
              "        obj[key] = original[key]\n",
              "    return obj;\n",
              "  }, {});\n",
              "}\n",
              "\n",
              "mpl.figure.prototype.mouse_event = function(event, name) {\n",
              "    var canvas_pos = mpl.findpos(event)\n",
              "\n",
              "    if (name === 'button_press')\n",
              "    {\n",
              "        this.canvas.focus();\n",
              "        this.canvas_div.focus();\n",
              "    }\n",
              "\n",
              "    var x = canvas_pos.x * mpl.ratio;\n",
              "    var y = canvas_pos.y * mpl.ratio;\n",
              "\n",
              "    this.send_message(name, {x: x, y: y, button: event.button,\n",
              "                             step: event.step,\n",
              "                             guiEvent: simpleKeys(event)});\n",
              "\n",
              "    /* This prevents the web browser from automatically changing to\n",
              "     * the text insertion cursor when the button is pressed.  We want\n",
              "     * to control all of the cursor setting manually through the\n",
              "     * 'cursor' event from matplotlib */\n",
              "    event.preventDefault();\n",
              "    return false;\n",
              "}\n",
              "\n",
              "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
              "    // Handle any extra behaviour associated with a key event\n",
              "}\n",
              "\n",
              "mpl.figure.prototype.key_event = function(event, name) {\n",
              "\n",
              "    // Prevent repeat events\n",
              "    if (name == 'key_press')\n",
              "    {\n",
              "        if (event.which === this._key)\n",
              "            return;\n",
              "        else\n",
              "            this._key = event.which;\n",
              "    }\n",
              "    if (name == 'key_release')\n",
              "        this._key = null;\n",
              "\n",
              "    var value = '';\n",
              "    if (event.ctrlKey && event.which != 17)\n",
              "        value += \"ctrl+\";\n",
              "    if (event.altKey && event.which != 18)\n",
              "        value += \"alt+\";\n",
              "    if (event.shiftKey && event.which != 16)\n",
              "        value += \"shift+\";\n",
              "\n",
              "    value += 'k';\n",
              "    value += event.which.toString();\n",
              "\n",
              "    this._key_event_extra(event, name);\n",
              "\n",
              "    this.send_message(name, {key: value,\n",
              "                             guiEvent: simpleKeys(event)});\n",
              "    return false;\n",
              "}\n",
              "\n",
              "mpl.figure.prototype.toolbar_button_onclick = function(name) {\n",
              "    if (name == 'download') {\n",
              "        this.handle_save(this, null);\n",
              "    } else {\n",
              "        this.send_message(\"toolbar_button\", {name: name});\n",
              "    }\n",
              "};\n",
              "\n",
              "mpl.figure.prototype.toolbar_button_onmouseover = function(tooltip) {\n",
              "    this.message.textContent = tooltip;\n",
              "};\n",
              "mpl.toolbar_items = [[\"Home\", \"Reset original view\", \"fa fa-home icon-home\", \"home\"], [\"Back\", \"Back to previous view\", \"fa fa-arrow-left icon-arrow-left\", \"back\"], [\"Forward\", \"Forward to next view\", \"fa fa-arrow-right icon-arrow-right\", \"forward\"], [\"\", \"\", \"\", \"\"], [\"Pan\", \"Pan axes with left mouse, zoom with right\", \"fa fa-arrows icon-move\", \"pan\"], [\"Zoom\", \"Zoom to rectangle\", \"fa fa-square-o icon-check-empty\", \"zoom\"], [\"\", \"\", \"\", \"\"], [\"Download\", \"Download plot\", \"fa fa-floppy-o icon-save\", \"download\"]];\n",
              "\n",
              "mpl.extensions = [\"eps\", \"jpeg\", \"pdf\", \"png\", \"ps\", \"raw\", \"svg\", \"tif\"];\n",
              "\n",
              "mpl.default_extension = \"png\";var comm_websocket_adapter = function(comm) {\n",
              "    // Create a \"websocket\"-like object which calls the given IPython comm\n",
              "    // object with the appropriate methods. Currently this is a non binary\n",
              "    // socket, so there is still some room for performance tuning.\n",
              "    var ws = {};\n",
              "\n",
              "    ws.close = function() {\n",
              "        comm.close()\n",
              "    };\n",
              "    ws.send = function(m) {\n",
              "        //console.log('sending', m);\n",
              "        comm.send(m);\n",
              "    };\n",
              "    // Register the callback with on_msg.\n",
              "    comm.on_msg(function(msg) {\n",
              "        //console.log('receiving', msg['content']['data'], msg);\n",
              "        // Pass the mpl event to the overridden (by mpl) onmessage function.\n",
              "        ws.onmessage(msg['content']['data'])\n",
              "    });\n",
              "    return ws;\n",
              "}\n",
              "\n",
              "mpl.mpl_figure_comm = function(comm, msg) {\n",
              "    // This is the function which gets called when the mpl process\n",
              "    // starts-up an IPython Comm through the \"matplotlib\" channel.\n",
              "\n",
              "    var id = msg.content.data.id;\n",
              "    // Get hold of the div created by the display call when the Comm\n",
              "    // socket was opened in Python.\n",
              "    var element = $(\"#\" + id);\n",
              "    var ws_proxy = comm_websocket_adapter(comm)\n",
              "\n",
              "    function ondownload(figure, format) {\n",
              "        window.open(figure.imageObj.src);\n",
              "    }\n",
              "\n",
              "    var fig = new mpl.figure(id, ws_proxy,\n",
              "                           ondownload,\n",
              "                           element.get(0));\n",
              "\n",
              "    // Call onopen now - mpl needs it, as it is assuming we've passed it a real\n",
              "    // web socket which is closed, not our websocket->open comm proxy.\n",
              "    ws_proxy.onopen();\n",
              "\n",
              "    fig.parent_element = element.get(0);\n",
              "    fig.cell_info = mpl.find_output_cell(\"<div id='\" + id + \"'></div>\");\n",
              "    if (!fig.cell_info) {\n",
              "        console.error(\"Failed to find cell for figure\", id, fig);\n",
              "        return;\n",
              "    }\n",
              "\n",
              "    var output_index = fig.cell_info[2]\n",
              "    var cell = fig.cell_info[0];\n",
              "\n",
              "};\n",
              "\n",
              "mpl.figure.prototype.handle_close = function(fig, msg) {\n",
              "    var width = fig.canvas.width/mpl.ratio\n",
              "    fig.root.unbind('remove')\n",
              "\n",
              "    // Update the output cell to use the data from the current canvas.\n",
              "    fig.push_to_output();\n",
              "    var dataURL = fig.canvas.toDataURL();\n",
              "    // Re-enable the keyboard manager in IPython - without this line, in FF,\n",
              "    // the notebook keyboard shortcuts fail.\n",
              "    IPython.keyboard_manager.enable()\n",
              "    $(fig.parent_element).html('<img src=\"' + dataURL + '\" width=\"' + width + '\">');\n",
              "    fig.close_ws(fig, msg);\n",
              "}\n",
              "\n",
              "mpl.figure.prototype.close_ws = function(fig, msg){\n",
              "    fig.send_message('closing', msg);\n",
              "    // fig.ws.close()\n",
              "}\n",
              "\n",
              "mpl.figure.prototype.push_to_output = function(remove_interactive) {\n",
              "    // Turn the data on the canvas into data in the output cell.\n",
              "    var width = this.canvas.width/mpl.ratio\n",
              "    var dataURL = this.canvas.toDataURL();\n",
              "    this.cell_info[1]['text/html'] = '<img src=\"' + dataURL + '\" width=\"' + width + '\">';\n",
              "}\n",
              "\n",
              "mpl.figure.prototype.updated_canvas_event = function() {\n",
              "    // Tell IPython that the notebook contents must change.\n",
              "    IPython.notebook.set_dirty(true);\n",
              "    this.send_message(\"ack\", {});\n",
              "    var fig = this;\n",
              "    // Wait a second, then push the new image to the DOM so\n",
              "    // that it is saved nicely (might be nice to debounce this).\n",
              "    setTimeout(function () { fig.push_to_output() }, 1000);\n",
              "}\n",
              "\n",
              "mpl.figure.prototype._init_toolbar = function() {\n",
              "    var fig = this;\n",
              "\n",
              "    var nav_element = $('<div/>');\n",
              "    nav_element.attr('style', 'width: 100%');\n",
              "    this.root.append(nav_element);\n",
              "\n",
              "    // Define a callback function for later on.\n",
              "    function toolbar_event(event) {\n",
              "        return fig.toolbar_button_onclick(event['data']);\n",
              "    }\n",
              "    function toolbar_mouse_event(event) {\n",
              "        return fig.toolbar_button_onmouseover(event['data']);\n",
              "    }\n",
              "\n",
              "    for(var toolbar_ind in mpl.toolbar_items){\n",
              "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
              "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
              "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
              "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
              "\n",
              "        if (!name) { continue; };\n",
              "\n",
              "        var button = $('<button class=\"btn btn-default\" href=\"#\" title=\"' + name + '\"><i class=\"fa ' + image + ' fa-lg\"></i></button>');\n",
              "        button.click(method_name, toolbar_event);\n",
              "        button.mouseover(tooltip, toolbar_mouse_event);\n",
              "        nav_element.append(button);\n",
              "    }\n",
              "\n",
              "    // Add the status bar.\n",
              "    var status_bar = $('<span class=\"mpl-message\" style=\"text-align:right; float: right;\"/>');\n",
              "    nav_element.append(status_bar);\n",
              "    this.message = status_bar[0];\n",
              "\n",
              "    // Add the close button to the window.\n",
              "    var buttongrp = $('<div class=\"btn-group inline pull-right\"></div>');\n",
              "    var button = $('<button class=\"btn btn-mini btn-primary\" href=\"#\" title=\"Stop Interaction\"><i class=\"fa fa-power-off icon-remove icon-large\"></i></button>');\n",
              "    button.click(function (evt) { fig.handle_close(fig, {}); } );\n",
              "    button.mouseover('Stop Interaction', toolbar_mouse_event);\n",
              "    buttongrp.append(button);\n",
              "    var titlebar = this.root.find($('.ui-dialog-titlebar'));\n",
              "    titlebar.prepend(buttongrp);\n",
              "}\n",
              "\n",
              "mpl.figure.prototype._root_extra_style = function(el){\n",
              "    var fig = this\n",
              "    el.on(\"remove\", function(){\n",
              "\tfig.close_ws(fig, {});\n",
              "    });\n",
              "}\n",
              "\n",
              "mpl.figure.prototype._canvas_extra_style = function(el){\n",
              "    // this is important to make the div 'focusable\n",
              "    el.attr('tabindex', 0)\n",
              "    // reach out to IPython and tell the keyboard manager to turn it's self\n",
              "    // off when our div gets focus\n",
              "\n",
              "    // location in version 3\n",
              "    if (IPython.notebook.keyboard_manager) {\n",
              "        IPython.notebook.keyboard_manager.register_events(el);\n",
              "    }\n",
              "    else {\n",
              "        // location in version 2\n",
              "        IPython.keyboard_manager.register_events(el);\n",
              "    }\n",
              "\n",
              "}\n",
              "\n",
              "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
              "    var manager = IPython.notebook.keyboard_manager;\n",
              "    if (!manager)\n",
              "        manager = IPython.keyboard_manager;\n",
              "\n",
              "    // Check for shift+enter\n",
              "    if (event.shiftKey && event.which == 13) {\n",
              "        this.canvas_div.blur();\n",
              "        // select the cell after this one\n",
              "        var index = IPython.notebook.find_cell_index(this.cell_info[0]);\n",
              "        IPython.notebook.select(index + 1);\n",
              "    }\n",
              "}\n",
              "\n",
              "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
              "    fig.ondownload(fig, null);\n",
              "}\n",
              "\n",
              "\n",
              "mpl.find_output_cell = function(html_output) {\n",
              "    // Return the cell and output element which can be found *uniquely* in the notebook.\n",
              "    // Note - this is a bit hacky, but it is done because the \"notebook_saving.Notebook\"\n",
              "    // IPython event is triggered only after the cells have been serialised, which for\n",
              "    // our purposes (turning an active figure into a static one), is too late.\n",
              "    var cells = IPython.notebook.get_cells();\n",
              "    var ncells = cells.length;\n",
              "    for (var i=0; i<ncells; i++) {\n",
              "        var cell = cells[i];\n",
              "        if (cell.cell_type === 'code'){\n",
              "            for (var j=0; j<cell.output_area.outputs.length; j++) {\n",
              "                var data = cell.output_area.outputs[j];\n",
              "                if (data.data) {\n",
              "                    // IPython >= 3 moved mimebundle to data attribute of output\n",
              "                    data = data.data;\n",
              "                }\n",
              "                if (data['text/html'] == html_output) {\n",
              "                    return [cell, data, j];\n",
              "                }\n",
              "            }\n",
              "        }\n",
              "    }\n",
              "}\n",
              "\n",
              "// Register the function which deals with the matplotlib target/channel.\n",
              "// The kernel may be null if the page has been refreshed.\n",
              "if (IPython.notebook.kernel != null) {\n",
              "    IPython.notebook.kernel.comm_manager.register_target('matplotlib', mpl.mpl_figure_comm);\n",
              "}\n"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<div id='59a74377-db32-4375-970c-f0d2827896ee'></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 345/345 [00:58<00:00,  5.89it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The network is evaluated and the results are stored in the subdirectory 'evaluation_results'.\n",
            "If it generalizes well, choose the best model for prediction and update the config file with the appropriate index for the 'snapshotindex'.\n",
            "Use the function 'analyze_video' to make predictions on new videos.\n",
            "Otherwise consider retraining the network (see DeepLabCut workflow Fig 2)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FvyHjHj3co3U"
      },
      "source": [
        "#deeplabcut.extract_save_all_maps(path_config_file, shuffle=1, Indices=[0, 5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TFvNo-dekJP8"
      },
      "source": [
        "### Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UBJ3OPIacrGX"
      },
      "source": [
        "videofile_path = '/content/drive/My Drive/Stage/white_TRIM.avi'\n",
        "deeplabcut.analyze_videos(path_config_file, [videofile_path], save_as_csv=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LjDe1CZVcsrR"
      },
      "source": [
        "deeplabcut.plot_trajectories(path_config_file, videofile_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Z33vM8AcuW7"
      },
      "source": [
        "deeplabcut.create_labeled_video(path_config_file,videofile_path, videotype='.avi', draw_skeleton = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s4HNeV7kd7GQ",
        "outputId": "f700b4c7-34fe-440c-b86c-6170f9a6c0ac"
      },
      "source": [
        "deeplabcut.analyze_videos_converth5_to_csv('/content/drive/My Drive/Stage/Noise_check/2019/',videotype=VideoType)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found output file for scorer: DLC_resnet50_downsampled_trackerSep16shuffle1_180000\n",
            "Converting /content/drive/My Drive/Stage/Noise_check/2019/rat_training_OS_N_S_SD_CI_1_t0001downsampledDLC_resnet50_downsampled_trackerSep16shuffle1_180000.h5...\n",
            "Found output file for scorer: DLC_resnet50_downsampled_trackerSep16shuffle1_180000\n",
            "Converting /content/drive/My Drive/Stage/Noise_check/2019/rat_training_OS_N_S_SD_CI_1_t0002downsampledDLC_resnet50_downsampled_trackerSep16shuffle1_180000.h5...\n",
            "Found output file for scorer: DLC_resnet50_downsampled_trackerSep16shuffle1_180000\n",
            "Converting /content/drive/My Drive/Stage/Noise_check/2019/rat_training_OS_N_S_SD_CI_1_t0003downsampledDLC_resnet50_downsampled_trackerSep16shuffle1_180000.h5...\n",
            "Found output file for scorer: DLC_resnet50_downsampled_trackerSep16shuffle1_180000\n",
            "Converting /content/drive/My Drive/Stage/Noise_check/2019/rat_training_OS_N_S_SD_CI_1_t0004downsampledDLC_resnet50_downsampled_trackerSep16shuffle1_180000.h5...\n",
            "Found output file for scorer: DLC_resnet50_downsampled_trackerSep16shuffle1_180000\n",
            "Converting /content/drive/My Drive/Stage/Noise_check/2019/rat_training_OS_N_S_SD_CI_1_t0005downsampledDLC_resnet50_downsampled_trackerSep16shuffle1_180000.h5...\n",
            "Found output file for scorer: DLC_resnet50_downsampled_trackerSep16shuffle1_180000\n",
            "Converting /content/drive/My Drive/Stage/Noise_check/2019/rat_training_OS_N_S_SD_CI_1_t0006downsampledDLC_resnet50_downsampled_trackerSep16shuffle1_180000.h5...\n",
            "Found output file for scorer: DLC_resnet50_downsampled_trackerSep16shuffle1_180000\n",
            "Converting /content/drive/My Drive/Stage/Noise_check/2019/rat_training_OS_N_S_SD_CI_1_t0007downsampledDLC_resnet50_downsampled_trackerSep16shuffle1_180000.h5...\n",
            "Found output file for scorer: DLC_resnet50_downsampled_trackerSep16shuffle1_180000\n",
            "Converting /content/drive/My Drive/Stage/Noise_check/2019/rat_training_OS_N_S_SD_CI_1_t0008downsampledDLC_resnet50_downsampled_trackerSep16shuffle1_180000.h5...\n",
            "Found output file for scorer: DLC_resnet50_downsampled_trackerSep16shuffle1_180000\n",
            "Converting /content/drive/My Drive/Stage/Noise_check/2019/rat_training_OS_N_S_SD_CI_1_t0009downsampledDLC_resnet50_downsampled_trackerSep16shuffle1_180000.h5...\n",
            "Found output file for scorer: DLC_resnet50_downsampled_trackerSep16shuffle1_180000\n",
            "Converting /content/drive/My Drive/Stage/Noise_check/2019/rat_training_OS_N_S_SD_CI_1_t0010downsampledDLC_resnet50_downsampled_trackerSep16shuffle1_180000.h5...\n",
            "Found output file for scorer: DLC_resnet50_downsampled_trackerSep16shuffle1_180000\n",
            "Converting /content/drive/My Drive/Stage/Noise_check/2019/rat_training_OS_N_S_SD_CI_1_t0011downsampledDLC_resnet50_downsampled_trackerSep16shuffle1_180000.h5...\n",
            "Found output file for scorer: DLC_resnet50_downsampled_trackerSep16shuffle1_180000\n",
            "Converting /content/drive/My Drive/Stage/Noise_check/2019/rat_training_OS_N_S_SD_CI_1_t0012downsampledDLC_resnet50_downsampled_trackerSep16shuffle1_180000.h5...\n",
            "Found output file for scorer: DLC_resnet50_downsampled_trackerSep16shuffle1_180000\n",
            "Converting /content/drive/My Drive/Stage/Noise_check/2019/rat_training_OS_N_S_SD_CI_1_t0013downsampledDLC_resnet50_downsampled_trackerSep16shuffle1_180000.h5...\n",
            "Found output file for scorer: DLC_resnet50_downsampled_trackerSep16shuffle1_180000\n",
            "Converting /content/drive/My Drive/Stage/Noise_check/2019/rat_training_OS_N_S_SD_CI_1_t0014downsampledDLC_resnet50_downsampled_trackerSep16shuffle1_180000.h5...\n",
            "Found output file for scorer: DLC_resnet50_downsampled_trackerSep16shuffle1_180000\n",
            "Converting /content/drive/My Drive/Stage/Noise_check/2019/rat_training_OS_N_S_SD_CI_1_t0015downsampledDLC_resnet50_downsampled_trackerSep16shuffle1_180000.h5...\n",
            "Found output file for scorer: DLC_resnet50_downsampled_trackerSep16shuffle1_180000\n",
            "Converting /content/drive/My Drive/Stage/Noise_check/2019/rat_training_OS_rgs14_5_24h_72h_inc_216_221_t0001downsampledDLC_resnet50_downsampled_trackerSep16shuffle1_180000.h5...\n",
            "Found output file for scorer: DLC_resnet50_downsampled_trackerSep16shuffle1_180000\n",
            "Converting /content/drive/My Drive/Stage/Noise_check/2019/rat_training_OS_rgs14_5_24h_72h_inc_216_221_t0002downsampledDLC_resnet50_downsampled_trackerSep16shuffle1_180000.h5...\n",
            "Found output file for scorer: DLC_resnet50_downsampled_trackerSep16shuffle1_180000\n",
            "Converting /content/drive/My Drive/Stage/Noise_check/2019/rat_training_OS_rgs14_5_24h_72h_inc_216_221_t0003downsampledDLC_resnet50_downsampled_trackerSep16shuffle1_180000.h5...\n",
            "Found output file for scorer: DLC_resnet50_downsampled_trackerSep16shuffle1_180000\n",
            "Converting /content/drive/My Drive/Stage/Noise_check/2019/rat_training_OS_rgs14_5_24h_72h_inc_216_221_t0004downsampledDLC_resnet50_downsampled_trackerSep16shuffle1_180000.h5...\n",
            "Found output file for scorer: DLC_resnet50_downsampled_trackerSep16shuffle1_180000\n",
            "Converting /content/drive/My Drive/Stage/Noise_check/2019/rat_training_OS_rgs14_5_24h_72h_inc_216_221_t0005downsampledDLC_resnet50_downsampled_trackerSep16shuffle1_180000.h5...\n",
            "Found output file for scorer: DLC_resnet50_downsampled_trackerSep16shuffle1_180000\n",
            "Converting /content/drive/My Drive/Stage/Noise_check/2019/rat_training_OS_rgs14_5_24h_72h_inc_216_221_t0006downsampledDLC_resnet50_downsampled_trackerSep16shuffle1_180000.h5...\n",
            "Found output file for scorer: DLC_resnet50_downsampled_trackerSep16shuffle1_180000\n",
            "Converting /content/drive/My Drive/Stage/Noise_check/2019/rat_training_OS_rgs14_5_24h_72h_inc_216_221_t0007downsampledDLC_resnet50_downsampled_trackerSep16shuffle1_180000.h5...\n",
            "Found output file for scorer: DLC_resnet50_downsampled_trackerSep16shuffle1_180000\n",
            "Converting /content/drive/My Drive/Stage/Noise_check/2019/rat_training_OS_rgs14_5_24h_72h_inc_216_221_t0008downsampledDLC_resnet50_downsampled_trackerSep16shuffle1_180000.h5...\n",
            "Found output file for scorer: DLC_resnet50_downsampled_trackerSep16shuffle1_180000\n",
            "Converting /content/drive/My Drive/Stage/Noise_check/2019/rat_training_OS_rgs14_5_24h_72h_inc_216_221_t0009downsampledDLC_resnet50_downsampled_trackerSep16shuffle1_180000.h5...\n",
            "Found output file for scorer: DLC_resnet50_downsampled_trackerSep16shuffle1_180000\n",
            "Converting /content/drive/My Drive/Stage/Noise_check/2019/rat_training_OS_rgs14_5_24h_72h_inc_216_221_t0010downsampledDLC_resnet50_downsampled_trackerSep16shuffle1_180000.h5...\n",
            "Found output file for scorer: DLC_resnet50_downsampled_trackerSep16shuffle1_180000\n",
            "Converting /content/drive/My Drive/Stage/Noise_check/2019/rat_training_OS_rgs14_5_24h_72h_inc_216_221_t0011downsampledDLC_resnet50_downsampled_trackerSep16shuffle1_180000.h5...\n",
            "Found output file for scorer: DLC_resnet50_downsampled_trackerSep16shuffle1_180000\n",
            "Converting /content/drive/My Drive/Stage/Noise_check/2019/rat_training_OS_rgs14_5_24h_72h_inc_216_221_t0012downsampledDLC_resnet50_downsampled_trackerSep16shuffle1_180000.h5...\n",
            "Found output file for scorer: DLC_resnet50_downsampled_trackerSep16shuffle1_180000\n",
            "Converting /content/drive/My Drive/Stage/Noise_check/2019/rat_training_OS_rgs14_5_24h_72h_inc_216_221_t0013downsampledDLC_resnet50_downsampled_trackerSep16shuffle1_180000.h5...\n",
            "Found output file for scorer: DLC_resnet50_downsampled_trackerSep16shuffle1_180000\n",
            "Converting /content/drive/My Drive/Stage/Noise_check/2019/rat_training_OS_rgs14_5_24h_72h_inc_216_221_t0014downsampledDLC_resnet50_downsampled_trackerSep16shuffle1_180000.h5...\n",
            "Found output file for scorer: DLC_resnet50_downsampled_trackerSep16shuffle1_180000\n",
            "Converting /content/drive/My Drive/Stage/Noise_check/2019/rat_training_OS_rgs14_5_24h_72h_inc_216_221_t0015downsampledDLC_resnet50_downsampled_trackerSep16shuffle1_180000.h5...\n",
            "All pose files were converted.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1sPAlokkY04"
      },
      "source": [
        "### Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iKkhjg-znmLk",
        "outputId": "af365d08-76e0-4fb2-cbfc-921d639ccecf"
      },
      "source": [
        " import os\n",
        " basepath = '/content/drive/My Drive/Stage/Downsampled'\n",
        "\n",
        "tracker_name = 'DLC_resnet50_downsampled_trackerSep16shuffle1_180000'\n",
        "all_files = []\n",
        "for root, dirs, files in os.walk(basepath):\n",
        "    for file in files:\n",
        "        if file.endswith('.avi'):\n",
        "            file = file.strip('.avi')\n",
        "            file = file+tracker_name+'.csv'\n",
        "            all_files.append(file)\n",
        "            print(file)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trail_1_whitedownsampledDLC_resnet50_downsampled_trackerSep16shuffle1_180000.csv\n",
            "trail_1_bluedownsampledDLC_resnet50_downsampled_trackerSep16shuffle1_180000.csv\n",
            "trail_1_greendownsampledDLC_resnet50_downsampled_trackerSep16shuffle1_180000.csv\n",
            "trail_1_reddownsampledDLC_resnet50_downsampled_trackerSep16shuffle1_180000.csv\n",
            "trail_1_white(2)downsampledDLC_resnet50_downsampled_trackerSep16shuffle1_180000.csv\n",
            "trail_2_bluedownsampledDLC_resnet50_downsampled_trackerSep16shuffle1_180000.csv\n",
            "trail_2_greendownsampledDLC_resnet50_downsampled_trackerSep16shuffle1_180000.csv\n",
            "trail_2_reddownsampledDLC_resnet50_downsampled_trackerSep16shuffle1_180000.csv\n",
            "trail_2_white(2)downsampledDLC_resnet50_downsampled_trackerSep16shuffle1_180000.csv\n",
            "trail_2_whitedownsampledDLC_resnet50_downsampled_trackerSep16shuffle1_180000.csv\n",
            "trail_3_bluedownsampledDLC_resnet50_downsampled_trackerSep16shuffle1_180000.csv\n",
            "trail_3_greendownsampledDLC_resnet50_downsampled_trackerSep16shuffle1_180000.csv\n",
            "trail_3_whitedownsampledDLC_resnet50_downsampled_trackerSep16shuffle1_180000.csv\n",
            "trail_4_bluedownsampledDLC_resnet50_downsampled_trackerSep16shuffle1_180000.csv\n",
            "trail_4_greendownsampledDLC_resnet50_downsampled_trackerSep16shuffle1_180000.csv\n",
            "trail_4_white(2)downsampledDLC_resnet50_downsampled_trackerSep16shuffle1_180000.csv\n",
            "trail_4_whitedownsampledDLC_resnet50_downsampled_trackerSep16shuffle1_180000.csv\n",
            "trail_5_greendownsampledDLC_resnet50_downsampled_trackerSep16shuffle1_180000.csv\n",
            "trail_5_white(2)downsampledDLC_resnet50_downsampled_trackerSep16shuffle1_180000.csv\n",
            "trail_5_whitedownsampledDLC_resnet50_downsampled_trackerSep16shuffle1_180000.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-g47BNDPkXob",
        "outputId": "010a64e1-35f3-4aa0-861f-70b54ff3b687"
      },
      "source": [
        "import os\n",
        "import deeplabcut\n",
        "\n",
        "def get_analysis_files(directory, tracker_name):\n",
        "  #tracker_name = 'DLC_resnet50_downsampled_trackerSep16shuffle1_180000'\n",
        "  all_analysis_files = []\n",
        "  all_skeleton_files = []\n",
        "  for root, dirs, files in os.walk(directory):\n",
        "      for file in files:\n",
        "          if file.endswith('.avi'):\n",
        "              file = file.strip('.avi')\n",
        "              file_a = file+tracker_name+'.csv'\n",
        "              file_s = file+tracker_name+'_skeleton.csv'\n",
        "              all_analysis_files.append(file_a)\n",
        "              all_skeleton_files.append(file_s)\n",
        "  \n",
        "def main_deeplabcut(directory, config, videoType, save_csv, create_labeled, create_plots, analyze_skeleton):\n",
        "  # analyse\n",
        "  deeplabcut.analyze_videos(config, [directory], save_as_csv=save_csv)\n",
        "  if create_plots:\n",
        "    # plot trajectory\n",
        "    deeplabcut.plot_trajectories(config, [directory])\n",
        "  if create_labeled:\n",
        "    deeplabcut.create_labeled_video(config, [directory], videotype=videoType, draw_skeleton = True)\n",
        "  if analyze_skeleton:\n",
        "    deeplabcut.analyzeskeleton(config, [directory], videotype=videoType, shuffle=1, trainingsetindex=0, save_as_csv=True, destfolder=None)\n",
        "\n",
        "\n",
        "ProjectFolderName = 'Stage/final_tracker-Sanne-2021-10-20'\n",
        "VideoType = 'avi' \n",
        "path_config_file = '/content/drive/My Drive/'+ProjectFolderName+'/config.yaml'\n",
        "\n",
        "main_deeplabcut('/content/drive/My Drive/Stage/Noise_check/2020/',path_config_file,VideoType,True,True,True,True)\n",
        "main_deeplabcut('/content/drive/My Drive/Stage/Noise_check/2017/',path_config_file,VideoType,save_csv=False,create_labeled=False,create_plots=True,analyze_skeleton=False)\n",
        "main_deeplabcut('/content/drive/My Drive/Stage/Noise_check/2018/',path_config_file,VideoType,save_csv=False,create_labeled=False,create_plots=True,analyze_skeleton=False)\n",
        "main_deeplabcut('/content/drive/My Drive/Stage/Noise_check/2019/',path_config_file,VideoType,save_csv=False,create_labeled=False,create_plots=True,analyze_skeleton=False)\n",
        "main_deeplabcut('/content/drive/My Drive/Stage/Trials/Downsampled/',path_config_file,VideoType,save_csv=False,create_labeled=False,create_plots=True,analyze_skeleton=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using snapshot-312000 for model /content/drive/My Drive/Stage/final_tracker-Sanne-2021-10-20/dlc-models/iteration-0/final_trackerOct20-trainset95shuffle1\n",
            "Analyzing all the videos in the directory...\n",
            "Starting to analyze %  /content/drive/My Drive/Stage/Noise_check/2020/rat_training_OS_N_S_SD_CI_1_t0004_raw_2020downsampled.avi\n",
            "/content/drive/My Drive/Stage/Noise_check/2020  already exists!\n",
            "Loading  /content/drive/My Drive/Stage/Noise_check/2020/rat_training_OS_N_S_SD_CI_1_t0004_raw_2020downsampled.avi\n",
            "Duration of video [s]:  336.37 , recorded with  30.0 fps!\n",
            "Overall # of frames:  10091  found with (before cropping) frame dimensions:  341 256\n",
            "Starting to extract posture\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  3%|▎         | 300/10091 [01:16<44:24,  3.67it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hk67h0rHfiRF"
      },
      "source": [
        "## Automatic assesment of tracker\n",
        "\n",
        "- ### tracking an object:\n",
        "\t##### signs:\n",
        "\t- Huge jump\n",
        "\t- Constant coordinates for a long period of time\n",
        "\t- likelihood can be high in this case (sometimes its just super certain its a rat while its not)\n",
        "- ### Uncertain tracking:\n",
        "\t##### signs:\n",
        "\t- Likelihood around 20-80%\n",
        "\t- Tracking graph might seem reasonable but is probably tracking wrong parts\n",
        "\t\t- Should manually assess the video to be certain\n",
        "\t- Large gaps when applying pc-cutOff (basically everything below 0,6 is not taken into account when plotting the trajectory or making a video)\n",
        "\n",
        "\n",
        "### What do I want to achieve:\n",
        "\n",
        "\t- Average likelihood of all bodyparts seperatly\n",
        "\t- Average likelihood of everything combined\n",
        "\t- List of videos that performed poorly\n",
        "\t- Reason as to why they performed poorly\n",
        "\t\t- Tracking an object (object not used before has similar features)\n",
        "\t\t- Uncertain tracking (new situation not trained on before)\n",
        "\n",
        "\n",
        "### Scoring the performance\n",
        "\n",
        "things to consider in score:\n",
        "\n",
        "\n",
        "*   Head and spine most important\n",
        "*   Tail parts least important\n",
        "*   Outliers can mean lots of false positives\n",
        "*   High likelihood is good if most is high and the rest very low if there is more in between than low, the model is uncertain\n",
        "\n",
        "\n",
        "### likelihood system:\n",
        "I give each video a score based on their likelihoods by calculating how much of their values are above 80%, between 80% and 20% and below 20%. If most is above 80% it is good but there might still be uncertainty. If most if above 80% and the more is below 20% than in the between area it means there is less uncertainty and it is good. If the likelihood is not on average above 80% it means it is simply uncertain\n",
        "\n",
        "these percentages should change\n",
        "\n",
        "*   1 = high likelihood and not much uncertainty\n",
        "*   2 = high likelihood but with some uncertainty\n",
        "*   3 = low likelihood with much uncertainty\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nCCt01McfrUS",
        "outputId": "58b27803-ebc6-4cfb-8e5e-4b37fcfaae75"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "import os\n",
        "\n",
        "\n",
        "def main3():\n",
        "  file_path = '/content/drive/My Drive/Stage/Noise_check/2017/'\n",
        "  #analysis_file = 'rat_training_OS_SERT_5d_t0013downsampledDLC_resnet50_downsampled_trackerSep16shuffle1_180000.csv'\n",
        "  \n",
        "  \n",
        "  all_files = []\n",
        "  for root, dirs, files in os.walk(file_path):\n",
        "      for file in files:\n",
        "          if file.endswith('.csv'):\n",
        "              if 'skeleton' not in file:\n",
        "                all_files.append(file)\n",
        "                print(file)\n",
        "  parts_dic = {'x' : 'Head', 'x.1' : 'Side left', 'x.2' : 'Side right', 'x.3' : 'Spine', 'x.4' : 'Tail base', 'x.5': 'Tail middle', 'x.6': 'Tail end'}\n",
        "  parts = ['x', 'x.1', 'x.2', 'x.3', 'x.4', 'x.5', 'x.6']\n",
        "  result_dic = {}\n",
        "\n",
        "  \n",
        "  for analysis_file in all_files:\n",
        "    DF_analysis = pd.read_csv(file_path+analysis_file, skiprows=2)\n",
        "    t = DF_analysis.diff()\n",
        "  #  print('Outlier/Jump analysis')\n",
        "    full_o = []\n",
        "    for value in parts:\n",
        "      # if the outlier value is above like 0.1% of everyting its already too much\n",
        "      # outlier value should be below 0.09% Basically as little as possible bc jumps of 100 pixels should not occur\n",
        "      outliers =t.loc[t[value]> 100]\n",
        "      #print(parts_dic[value])\n",
        "      #print(str(len(outliers)/len(DF_analysis)*100) + ' %')\n",
        "      full_o.append(len(outliers)/len(DF_analysis)*100)\n",
        "    print(sum(full_o)/len(full_o))\n",
        "    if sum(full_o)/len(full_o) < 0.5:\n",
        "      print('false')\n",
        "      result_dic[analysis_file] = {'outlier' : False}\n",
        "    else:\n",
        "      print('true')\n",
        "      result_dic[analysis_file] = {'outlier' : True}\n",
        "    result_dic[analysis_file]['outlier_results'] = full_o\n",
        "\n",
        "    #print()\n",
        "    #print('likelihood analysis')\n",
        "    full_l = []\n",
        "    full_c = []\n",
        "    full_low = []\n",
        "    for part in parts:\n",
        "      likelihood = part.replace('x', 'likelihood')\n",
        "      # if a and b are low and c is high it is good\n",
        "      # if b and a are high en c is low it is very bad\n",
        "      # if b is low and b is average and c is high it is good\n",
        "      ## Find out what high, low and average values are\n",
        "      uncertain = DF_analysis.loc[DF_analysis[likelihood].between(0.1,0.9)]\n",
        "      low = DF_analysis.loc[DF_analysis[likelihood] < 0.1]\n",
        "      high = DF_analysis.loc[DF_analysis[likelihood] > 0.9]\n",
        "      full_l.append(len(high)/len(DF_analysis)*100)\n",
        "      full_c.append(len(uncertain)/len(DF_analysis)*100)\n",
        "      full_low.append(len(low)/len(DF_analysis)*100)\n",
        "      #print(parts_dic[part])\n",
        "      #print(len(uncertain)/len(DF_analysis)*100)\n",
        "      #print(len(low)/len(DF_analysis)*100)\n",
        "      #print(len(high)/len(DF_analysis)*100)\n",
        "    if sum(full_l)/len(full_l) >= 80.0 and sum(full_low)/len(full_low) > sum(full_c)/len(full_c):\n",
        "      result_dic[analysis_file]['likelihood'] = 1\n",
        "    elif sum(full_l)/len(full_l) >= 80.0:\n",
        "      result_dic[analysis_file]['likelihood'] = 2\n",
        "    else:\n",
        "      result_dic[analysis_file]['likelihood'] = 3\n",
        "    result_dic[analysis_file]['likelihood_results'] = full_l\n",
        "    max_l = 'x.' + str(result_dic[analysis_file]['likelihood_results'].\n",
        "                      index(max(result_dic[analysis_file]['likelihood_results']))\n",
        "                      )\n",
        "    min_l = 'x.' + str(result_dic[analysis_file]['likelihood_results'].\n",
        "                      index(min(result_dic[analysis_file]['likelihood_results']))\n",
        "                      )\n",
        "    max_o = 'x.' + str(result_dic[analysis_file]['outlier_results'].\n",
        "                      index(max(result_dic[analysis_file]['outlier_results']))\n",
        "                      )\n",
        "    min_o = 'x.' + str(result_dic[analysis_file]['outlier_results'].\n",
        "                      index(min(result_dic[analysis_file]['outlier_results']))\n",
        "                      )\n",
        "  return result_dic\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "d = main3()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "rat_training_OS_SERT_5d_t0001downsampledDLC_resnet50_downsampled_trackerSep16shuffle1_180000.csv\n",
            "rat_training_OS_SERT_5d_t0002downsampledDLC_resnet50_downsampled_trackerSep16shuffle1_180000.csv\n",
            "rat_training_OS_SERT_5d_t0003downsampledDLC_resnet50_downsampled_trackerSep16shuffle1_180000.csv\n",
            "rat_training_OS_SERT_5d_t0004downsampledDLC_resnet50_downsampled_trackerSep16shuffle1_180000.csv\n",
            "rat_training_OS_SERT_5d_t0005downsampledDLC_resnet50_downsampled_trackerSep16shuffle1_180000.csv\n",
            "rat_training_OS_SERT_5d_t0006downsampledDLC_resnet50_downsampled_trackerSep16shuffle1_180000.csv\n",
            "rat_training_OS_SERT_5d_t0007downsampledDLC_resnet50_downsampled_trackerSep16shuffle1_180000.csv\n",
            "rat_training_OS_SERT_5d_t0008downsampledDLC_resnet50_downsampled_trackerSep16shuffle1_180000.csv\n",
            "rat_training_OS_SERT_5d_t0009downsampledDLC_resnet50_downsampled_trackerSep16shuffle1_180000.csv\n",
            "rat_training_OS_SERT_5d_t0010downsampledDLC_resnet50_downsampled_trackerSep16shuffle1_180000.csv\n",
            "rat_training_OS_SERT_5d_t0011downsampledDLC_resnet50_downsampled_trackerSep16shuffle1_180000.csv\n",
            "rat_training_OS_SERT_5d_t0012downsampledDLC_resnet50_downsampled_trackerSep16shuffle1_180000.csv\n",
            "rat_training_OS_SERT_5d_t0013downsampledDLC_resnet50_downsampled_trackerSep16shuffle1_180000.csv\n",
            "rat_training_OS_SERT_5d_t0014downsampledDLC_resnet50_downsampled_trackerSep16shuffle1_180000.csv\n",
            "rat_training_OS_SERT_5d_t0015downsampledDLC_resnet50_downsampled_trackerSep16shuffle1_180000.csv\n",
            "rat_training_OS_SERT_5d_t0016downsampledDLC_resnet50_downsampled_trackerSep16shuffle1_180000.csv\n",
            "rat_training_OS_SERT_5d_t0017downsampledDLC_resnet50_downsampled_trackerSep16shuffle1_180000.csv\n",
            "rat_training_OS_SERT_5d_t0018downsampledDLC_resnet50_downsampled_trackerSep16shuffle1_180000.csv\n",
            "rat_training_OS_SERT_5d_t0019downsampledDLC_resnet50_downsampled_trackerSep16shuffle1_180000.csv\n",
            "rat_training_OS_SERT_5d_t0020downsampledDLC_resnet50_downsampled_trackerSep16shuffle1_180000.csv\n",
            "1.3694621642184117\n",
            "true\n",
            "0.8635527330998872\n",
            "true\n",
            "1.097076439542193\n",
            "true\n",
            "0.9384129607224009\n",
            "true\n",
            "1.3243683781581093\n",
            "true\n",
            "0.9836937256292095\n",
            "true\n",
            "1.2038523274478332\n",
            "true\n",
            "1.3429227968605448\n",
            "true\n",
            "1.946572789397391\n",
            "true\n",
            "0.16494845360824745\n",
            "false\n",
            "1.0692010692010694\n",
            "true\n",
            "1.022131098112525\n",
            "true\n",
            "0.854100854100854\n",
            "true\n",
            "0.2560057154764385\n",
            "false\n",
            "0.8945513689346706\n",
            "true\n",
            "0.7148889961389963\n",
            "true\n",
            "2.0174708818635607\n",
            "true\n",
            "0.9001465354825205\n",
            "true\n",
            "1.9880476891197933\n",
            "true\n",
            "0.6136424272967125\n",
            "true\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yy2sqc6gnUB1",
        "outputId": "01349f1d-ac85-4569-a24a-6ceb23753f7f"
      },
      "source": [
        "d_2 = pd.DataFrame.from_dict(d, orient='index')\n",
        "videos_for_manual_inspection = d_2[(d_2['likelihood'].between(2,3)) & (d_2['outlier']==True)]\n",
        "print(videos_for_manual_inspection)\n",
        "d_2.to_csv('assess_2.csv')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                                    outlier  ...                                 likelihood_results\n",
            "rat_training_OS_SERT_5d_t0001downsampledDLC_res...     True  ...  [20.995493650143384, 55.36665301106104, 75.747...\n",
            "rat_training_OS_SERT_5d_t0002downsampledDLC_res...     True  ...  [16.784378894889905, 59.45159950145409, 58.558...\n",
            "rat_training_OS_SERT_5d_t0003downsampledDLC_res...     True  ...  [18.472395184723954, 27.41801577418016, 39.684...\n",
            "rat_training_OS_SERT_5d_t0004downsampledDLC_res...     True  ...  [13.509605453418716, 43.069613716174345, 44.24...\n",
            "rat_training_OS_SERT_5d_t0005downsampledDLC_res...     True  ...  [9.963325183374083, 20.925020374898125, 26.792...\n",
            "rat_training_OS_SERT_5d_t0006downsampledDLC_res...     True  ...  [7.07196029776675, 38.13068651778329, 43.36228...\n",
            "rat_training_OS_SERT_5d_t0007downsampledDLC_res...     True  ...  [18.830628381190177, 19.662921348314608, 30.02...\n",
            "rat_training_OS_SERT_5d_t0008downsampledDLC_res...     True  ...  [24.65009400459578, 36.724462084813034, 39.064...\n",
            "rat_training_OS_SERT_5d_t0009downsampledDLC_res...     True  ...  [26.175191551045767, 31.72499482294471, 57.299...\n",
            "rat_training_OS_SERT_5d_t0011downsampledDLC_res...     True  ...  [61.62162162162163, 68.56548856548856, 84.2619...\n",
            "rat_training_OS_SERT_5d_t0012downsampledDLC_res...     True  ...  [45.14563106796117, 39.61587167581258, 74.8205...\n",
            "rat_training_OS_SERT_5d_t0013downsampledDLC_res...     True  ...  [25.614250614250615, 41.584766584766584, 63.77...\n",
            "rat_training_OS_SERT_5d_t0015downsampledDLC_res...     True  ...  [18.532574320050603, 22.51739405439595, 40.375...\n",
            "rat_training_OS_SERT_5d_t0016downsampledDLC_res...     True  ...  [12.246621621621621, 38.28125, 41.955236486486...\n",
            "rat_training_OS_SERT_5d_t0017downsampledDLC_res...     True  ...  [51.331114808652245, 50.374376039933445, 72.25...\n",
            "rat_training_OS_SERT_5d_t0018downsampledDLC_res...     True  ...  [7.682646012141511, 30.604982206405694, 71.048...\n",
            "rat_training_OS_SERT_5d_t0019downsampledDLC_res...     True  ...  [33.46647046457852, 19.76035316375867, 79.5669...\n",
            "rat_training_OS_SERT_5d_t0020downsampledDLC_res...     True  ...  [1.016808466486823, 36.812616725461716, 46.067...\n",
            "\n",
            "[18 rows x 4 columns]\n"
          ]
        }
      ]
    }
  ]
}