{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Stage_mini.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMCwIFDoabOu91HBiuWDKj9",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sannevastaveren/Donders_internship/blob/main/Stage_mini.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WhOaAKrdjJcY"
      },
      "source": [
        "## Mount Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nBNK6_20b3sS",
        "outputId": "24b993f0-ab59-4a93-d4e0-fecb6d26effb"
      },
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zEQ4aGBNjMvP"
      },
      "source": [
        "## Install Modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "-ZCeh9WLm6Wv",
        "outputId": "da33a98d-6e6a-4cc8-cb2b-99877855380c"
      },
      "source": [
        "#@title <font size=\"5\">← ឵឵<i>Upgrade FFmpeg to v4.2.2</font> { vertical-output: true }\n",
        "from IPython.display import clear_output\n",
        "import os, urllib.request\n",
        "HOME = os.path.expanduser(\"~\")\n",
        "pathDoneCMD = f'{HOME}/doneCMD.sh'\n",
        "if not os.path.exists(f\"{HOME}/.ipython/ttmg.py\"):\n",
        "    hCode = \"https://raw.githubusercontent.com/yunooooo/gcct/master/res/ttmg.py\"\n",
        "    urllib.request.urlretrieve(hCode, f\"{HOME}/.ipython/ttmg.py\")\n",
        "\n",
        "from ttmg import (\n",
        "    loadingAn,\n",
        "    textAn,\n",
        ")\n",
        "\n",
        "loadingAn(name=\"lds\")\n",
        "textAn(\"Installing Dependencies...\", ty='twg')\n",
        "os.system('pip install git+git://github.com/AWConant/jikanpy.git')\n",
        "os.system('add-apt-repository -y ppa:jonathonf/ffmpeg-4')\n",
        "os.system('apt-get update')\n",
        "os.system('apt install mediainfo')\n",
        "os.system('apt-get install ffmpeg')\n",
        "clear_output()\n",
        "print('Installation finished.')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installation finished.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZkEGmo8b5sI"
      },
      "source": [
        "#(this will take a few minutes to install all the dependences!)\n",
        "!pip install scenedetect\n",
        "!pip install moviepy\n",
        "!pip install deeplabcut\n",
        "\n",
        "# Use TensorFlow 1.x:\n",
        "%tensorflow_version 1.x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6aVBUJIGjCQA"
      },
      "source": [
        "## Open Ephys code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-7vRcMpit_h"
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Sun Aug  3 15:18:38 2014\n",
        "@author: Dan Denman and Josh Siegle\n",
        "Loads .continuous, .events, and .spikes files saved from the Open Ephys GUI\n",
        "Usage:\n",
        "    import OpenEphys\n",
        "    data = OpenEphys.load(pathToFile) # returns a dict with data, timestamps, etc.\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import scipy.signal\n",
        "import scipy.io\n",
        "import time\n",
        "import struct\n",
        "from copy import deepcopy\n",
        "\n",
        "# constants\n",
        "NUM_HEADER_BYTES = 1024\n",
        "SAMPLES_PER_RECORD = 1024\n",
        "BYTES_PER_SAMPLE = 2\n",
        "RECORD_SIZE = 4 + 8 + SAMPLES_PER_RECORD * BYTES_PER_SAMPLE + 10 # size of each continuous record in bytes\n",
        "RECORD_MARKER = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 255])\n",
        "\n",
        "# constants for pre-allocating matrices:\n",
        "MAX_NUMBER_OF_SPIKES = int(1e6)\n",
        "MAX_NUMBER_OF_RECORDS = int(1e6)\n",
        "MAX_NUMBER_OF_EVENTS = int(1e6)\n",
        "\n",
        "def load(filepath, dtype = float):\n",
        "\n",
        "    # redirects to code for individual file types\n",
        "    if 'continuous' in filepath:\n",
        "        data = loadContinuous(filepath, dtype)\n",
        "    elif 'spikes' in filepath:\n",
        "        data = loadSpikes(filepath)\n",
        "    elif 'events' in filepath:\n",
        "        data = loadEvents(filepath)\n",
        "    else:\n",
        "        raise Exception(\"Not a recognized file type. Please input a .continuous, .spikes, or .events file\")\n",
        "\n",
        "    return data\n",
        "\n",
        "def loadFolder(folderpath, dtype = float, **kwargs):\n",
        "\n",
        "    # load all continuous files in a folder\n",
        "\n",
        "    data = { }\n",
        "\n",
        "    # load all continuous files in a folder\n",
        "    if 'channels' in kwargs.keys():\n",
        "        filelist = ['100_CH'+x+'.continuous' for x in map(str,kwargs['channels'])]\n",
        "    else:\n",
        "        filelist = os.listdir(folderpath)\n",
        "\n",
        "    t0 = time.time()\n",
        "    numFiles = 0\n",
        "\n",
        "    for i, f in enumerate(filelist):\n",
        "        if '.continuous' in f:\n",
        "            data[f.replace('.continuous','')] = loadContinuous(os.path.join(folderpath, f), dtype = dtype)\n",
        "            numFiles += 1\n",
        "\n",
        "    print(''.join(('Avg. Load Time: ', str((time.time() - t0)/numFiles),' sec')))\n",
        "    print(''.join(('Total Load Time: ', str((time.time() - t0)),' sec')))\n",
        "\n",
        "    return data\n",
        "\n",
        "def loadFolderToArray(folderpath, channels = 'all', chprefix = 'CH',\n",
        "                      dtype = float, session = '0', source = '100'):\n",
        "    '''Load continuous files in specified folder to a single numpy array. By default all\n",
        "    CH continous files are loaded in numerical order, ordering can be specified with\n",
        "    optional channels argument which should be a list of channel numbers.'''\n",
        "\n",
        "    if channels == 'all':\n",
        "        channels = _get_sorted_channels(folderpath, chprefix, session, source)\n",
        "\n",
        "    if session == '0':\n",
        "        filelist = [source + '_'+chprefix + x + '.continuous' for x in map(str,channels)]\n",
        "    else:\n",
        "        filelist = [source + '_'+chprefix + x + '_' + session + '.continuous' for x in map(str,channels)]\n",
        "\n",
        "    t0 = time.time()\n",
        "    numFiles = 1\n",
        "\n",
        "    channel_1_data = loadContinuous(os.path.join(folderpath, filelist[0]), dtype)['data']\n",
        "\n",
        "    n_samples  = len(channel_1_data)\n",
        "    n_channels = len(filelist)\n",
        "\n",
        "    data_array = np.zeros([n_samples, n_channels], dtype)\n",
        "    data_array[:,0] = channel_1_data\n",
        "\n",
        "    for i, f in enumerate(filelist[1:]):\n",
        "            data_array[:, i + 1] = loadContinuous(os.path.join(folderpath, f), dtype)['data']\n",
        "            numFiles += 1\n",
        "\n",
        "    print(''.join(('Avg. Load Time: ', str((time.time() - t0)/numFiles),' sec')))\n",
        "    print(''.join(('Total Load Time: ', str((time.time() - t0)),' sec')))\n",
        "\n",
        "    return data_array\n",
        "\n",
        "def loadContinuous(filepath, dtype = float):\n",
        "\n",
        "    assert dtype in (float, np.int16), \\\n",
        "      'Invalid data type specified for loadContinous, valid types are float and np.int16'\n",
        "\n",
        "    print(\"Loading continuous data...\")\n",
        "\n",
        "    ch = { }\n",
        "\n",
        "    #read in the data\n",
        "    f = open(filepath,'rb')\n",
        "\n",
        "    fileLength = os.fstat(f.fileno()).st_size\n",
        "\n",
        "    # calculate number of samples\n",
        "    recordBytes = fileLength - NUM_HEADER_BYTES\n",
        "    if  recordBytes % RECORD_SIZE != 0:\n",
        "        raise Exception(\"File size is not consistent with a continuous file: may be corrupt\")\n",
        "    nrec = recordBytes // RECORD_SIZE\n",
        "    nsamp = nrec * SAMPLES_PER_RECORD\n",
        "    # pre-allocate samples\n",
        "    samples = np.zeros(nsamp, dtype)\n",
        "    timestamps = np.zeros(nrec)\n",
        "    recordingNumbers = np.zeros(nrec)\n",
        "    indices = np.arange(0, nsamp + 1, SAMPLES_PER_RECORD, np.dtype(np.int64))\n",
        "\n",
        "    header = readHeader(f)\n",
        "\n",
        "    recIndices = np.arange(0, nrec)\n",
        "\n",
        "    for recordNumber in recIndices:\n",
        "\n",
        "        timestamps[recordNumber] = np.fromfile(f,np.dtype('<i8'),1) # little-endian 64-bit signed integer\n",
        "        N = np.fromfile(f,np.dtype('<u2'),1)[0] # little-endian 16-bit unsigned integer\n",
        "\n",
        "        #print index\n",
        "\n",
        "        if N != SAMPLES_PER_RECORD:\n",
        "            raise Exception('Found corrupted record in block ' + str(recordNumber))\n",
        "\n",
        "        recordingNumbers[recordNumber] = (np.fromfile(f,np.dtype('>u2'),1)) # big-endian 16-bit unsigned integer\n",
        "\n",
        "        if dtype == float: # Convert data to float array and convert bits to voltage.\n",
        "            data = np.fromfile(f,np.dtype('>i2'),N) * float(header['bitVolts']) # big-endian 16-bit signed integer, multiplied by bitVolts\n",
        "        else:  # Keep data in signed 16 bit integer format.\n",
        "            data = np.fromfile(f,np.dtype('>i2'),N)  # big-endian 16-bit signed integer\n",
        "        samples[indices[recordNumber]:indices[recordNumber+1]] = data\n",
        "\n",
        "        marker = f.read(10) # dump\n",
        "\n",
        "    #print recordNumber\n",
        "    #print index\n",
        "\n",
        "    ch['header'] = header\n",
        "    ch['timestamps'] = timestamps\n",
        "    ch['data'] = samples  # OR use downsample(samples,1), to save space\n",
        "    ch['recordingNumber'] = recordingNumbers\n",
        "    f.close()\n",
        "    return ch\n",
        "\n",
        "def loadSpikes(filepath):\n",
        "\n",
        "    '''\n",
        "    Loads spike waveforms and timestamps from filepath (should be .spikes file)\n",
        "    '''\n",
        "\n",
        "    data = { }\n",
        "\n",
        "    print('loading spikes...')\n",
        "\n",
        "    f = open(filepath, 'rb')\n",
        "    header = readHeader(f)\n",
        "\n",
        "    if float(header[' version']) < 0.4:\n",
        "        raise Exception('Loader is only compatible with .spikes files with version 0.4 or higher')\n",
        "\n",
        "    data['header'] = header\n",
        "    numChannels = int(header['num_channels'])\n",
        "    numSamples = 40 # **NOT CURRENTLY WRITTEN TO HEADER**\n",
        "\n",
        "    spikes = np.zeros((MAX_NUMBER_OF_SPIKES, numSamples, numChannels))\n",
        "    timestamps = np.zeros(MAX_NUMBER_OF_SPIKES)\n",
        "    source = np.zeros(MAX_NUMBER_OF_SPIKES)\n",
        "    gain = np.zeros((MAX_NUMBER_OF_SPIKES, numChannels))\n",
        "    thresh = np.zeros((MAX_NUMBER_OF_SPIKES, numChannels))\n",
        "    sortedId = np.zeros((MAX_NUMBER_OF_SPIKES, numChannels))\n",
        "    recNum = np.zeros(MAX_NUMBER_OF_SPIKES)\n",
        "\n",
        "    currentSpike = 0\n",
        "\n",
        "    while f.tell() < os.fstat(f.fileno()).st_size:\n",
        "        eventType = np.fromfile(f, np.dtype('<u1'),1) #always equal to 4, discard\n",
        "        timestamps[currentSpike] = np.fromfile(f, np.dtype('<i8'), 1)\n",
        "        software_timestamp = np.fromfile(f, np.dtype('<i8'), 1)\n",
        "        source[currentSpike] = np.fromfile(f, np.dtype('<u2'), 1)\n",
        "        numChannels = np.fromfile(f, np.dtype('<u2'), 1)[0]\n",
        "        numSamples = np.fromfile(f, np.dtype('<u2'), 1)[0]\n",
        "        sortedId[currentSpike] = np.fromfile(f, np.dtype('<u2'),1)\n",
        "        electrodeId = np.fromfile(f, np.dtype('<u2'),1)\n",
        "        channel = np.fromfile(f, np.dtype('<u2'),1)\n",
        "        color = np.fromfile(f, np.dtype('<u1'), 3)\n",
        "        pcProj = np.fromfile(f, np.float32, 2)\n",
        "        sampleFreq = np.fromfile(f, np.dtype('<u2'),1)\n",
        "\n",
        "        waveforms = np.fromfile(f, np.dtype('<u2'), numChannels*numSamples)\n",
        "        gain[currentSpike,:] = np.fromfile(f, np.float32, numChannels)\n",
        "        thresh[currentSpike,:] = np.fromfile(f, np.dtype('<u2'), numChannels)\n",
        "        recNum[currentSpike] = np.fromfile(f, np.dtype('<u2'), 1)\n",
        "\n",
        "        waveforms_reshaped = np.reshape(waveforms, (numChannels, numSamples))\n",
        "        waveforms_reshaped = waveforms_reshaped.astype(float)\n",
        "        waveforms_uv = waveforms_reshaped\n",
        "\n",
        "        for ch in range(numChannels):\n",
        "            waveforms_uv[ch, :] -= 32768\n",
        "            waveforms_uv[ch, :] /= gain[currentSpike, ch]*1000\n",
        "\n",
        "        spikes[currentSpike] = waveforms_uv.T\n",
        "\n",
        "        currentSpike += 1\n",
        "\n",
        "    data['spikes'] = spikes[:currentSpike,:,:]\n",
        "    data['timestamps'] = timestamps[:currentSpike]\n",
        "    data['source'] = source[:currentSpike]\n",
        "    data['gain'] = gain[:currentSpike,:]\n",
        "    data['thresh'] = thresh[:currentSpike,:]\n",
        "    data['recordingNumber'] = recNum[:currentSpike]\n",
        "    data['sortedId'] = sortedId[:currentSpike]\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "def loadEvents(filepath):\n",
        "\n",
        "    data = { }\n",
        "\n",
        "    print('loading events...')\n",
        "\n",
        "    f = open(filepath,'rb')\n",
        "    header = readHeader(f)\n",
        "\n",
        "    if float(header[' version']) < 0.4:\n",
        "        raise Exception('Loader is only compatible with .events files with version 0.4 or higher')\n",
        "\n",
        "    data['header'] = header\n",
        "\n",
        "    index = -1\n",
        "\n",
        "    channel = np.zeros(MAX_NUMBER_OF_EVENTS)\n",
        "    timestamps = np.zeros(MAX_NUMBER_OF_EVENTS)\n",
        "    sampleNum = np.zeros(MAX_NUMBER_OF_EVENTS)\n",
        "    nodeId = np.zeros(MAX_NUMBER_OF_EVENTS)\n",
        "    eventType = np.zeros(MAX_NUMBER_OF_EVENTS)\n",
        "    eventId = np.zeros(MAX_NUMBER_OF_EVENTS)\n",
        "    recordingNumber = np.zeros(MAX_NUMBER_OF_EVENTS)\n",
        "\n",
        "    while f.tell() < os.fstat(f.fileno()).st_size:\n",
        "\n",
        "        index += 1\n",
        "\n",
        "        timestamps[index] = np.fromfile(f, np.dtype('<i8'), 1)\n",
        "        sampleNum[index] = np.fromfile(f, np.dtype('<i2'), 1)\n",
        "        eventType[index] = np.fromfile(f, np.dtype('<u1'), 1)\n",
        "        nodeId[index] = np.fromfile(f, np.dtype('<u1'), 1)\n",
        "        eventId[index] = np.fromfile(f, np.dtype('<u1'), 1)\n",
        "        channel[index] = np.fromfile(f, np.dtype('<u1'), 1)\n",
        "        recordingNumber[index] = np.fromfile(f, np.dtype('<u2'), 1)\n",
        "\n",
        "    data['channel'] = channel[:index]\n",
        "    data['timestamps'] = timestamps[:index]\n",
        "    data['eventType'] = eventType[:index]\n",
        "    data['nodeId'] = nodeId[:index]\n",
        "    data['eventId'] = eventId[:index]\n",
        "    data['recordingNumber'] = recordingNumber[:index]\n",
        "    data['sampleNum'] = sampleNum[:index]\n",
        "\n",
        "    return data\n",
        "\n",
        "def readHeader(f):\n",
        "    header = { }\n",
        "    h = f.read(1024).decode().replace('\\n','').replace('header.','')\n",
        "    for i,item in enumerate(h.split(';')):\n",
        "        if '=' in item:\n",
        "            header[item.split(' = ')[0]] = item.split(' = ')[1]\n",
        "    return header\n",
        "\n",
        "def downsample(trace,down):\n",
        "    downsampled = scipy.signal.resample(trace,np.shape(trace)[0]/down)\n",
        "    return downsampled\n",
        "\n",
        "def pack(folderpath,source='100',**kwargs):\n",
        "#convert single channel open ephys channels to a .dat file for compatibility with the KlustaSuite, Neuroscope and Klusters\n",
        "#should not be necessary for versions of open ephys which write data into HDF5 format.\n",
        "#loads .continuous files in the specified folder and saves a .DAT in that folder\n",
        "#optional arguments:\n",
        "#   source: string name of the source that openephys uses as the prefix. is usually 100, if the headstage is the first source added, but can specify something different\n",
        "#\n",
        "#   data: pre-loaded data to be packed into a .DAT\n",
        "#   dref: int specifying a channel # to use as a digital reference. is subtracted from all channels.\n",
        "#   order: the order in which the .continuos files are packed into the .DAT. should be a list of .continious channel numbers. length must equal total channels.\n",
        "#   suffix: appended to .DAT filename, which is openephys.DAT if no suffix provided.\n",
        "\n",
        "    #load the openephys data into memory\n",
        "    if 'data' not in kwargs.keys():\n",
        "        if 'channels' not in kwargs.keys():\n",
        "            data = loadFolder(folderpath, dtype = np.int16)\n",
        "        else:\n",
        "            data = loadFolder(folderpath, dtype = np.int16, channels=kwargs['channels'])\n",
        "    else:\n",
        "        data = kwargs['data']\n",
        "    #if specified, do the digital referencing\n",
        "    if 'dref' in kwargs.keys():\n",
        "        ref =load(os.path.join(folderpath,''.join((source,'_CH',str(kwargs['dref']),'.continuous'))))\n",
        "        for i,channel in enumerate(data.keys()):\n",
        "            data[channel]['data'] = data[channel]['data'] - ref['data']\n",
        "    #specify the order the channels are written in\n",
        "    if 'order' in kwargs.keys():\n",
        "        order = kwargs['order']\n",
        "    else:\n",
        "        order = list(data)\n",
        "    #add a suffix, if one was specified\n",
        "    if 'suffix' in kwargs.keys():\n",
        "        suffix=kwargs['suffix']\n",
        "    else:\n",
        "        suffix=''\n",
        "\n",
        "    #make a file to write the data back out into .dat format\n",
        "    outpath = os.path.join(folderpath,''.join(('openephys',suffix,'.dat')))\n",
        "    out = open(outpath,'wb')\n",
        "\n",
        "    #go through the data and write it out in the .dat format\n",
        "    #.dat format specified here: http://neuroscope.sourceforge.net/UserManual/data-files.html\n",
        "    channelOrder = []\n",
        "    print(''.join(('...saving .dat to ',outpath,'...')))\n",
        "    random_datakey = next(iter(data))\n",
        "    bar = ProgressBar(len(data[random_datakey]['data']))\n",
        "    for i in range(len(data[random_datakey]['data'])):\n",
        "        for j in range(len(order)):\n",
        "            if source in random_datakey:\n",
        "                ch = data[order[j]]['data']\n",
        "            else:\n",
        "                ch = data[''.join(('CH',str(order[j]).replace('CH','')))]['data']\n",
        "            out.write(struct.pack('h',ch[i]))#signed 16-bit integer\n",
        "            #figure out which order this thing packed the channels in. only do this once.\n",
        "            if i == 0:\n",
        "                channelOrder.append(order[j])\n",
        "        #update how mucb we have list\n",
        "        if i%(len(data[random_datakey]['data'])/100)==0:\n",
        "            bar.animate(i)\n",
        "    out.close()\n",
        "    print(''.join(('order: ',str(channelOrder))))\n",
        "    print(''.join(('.dat saved to ',outpath)))\n",
        "\n",
        "#**********************************************************\n",
        "# progress bar class used to show progress of pack()\n",
        "    #stolen from some post on stack overflow\n",
        "import sys\n",
        "try:\n",
        "    from IPython.display import clear_output\n",
        "    have_ipython = True\n",
        "except ImportError:\n",
        "    have_ipython = False\n",
        "class ProgressBar:\n",
        "    def __init__(self, iterations):\n",
        "        self.iterations = iterations\n",
        "        self.prog_bar = '[]'\n",
        "        self.fill_char = '*'\n",
        "        self.width = 40\n",
        "        self.__update_amount(0)\n",
        "        if have_ipython:\n",
        "            self.animate = self.animate_ipython\n",
        "        else:\n",
        "            self.animate = self.animate_noipython\n",
        "\n",
        "    def animate_ipython(self, iter):\n",
        "        print('\\r', self,)\n",
        "        sys.stdout.flush()\n",
        "        self.update_iteration(iter + 1)\n",
        "\n",
        "    def update_iteration(self, elapsed_iter):\n",
        "        self.__update_amount((elapsed_iter / float(self.iterations)) * 100.0)\n",
        "        self.prog_bar += '  %d of %s complete' % (elapsed_iter, self.iterations)\n",
        "\n",
        "    def __update_amount(self, new_amount):\n",
        "        percent_done = int(round((new_amount / 100.0) * 100.0))\n",
        "        all_full = self.width - 2\n",
        "        num_hashes = int(round((percent_done / 100.0) * all_full))\n",
        "        self.prog_bar = '[' + self.fill_char * num_hashes + ' ' * (all_full - num_hashes) + ']'\n",
        "        pct_place = (len(self.prog_bar) // 2) - len(str(percent_done))\n",
        "        pct_string = '%d%%' % percent_done\n",
        "        self.prog_bar = self.prog_bar[0:pct_place] + \\\n",
        "            (pct_string + self.prog_bar[pct_place + len(pct_string):])\n",
        "\n",
        "    def __str__(self):\n",
        "        return str(self.prog_bar)\n",
        "#*************************************************************\n",
        "\n",
        "def pack_2(folderpath, filename = '', channels = 'all', chprefix = 'CH',\n",
        "           dref = None, session = '0', source = '100'):\n",
        "\n",
        "    '''Alternative version of pack which uses numpy's tofile function to write data.\n",
        "    pack_2 is much faster than pack and avoids quantization noise incurred in pack due\n",
        "    to conversion of data to float voltages during loadContinous followed by rounding\n",
        "    back to integers for packing.\n",
        "    filename: Name of the output file. By default, it follows the same layout of continuous files,\n",
        "              but without the channel number, for example, '100_CHs_3.dat' or '100_ADCs.dat'.\n",
        "    channels:  List of channel numbers specifying order in which channels are packed. By default\n",
        "               all CH continous files are packed in numerical order.\n",
        "    chprefix:  String name that defines if channels from headstage, auxiliary or ADC inputs\n",
        "               will be loaded.\n",
        "    dref:  Digital referencing - either supply a channel number or 'ave' to reference to the\n",
        "           average of packed channels.\n",
        "    source: String name of the source that openephys uses as the prefix. It is usually 100,\n",
        "            if the headstage is the first source added, but can specify something different.\n",
        "    '''\n",
        "\n",
        "    data_array = loadFolderToArray(folderpath, channels, chprefix, np.int16, session, source)\n",
        "\n",
        "    if dref:\n",
        "        if dref == 'ave':\n",
        "            print('Digital referencing to average of all channels.')\n",
        "            reference = np.mean(data_array,1)\n",
        "        else:\n",
        "            print('Digital referencing to channel ' + str(dref))\n",
        "            if channels == 'all':\n",
        "                channels = _get_sorted_channels(folderpath, chprefix, session, source)\n",
        "            reference = deepcopy(data_array[:,channels.index(dref)])\n",
        "        for i in range(data_array.shape[1]):\n",
        "            data_array[:,i] = data_array[:,i] - reference\n",
        "\n",
        "    if session == '0': session = ''\n",
        "    else: session = '_'+session\n",
        "\n",
        "    if not filename: filename = source + '_' + chprefix + 's' + session + '.dat'\n",
        "    print('Packing data to file: ' + filename)\n",
        "    data_array.tofile(os.path.join(folderpath,filename))\n",
        "\n",
        "\n",
        "def _get_sorted_channels(folderpath, chprefix='CH', session='0', source='100'):\n",
        "    Files = [f for f in os.listdir(folderpath) if '.continuous' in f\n",
        "                                               and '_'+chprefix in f\n",
        "                                               and source in f]\n",
        "\n",
        "    if session == '0':\n",
        "        Files = [f for f in Files if len(f.split('_')) == 2]\n",
        "        Chs = sorted([int(f.split('_'+chprefix)[1].split('.')[0]) for f in Files])\n",
        "    else:\n",
        "        Files = [f for f in Files if len(f.split('_')) == 3\n",
        "                                  and f.split('.')[0].split('_')[2] == session]\n",
        "\n",
        "        Chs = sorted([int(f.split('_'+chprefix)[1].split('_')[0]) for f in Files])\n",
        "    return(Chs)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QA8hTuFxjwgT"
      },
      "source": [
        "## Synchronization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2XL3jDVIjSSL"
      },
      "source": [
        "### Synchronization Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aH0YT3Lq6fvd"
      },
      "source": [
        "from moviepy.video.io.ffmpeg_tools import ffmpeg_extract_subclip\n",
        "import scenedetect.frame_timecode as sce\n",
        "import os, sys, re\n",
        "\n",
        "def frames_to_timecode(frames,framerate):\n",
        "    return'{0:02d}:{1:02d}:{2:02d}:{3:02d}'.format(int(frames / (3600*framerate)),\n",
        "                                                    int(frames / (60*framerate) % 60),\n",
        "                                                    int(frames / framerate % 60),\n",
        "                                                    int(frames % framerate))\n",
        "\n",
        "    return b\n",
        "\n",
        "\n",
        "def get_seconds(b_list):\n",
        "    times_list = []\n",
        "    for l in b_list:\n",
        "        s_list = []\n",
        "        for t in l:\n",
        "            hh, mm, ss, ms = t.split(':')\n",
        "            s = int(hh) * 3600 + int(mm) * 60 + int(ss)\n",
        "            s_list.append(s)\n",
        "        time = str(s_list[0]) + '-' + str(s_list[1])\n",
        "        times_list.append(time)\n",
        "    print(times_list)\n",
        "    return times_list\n",
        "  \n",
        "def load_data_into_dataframe(data):\n",
        "   samples = np.array(data['data'])\n",
        "   timestamps = np.array(data['timestamps'])\n",
        "   DF_samples = pd.DataFrame(samples)\n",
        "   DF_timestamps= pd.DataFrame(timestamps)\n",
        "   return DF_samples, DF_timestamps\n",
        "\n",
        "\n",
        "def transfer_frames(times, start_time):\n",
        "    x = times - start_time\n",
        "    return x\n",
        "def add_delay(times, start_time):\n",
        "    x = times + start_time\n",
        "    return x\n",
        "def cut_video(time_list, video_file):\n",
        "    for time in times:\n",
        "      starttime = int(time.split(\"-\")[0])\n",
        "      endtime = int(time.split(\"-\")[1])\n",
        "      x = sce.FrameTimecode(timecode = float(starttime), fps = 30.0)\n",
        "      y = sce.FrameTimecode(timecode = float(endtime), fps = 30.0)\n",
        "      print(str(x.get_frames()) + '-' + str(y.get_frames()))\n",
        "      ffmpeg_extract_subclip(required_video_file, starttime, endtime, targetname=str(times.index(time)+1)+\".avi\")\n",
        "\n",
        "\n",
        "def converttime(time):\n",
        "      #offset = time & 0xFFF\n",
        "      time = time.to_numpy()\n",
        "      cycle1 = (time >> 12) & 0x1FFF\n",
        "      cycle2 = (time >> 25) & 0x7F\n",
        "      seconds = cycle2 + cycle1 / 8000.\n",
        "      return seconds\n",
        "def uncycle(time):\n",
        "      cycles = np.insert(np.diff(time) < 0, 0, False)\n",
        "      cycleindex = np.cumsum(cycles)\n",
        "      return time + cycleindex * 128\n",
        "\n",
        "def convert_and_uncycle_timestamps(df):\n",
        "    df = df.astype({col: 'int32' for col in df.select_dtypes('int64').columns})\n",
        "    modDfObj = df.apply(converttime)\n",
        "    uncycledDF = modDfObj.apply(uncycle)\n",
        "    return uncycledDF\n",
        "\n",
        "def dividething(n):\n",
        "    n = n/30000\n",
        "    return n\n",
        "\n",
        "def calculate_pixel_size(dpi):\n",
        "  #dpi is the pixel density or dots per inch.\n",
        "  #96 dpi means there are 96 pixels per inch.\n",
        "  #1 inch is equal to 25.4 millimeters.\n",
        "  inch = 25.4\n",
        "  pixel_size= dpi/inch\n",
        "  return pixel_size\n",
        "\n",
        "def cut(video_file_path, start_time, end_time):\n",
        "\n",
        "  output_file_path = re.search(\"^[\\/].+\\/\", video_file_path)\n",
        "  output_file_path_raw = output_file_path.group(0)\n",
        "  delsplit = re.search(\"\\/(?:.(?!\\/))+$\", video_file_path)\n",
        "  filename = re.sub(\"^[\\/]\", \"\", delsplit.group(0))\n",
        "  filename_raw = re.sub(\".{4}$\", \"\", filename)\n",
        "  file_extension = re.search(\".{3}$\", filename)\n",
        "  file_extension_raw = file_extension.group(0)\n",
        "\n",
        "  os.environ['inputFile'] = video_file_path\n",
        "  os.environ['outputPath'] = output_file_path_raw\n",
        "  os.environ['startTime'] = start_time\n",
        "  os.environ['endTime'] = end_time\n",
        "  os.environ['fileName'] = filename_raw\n",
        "  os.environ['fileExtension'] = file_extension_raw\n",
        "\n",
        "  !ffmpeg -hide_banner -i \"$inputFile\" -ss \"$startTime\" -to \"$endTime\" -c copy \"$outputPath\"/\"$fileName\"-TRIM.\"$fileExtension\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0D-OOSEfjo1A"
      },
      "source": [
        "### Synchronization Code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OW2uiBAaRanh"
      },
      "source": [
        "### LED synch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 993
        },
        "id": "btlLPdVfRZd0",
        "outputId": "b866f97e-8ea9-49ce-f96f-bacd947e3c6f"
      },
      "source": [
        "import pandas as pd\n",
        "import time\n",
        "from moviepy.video.io.ffmpeg_tools import ffmpeg_extract_subclip\n",
        "import scenedetect.frame_timecode as sce\n",
        "import deeplabcut\n",
        "import os\n",
        "\n",
        "\n",
        "def main1():\n",
        "  # DLC variables\n",
        "  %matplotlib inline\n",
        "  ProjectFolderName = 'Stage/sk_test-sanne-2021-09-08'\n",
        "  VideoType = 'avi' \n",
        "  # video path \n",
        "  videofile_path = '/content/drive/My Drive/'+ProjectFolderName+'/videos/'\n",
        "  videofile_path\n",
        "  # config file\n",
        "  path_config_file = '/content/drive/My Drive/'+ProjectFolderName+'/config.yaml'\n",
        "  path_config_file\n",
        "\n",
        "  # make file variables\n",
        "  VIDEO_file = 'Full Videos/PointGreyVideo2020-05-04T10_34_08.avi'\n",
        "  file_path = '/content/drive/My Drive/Stage/ADC_trial1/'\n",
        "  ADC_file = '100_ADC1_0.continuous'\n",
        "  LED_file = 'PointGreyLEDStatus2020-05-04T10_34_07.csv'\n",
        "\n",
        "\n",
        "\n",
        "  # load in data as dataframes\n",
        "  data = load(file_path+ADC_file)\n",
        "  DF_ADC_samples, DF_ADC_timestamps = load_data_into_dataframe(data)\n",
        "  DF_p_led = pd.read_csv(file_path+LED_file, header=None)\n",
        "\n",
        "  DF_ADC_samples.rename(columns={0 : 'value'}, inplace=True)\n",
        "  DF_ADC_samples.iloc[0:10000].plot.line()\n",
        "  DF_ADC_samples['state'] = 0\n",
        "  DF_ADC_samples.loc[DF_ADC_samples.value.between(0,1), 'state'] = 0\n",
        "  DF_ADC_samples.loc[DF_ADC_samples.value.between(1,5), 'state'] = 1\n",
        "  DF_ADC_samples.iloc[0:10000].plot.line(subplots=True)\n",
        "  group_size = 1000\n",
        "  lst = [DF_ADC_samples.iloc[i:i+group_size] for i in range(0,len(DF_ADC_samples)-group_size+1,group_size)]\n",
        "\n",
        "  bla = []\n",
        "  for df in lst:\n",
        "    m = df[\"state\"].mean()\n",
        "    bla.append(m)\n",
        "\n",
        "  blabla = pd.DataFrame(bla)\n",
        "  blabla.rename(columns={0: 'value'}, inplace=True)\n",
        "  blabla['state'] = 0\n",
        "  blabla.iloc[0:100].plot.line(subplots=True)\n",
        "  blabla.loc[blabla['value'] <= 0, 'state'] = 0\n",
        "  blabla.loc[blabla['value'] > 0, 'state'] = 1\n",
        "\n",
        "  DF_p_led.rename(columns={0: 'value'}, inplace=True)\n",
        "  DF_p_led['state'] = 0\n",
        "  DF_p_led.loc[DF_p_led['value'] <= 500, 'state'] = 0\n",
        "  DF_p_led.loc[DF_p_led['value'] > 500, 'state'] = 1\n",
        "  DF_p_led.iloc[0:100].plot.line(subplots=True)\n",
        "\n",
        "  trial = blabla.state.astype(str).str.cat()\n",
        "  whole = DF_p_led['state'].astype(str).str.cat()\n",
        "\n",
        "  \n",
        "  #blabla.iloc[0:100].plot.line()\n",
        "  #DF_p_led.iloc[0:100].plot.line()\n",
        "  return whole, trial\n",
        "whole,trial = main1()\n",
        "\n",
        "\n",
        "\n",
        "  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading continuous data...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAADrCAYAAABkdpGvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3de3hc9X3n8fdX1/FFMmD5BsbIDRDAXGzQEihJKUsI0CSwTeIaQtqQduvNBSjJszw1T9JACJsm2bRN2NL0gYRCuimXdbbBSSGETcjSJpBg0xBis05cKCAbXSyDZmRrRhrpu3+cM5KQZWxpzsWa83k9jx5rZo7O+R2fM+f7u//M3RERkeyqSzsBIiKSLgUCEZGMUyAQEck4BQIRkYxTIBARyTgFAhGRjGuIc+dmdhfwLqDH3U+d4nMDvgL8DrAPuNrdnz7Yftva2ry9vT3i1IqI1LYtW7bsdvdFk9+PNRAAdwN/DXzjAJ9fCpwQ/rwF+Gr47xtqb29n8+bNESVRRCQbzOzFqd6PtWrI3R8H9rzBJpcD3/DAk8ARZrYszjSJiMjrxV0iOJhjgJcnvO4M33slneRUp6dQ5I/v2cxAqRz5vt95+tF84qITI9/vdNz4v5/lZy/0Rb7fhfOb+cYfnk2usT7yfQv0DZT4w3s2M1Acjnzfl5y6lBsuPiny/Urg+1u7+O+PbGd0wgwQD/3J22huiPa7knYgOGRmth5YD7BixYqUUzO1514p8ExnP+cdv5Aj5jZFtt+fv/Qa33lmV+qB4LvP7GJRSzMnH90a2T578yV+9sIenu/dyykR7lfG/b+uAs+8/Bq/+aaFHDkvuvvymZdf4zvPvKJAEKPHf93Li3v2cdEpS8beMyzy46QdCHYCx054vTx8bz/ufgdwB0BHR8dhOUFSIcxx3fTuVZy4pCWy/X72u9u492cv4e4E7evJGx11BobKfOj0dj7xjjdHtt+nX3qV9/zNT+jOFxUIYlK5Lz/1zlMi/T/+3EPPcc9P/j3V+7LWFYpllrbmuP39Z8Z6nLS7j24C/sAC5wD97j4rq4UguGgALblo4+uS1mb2DY1QiKHK6VANDJVxh5ZcY6T7XdKaA6ArX4x0vzIuPxjcN61zor4vc5TKo2P7l+jlB4cjv25Tibv76L3AbwNtZtYJ3AQ0Arj73wIPEXQd3UHQffRDcaYnbpWcV2tMD8uefDHyfR+q/GB4bhHflItbmgHoViCITT68L6MP4uG1KxRZMDed+7LWFYrlRL7zsQYCd7/yIJ878LE405Ck/GCZ+jpjblO0DTljueb+Escvjq7KaTrGSzvR3pSN9XW0zW9SIIhRoVjGDFqaoy8RAHT1FyOtCpVx+eIwK9vmxX6ctKuGakqhOMz85obI60uXHgbVJ3FVe0HwQOnOlyLfrwTyxWHmNzVQVxftfbmkJbgvFcTjk1SJQIEgQoViObYHJaT7hRurGorhplzSmqOrXw+TuMR1Xy4Oq4Z6CgricckPDkdeCp+KAkGE8sXhWB6Uc5rqac01pBoICqVKPXM8ga6noEAQl6DBMfr7MtdYz4I5jSoRxKQ8MsreoZFEGosVCCKUjynnBbB0Qbq55krVUBwPlCWtzeweGGKoPBr5viW+EgEE106BIB6VgamqGpplgi9cPBdtSWuO7hSL4JWqoTgeKJU2kN4BVTHEIa6SKqh9J06VbrlxBfGJFAgiFGef3yWtObpTLhE0NdRFPrQdXt/7RKIXb4kgR49KBLGodPuNoxQ+mQJBhAox5ryWtuboHSgxMprOoOp8jL0XJo6TkOjli/G0EUBQNdRTKDGa0n1Zy8bHf6hEMGu4OwOleOtiR0advpSqT4IgF9+5gUYXx8HdYy8RlEedvr1Dsew/y8ba5dRGMHvsHRph1OOL3mlPxRBnQ/hR85porDfVNcdg39AII6Me28NkscYSxKbSLrdAVUOzR5z97CHoNQSk9rAsxFi9YGYsbsnpYRKDuEaEVywZG0ugaxe1fIyDOCdTIIhI/F+4dEsEcVYvQBDoFAiiN97gGG9JVaW56FXmLpsf8dQgU1EgiEgh5oadtvnN1NdZaj2H8oPDtDTHV0Rd0tqsNoIYFGKacK5iUUszZqoaikN+sMy8pnoa6uN/TCsQRCTuFv76OmPR/PQG7xSK5VhHOAbdEJWrjFrcfdEb6+tYOK9ZJYIYxFkdO5kCQUTiHHlbkVaueXhklMHhkVjnPFnSmmOgVI5lmc8sy8c0NfpES1qb1fU3BnEOBJxMgSAiSTTsBKM4k//CxTnzaMXSw2BivVo03gUx5vtSjcWRi7tdbiIFgojE3WsI0hvOH9eCOxNVZrJMc/R0LUpidOqS1ma6+lU1FLU4BwJOpkAQkUKxTFN9Hc0N8f2XLl2Qo39wmOLwSGzHmEqiJQLlLCNVKJZprLdY78vFLTn69pYYHtGkgVHKD6pEMOsUisO05KJflGaitNYlGJ9wLt7SDqCcZcTyg0E9c9z3pTvs1qSBkYpzyprJFAgikkR93thUDAlXn+SL8Sx+PtG85gZamtNdc6EWJXFfLl1QWXdagSAq7h7M75XAWgSgQBCZfDH+lYTSWrIyiTYCCNoJFAiilUQ9s6aZiN7gcDA1SBKrk4ECQWTi7mcPsHhsls5kc15JDXXX6OLoJVNS1eyxUauM/1DV0CxTKMY78haCLoBzGutTKxHEPdR9SYsWOYlapY0gTgvnNQWj3nXtIhP3TAWTKRBEJIkWfjMLlqxMPBAkM9R9yYJg7WLNbR+dJEoEdXXG4hZV60UpyUVpQIEgMkkNB1/ckvwozrgWP59sSUszwyPOnn2a2z4qSbRdQVBtqbmiojNeNaQSwawxMursHRpJpBi3pDVHT8JrFyc1wjGt7rG1qjwyyr6hkUTqmZeoRBCpfMyTBU6mQBCBgZinoJ6odU7DWL/+pBRKyeQqF8wNjlHJDUl1khgIWLFwfjOv7kv2vqxlSXTZnkiBIAJJri3ammukUCzjnlw9en6wnEgRtZJzrfx/SnWSmAixojWXfAalliXVZbtCgSACSczwWNGSa6Q86gwmOM1EIaF65sr/X+UBJtVJNIMyp5FSeZRSOdnpT2pVfjD+KWsmUiCIQBIzPFZUvtRJPiyTaiOoHEM5y2gkm0FJ/r6sZUHnk3inrJlIgSACSczFU1Ep5hcSqj4Jhron02tovh4mkUqyjUCluWjli+XEGopBgSAShQQbdipf6v6EGlRL5VGGRzyRh0ljfR1zm+oTC3K1rpJBWZBAEFdpLlrBQMBkGopBgSASca8LO1HrWK45mS9c0t3YWnINaiyOSKIlgjkqEUQpqXa5ilgDgZldYmbbzWyHmW2Y4vOrzazXzH4e/vznONMTl6Tm4oHki+BJD2yp9IqS6uUTmhoEJpQIFMQjkeTMowCxHcnM6oHbgYuATuApM9vk7tsmbXq/u18TVzqSUCgOk2usozHmKRhgPGee1Bcu6W5sLbkGBYKIJDU1CEzMoCgQRCHJtQgg3hLB2cAOd3/e3YeA+4DLYzxeagrFcqIPysoxk5Bk9UJwnEblKiOSH0yuemG8jUBBPApJrk4G8QaCY4CXJ7zuDN+b7L1m9gsz22hmxx5oZ2a23sw2m9nm3t7eqNNalSQXmZ7bVE99nSXeRpDU5Fetc1Q1FJUkpkavmNfUQJ2pRBCF4ZFRBoeTmRqkIu3G4u8A7e5+OvAocM+BNnT3O9y9w907Fi1alFgCD0VSE3tBMANpktUnyZcIGvQwiUiS92VdnTG/uWGsvUxmLunvHMQbCHYCE3P4y8P3xrh7n7tXZlD7GnBWjOmJTdCwk1z0bklwOH+SPaKC4zSoeiEiQZVlcg8TVetFo/LdTvKZEmcgeAo4wcxWmlkTcAWwaeIGZrZswsvLgOdiTE9sKgvXJyXJnjX5wTJ1BvOa6hM5XmuukaGRUYoJTqFRq5IsEUDw4FIQr974TAXJXbvYnl7uXjaza4BHgHrgLnffama3AJvdfRNwnZldBpSBPcDVcaUnTklNylaRbNVQ8DBJaqh764TG8FxjMsGnViXZRgCq1otKknNEVcR6JHd/CHho0nufnvD7jcCNcaYhCUkP/mjJNfLynn2JHCvJhnB4fffYRS3NiR231rh7or2GIMjB7nxtMLHj1apCwh00IP3G4llvqDxKqTxasyWCfML9mSs5WPUcqk5xeJTyqCd77VQiiESleq1WGoszIenGVAhyXkk1yuVTKhHogVKdNKoXgjYCXbdqJd1lGxQIqpbk9BIVrbkGBkrlRBZ5LyTcI2pscRo1OlYljeqFlgTvy1qWL5Yxg/lNKhHMGklPwQBBrtkdBobif1gG9czJVnuBSgTVSieD0siow94E7stalh8cZn5zA3V1yXTQAAWCqqUx+CPJevSk5zzR5GXRGOuLnkoQVyCoRn5wOJGpwydSIKjSa+GC3ZWF15Mw1rMm5vrY4ZFRCqVyojfl+FQFephUY/fAEABt85PreVWphlIQr05PocTihHvMKRBUqStfBGBJSy6xYyaV8+otlHCHpQuSO7fKVAUKBNXpDu/LxTV4X9a67nyRJa3JXTdQIKhaT75IU0MdRyRYIkhqyt/Kw2RJa7K5k5acep9UqztfpDXXwJyERoTDxIZ+XbtqKBDMQl35IktamxMbeQvJ1aOPB4Jkb8rWOY2avKxKaTxMVCKo3uDQCPlimcUJZ74UCKrU1V9kaeJfuGRWKevOB/MBpvFAUa+h6nTnS4lW6YHaCKLQU0i+qhkUCKrWUyixuEZzXl35Io31xlFzm2I9zmTBgDnlKqvRky8m2j4AKhFEoZL5UolgFnH3VEoEucZ6mhrqYq+L7Q4fJkn2ZwZNVVCt0VGnp1BKvG2nuaGe5gTuy1qWVnWsAkEV8sUyg8MjiQcCCB6Wceeau8P2j6Rp3eLq9O0dojzqiT9MoLImga7dTHWn0AsRFAiqMnbREq6LhcqaBPHmvLr6i4nXM0Nlucph3DVVwUyk1dsLKhkUlQhmqqdQormhLtHpw0GBoCrj0TudXHPcOa+efCnxemYIzi2YqkCL08zEWINjGiUCrTldlZ6wt1eSvRBBgaAqXf3BFy6NXHNLzCWCvaUyhVI5tXMDzTc0U2n19oKwRKA2ghnrzifftgMxL0xT69Jq2IFgvqHKqOY4pFu9MD4D6bIF0/vb4eFhOjs7KRbj+785XOVyOZYvXz6WQUljYZ/WXCO7tDjNjHUXipy8rDXx4yoQVKErX+SIuY2pLKnY0hzv6NuuFINcNTOQdnZ20tLSQnt7e+LF6zS5O319fXR2dtJTKNI2v4nG+uQL/ElUWdaynnyJ809MPoCraqgK3flS4q37FXH3rEmztFPNyOliscjChQszFQQAzIyFCxdSLBbpTqltB8Yb+mX6BkplBkrlVL5zCgRV6M4XU+kxBMEXbnB4hOGR0Vj2X6lnTqVr7JzqRk5nLQhUVM67O59Oby+AluYGisOjDJXjuS9rWU+K1bEKBFUIBpOls8B63KM4u/qLtDQ3MK85+drD8RJBNqoY5s+fH+n+0mpwhIlBXKWC6Rpr5E+hNKdAMEPlkVF2D5RSyTFD/D1regrFxIe5V2gWy5lzd/r2plc1lLUgHqVKt9+kp6wBBYIZ2z0wxKinc9FgfOWpOEsEaVUv5Brraaqvm7X90Tds2MDtt98+9vrmm2/m1ltv5cILL+TMM8/ktNNO48EHH9zv7370ox/xrne9a+z1Nddcw9133w3Ali1bOP/88znrrLO4+OKLeeWVV6Y89qiDezptO5DcFOm1KM2eeuo1NEOVXjVplwjiyjV350u8ZeVRsez7UEQxA+lnvrOVbbvyEaUocMrRrdz07lVvuM26deu4/vrr+djHPgbAAw88wCOPPMJ1111Ha2sru3fv5pxzzuGyyy47pPaM4eFhrr32Wh588EEWLVrE/fffzyc/+Unuuuuu/bYdCReOT6tqaKxEMDg7g3iaevIl5jTWMz+F6lgFghlKczAZxFsEDyYtS68hHGb3mgRr1qyhp6eHXbt20dvby5FHHsnSpUv5+Mc/zuOPP05dXR07d+6ku7ubpUuXHnR/27dv55e//CUXXXQRACMjIyxbtmzKbccDQXqdGEAlgpnoDicKTKOzgwLBDKU5jB/iLYLv2TfE8IinMnVGRRQlgoPl3OO0du1aNm7cSFdXF+vWreOb3/wmvb29bNmyhcbGRtrb2/cb9NbQ0MDo6Hhvm8rn7s6qVat44oknDnrcEU83ECS1aFIt6s4XU6tqVhvBDHX1F2moMxbOS3au/orKpFRx5JordZVplXZg9s9Aum7dOu677z42btzI2rVr6e/vZ/HixTQ2NvLYY4/x4osv7vc3xx13HNu2baNUKvHaa6/xgx/8AIA3v/nN9Pb2jgWC4eFhtm7dOuVxR0ad+lTvy2QWTapFPSmsKlehEsEMdeWLLG5pTnyu/opKPWIcJYKxhc9TuikhKPH05AdSO361Vq1aRaFQ4JhjjmHZsmVcddVVvPvd7+a0006jo6ODk046ab+/OfbYY/m93/s9Tj31VFauXMmaNWsAaGpqYuPGjVx33XX09/dTLpe5/vrrWbVq/xLP6Kine182NWCmHl/T5e5050u8PaVSuALBDKU5mAygob6OuU31seS80hxMVjHbSwQAzz777NjvbW1tB6zaGRgYD3hf/OIX+eIXv7jfNqtXr+bxxx8/6DFH3FMN4HV1xvxmTTMxXYVSsLZJWiUCVQ3NUJrTS1S05uKZb6irv4hZOpOWVSSx3kItGhn11AY5VgRLjeraTUfPWCk8nWunQDBD3Sn2s6+IK9fcUyiycF5zKpOWVbTkGtk7NEI5pik0atVISiuTTVQLpbmkja1VnFLmMvZvupldYmbbzWyHmW2Y4vNmM7s//PynZtYed5qqVZmr/7D4wpXiKREsXZBurrLS+2SgpAfKoRoddUZTHExWEVdJtZalOZgMYg4EZlYP3A5cCpwCXGlmp0za7I+AV939eOCvgC/EmaYojPeqSbkIPqcxloE7XYdDtVcVvU+yusTl0MgITtBYnCaVCKavpxCWCGq0jeBsYIe7P+/uQ8B9wOWTtrkcuCf8fSNwoR3m00d2pbTA9GRxrVLWk3JDOIyXCPqnmbPM5XL09fVlLhi4O7t39/Hia8PplwjmqI1gurrzReY3N6Qyqhji7zV0DPDyhNedwFsOtI27l82sH1gI7I46MWv/9ic837u36v1UpthN+2HZmmvgxT37OOuzj0a63769Q6kHucqAufff+eS02ipamowPrW5leetLHNa5iYg58FJ/mf/x01fZeFba166Bna8NRn5f1rJCqczyI+ekdvxZ033UzNYD6wFWrFgxo3287YRFvHlpSyTpaZvfzG+0zYtkXzN11VuOo84MJ9rcb0NdHe8585hI9zlda1YcwYfPfxMDM2gD+fVg8JM9jVz91jdx/KJop7WerivfsgIHRjNWKqvWW49vS+3YFmcR2szOBW5294vD1zcCuPufT9jmkXCbJ8ysAegCFvkbJKyjo8M3b94cW7pFRGqRmW1x947J78fdRvAUcIKZrTSzJuAKYNOkbTYBHwx/fx/wwzcKAiIiEq1Yq4bCOv9rgEeAeuAud99qZrcAm919E/B14O/NbAewhyBYiIhIQmKtGoqLmfUC+8/adWjaiKEh+jCXxXOGbJ53Fs8ZsnneMznn49x90eQ3Z2UgqIaZbZ6qjqyWZfGcIZvnncVzhmyed5TnrCkmREQyToFARCTjshgI7kg7ASnI4jlDNs87i+cM2TzvyM45c20EIiLyelksEYiIyAQKBCIiGadAICKScQoEIiIZp0AgIpJxCgQiIhmnQCAiknEKBCIiGadAICKScQoEIiIZp0AgIpJxCgQiIhkXyVKVZnYJ8BWC5Si/5u6fn/R5M/AN4CygD1jn7v9uZu3Ac8D2cNMn3f3DBzteW1ubt7e3R5F0EZHM2LJly+6pViirOhCYWT1wO3AR0Ak8ZWab3H3bhM3+CHjV3Y83syuALwDrws/+zd1XT+eY7e3tbN68udqki4hkiplNucRvFFVDZwM73P15dx8C7gMun7TN5cA94e8bgQvNzCI4toiIVCmKQHAM8PKE153he1Nu4+5loB9YGH620sz+1cz+r5m9LYL0iIjINETSRlCFV4AV7t5nZmcB3zazVe6en7yhma0H1gOsWLEi4WSKiNSuKALBTuDYCa+Xh+9NtU2nmTUAC4A+D5ZHKwG4+xYz+zfgRGC/BgB3v4NwabaOjg4tqyYih2R4eJjOzk6KxWLaSUlMLpdj+fLlNDY2HtL2UQSCp4ATzGwlwQP/CuD9k7bZBHwQeAJ4H/BDd3czWwTscfcRM/sN4ATg+QjSJCICQGdnJy0tLbS3t5OFpkl3p6+vj87OTlauXHlIf1N1G0FY538N8AhBV9AH3H2rmd1iZpeFm30dWGhmO4BPABvC938L+IWZ/ZygEfnD7r6n2jSJiFQUi0UWLlyYiSAAYGYsXLhwWiWgSNoI3P0h4KFJ7316wu9FYO0Uf/ct4FtRpEFE5ECyEgQqpnu+GlksIpKCL3/5y+zbty+y7aqhQCAikgIFAhGRDNm7dy/vfOc7OeOMMzj11FP5zGc+w65du7jgggu44IILAPjIRz5CR0cHq1at4qabbgLgtttu22+773//+5x77rmceeaZrF27loGBgarTZ0EPztmlo6PDNcWEiByK5557jpNPPjl48fAG6Ho22gMsPQ0u/fwbbvKtb32L733ve9x5550A9Pf3c8YZZ7B582ba2toA2LNnD0cddRQjIyNceOGF3HbbbZx++uljU+q0tbWxe/du3vOe9/Dwww8zb948vvCFL1Aqlfj0pz+93zFfd94hM9vi7h2Tt1WJQEQkZqeddhqPPvoof/qnf8o///M/s2DBgv22eeCBBzjzzDNZs2YNW7duZdu2bftt8+STT7Jt2zbOO+88Vq9ezT333MOLL045fdC0pD2yWEQkOQfJucflxBNP5Omnn+ahhx7iU5/6FBdeeOHrPn/hhRf40pe+xFNPPcWRRx7J1VdfPWX3T3fnoosu4t577400fSoRiIjEbNeuXcydO5cPfOAD3HDDDTz99NO0tLRQKBQAyOfzzJs3jwULFtDd3c3DDz889rcTtzvnnHP48Y9/zI4dO4Cg7eFXv/pV1elTiUBEJGbPPvssN9xwA3V1dTQ2NvLVr36VJ554gksuuYSjjz6axx57jDVr1nDSSSdx7LHHct5554397fr161+33d13382VV15JqVQC4NZbb+XEE0+sKn1qLBaRmjZVo2kWqLFYREQOmQKBiEjGKRCIiGScAoGI1LzZ2BZajemerwKBiNS0XC5HX19fZoJBZT2CXC53yH+j7qMiUtOWL19OZ2cnvb29aSclMZUVyg6VAoGI1LTGxsZDXqkrq1Q1JCKScQoEIiIZp0AgIpJxCgQiIhmnQCAiknEKBCIiGadAICKScQoEIiIZp0AgIpJxCgQiIhmnQCAiknEKBCIiGadAICKScQoEIiIZp0AgIpJxCgQiIhkXSSAws0vMbLuZ7TCzDVN83mxm94ef/9TM2id8dmP4/nYzuziK9IiIyKGrOhCYWT1wO3ApcApwpZmdMmmzPwJedffjgb8CvhD+7SnAFcAq4BLgb8L9iYhIQqJYqvJsYIe7Pw9gZvcBlwPbJmxzOXBz+PtG4K/NzML373P3EvCCme0I9/dEBOna38MboOvZWHYtIhK7pafBpZ+PfLdRVA0dA7w84XVn+N6U27h7GegHFh7i3wJgZuvNbLOZbc7SItQiInGbNYvXu/sdwB0AHR0dPqOdxBBJRURmuyhKBDuBYye8Xh6+N+U2ZtYALAD6DvFvRUQkRlEEgqeAE8xspZk1ETT+bpq0zSbgg+Hv7wN+6O4evn9F2KtoJXAC8LMI0iQiIoeo6qohdy+b2TXAI0A9cJe7bzWzW4DN7r4J+Drw92Fj8B6CYEG43QMEDctl4GPuPlJtmkRE5NBZkDGfXTo6Onzz5s1pJ0NEZFYxsy3u3jH5fY0sFhHJOAUCEZGMUyAQEck4BQIRkYxTIBARyTgFAhGRjFMgEBHJOAUCEZGMUyAQEck4BQIRkYxTIBARyTgFAhGRjFMgEBHJOAUCEZGMUyAQEck4BQIRkYxTIBARyTgFAhGRjFMgEBHJOAUCEZGMUyAQEck4BQIRkYxTIBARyTgFAhGRjFMgEBHJOAUCEZGMUyAQEck4BQIRkYxTIBARyTgFAhGRjFMgEBHJuKoCgZkdZWaPmtmvw3+PPMB2Hwy3+bWZfXDC+z8ys+1m9vPwZ3E16RERkemrtkSwAfiBu58A/CB8/TpmdhRwE/AW4GzgpkkB4yp3Xx3+9FSZHhERmaZqA8HlwD3h7/cA/2mKbS4GHnX3Pe7+KvAocEmVxxURkYhUGwiWuPsr4e9dwJIptjkGeHnC687wvYq/C6uF/szMrMr0iIjINDUcbAMz+z/A0ik++uTEF+7uZubTPP5V7r7TzFqAbwG/D3zjAOlYD6wHWLFixTQPIyIiB3LQQODubz/QZ2bWbWbL3P0VM1sGTFXHvxP47QmvlwM/Cve9M/y3YGb/QNCGMGUgcPc7gDsAOjo6phtwRETkAKqtGtoEVHoBfRB4cIptHgHeYWZHho3E7wAeMbMGM2sDMLNG4F3AL6tMj4iITFO1geDzwEVm9mvg7eFrzKzDzL4G4O57gM8CT4U/t4TvNRMEhF8APycoOdxZZXpERGSazH321bKYWS/w4gz/vA3YHWFyZoMsnjNk87yzeM6QzfOeyTkf5+6LJr85KwNBNcxss7t3pJ2OJGXxnCGb553Fc4ZsnneU56wpJkREMk6BQEQk47IYCO5IOwEpyOI5QzbPO4vnDNk878jOOXNtBCIi8npZLBGIiMgENRkIzOygI6ZFRCRQU4EgHK38JeAvzOyAU2PUGjP7AzM738wWhK9r6roeiJm918xWm1l9+LrmJy3Utc7UtV4+4fdYr3PNtBGEN8btwALgIeBq4NvA19y9lGLSYhGe71LgH4BRYAfQAlzn7rvNzLxWLu4E4XmvADYCeaAP2A78hbu/VsPnvRS4DxhB17rWrxqp3joAAAWWSURBVPUK4G6CueBeAG529xfiPGYt5SZagNXAh939m8CXgBOBtammKgZmVh9+AVqAne5+IfBRglGGNdt7wsxaw/M+BngqPO8/I/h/+G+pJi4mZnZ0OCdXC9CZoWs9P7zWRwM/rfVrPamE8xHgSXf/LeAV4CtmdkScx6+ZQODueeDfCUoCAD8G/hX4zTA3NeuZWb2ZfQ74nJmdD7yZIIeIu48A1xOc7/nhtOA1c33N7GPA42Z2CsEMtsvCj/4N+EvgrWZ2dnjes77awMzqwmv9JHAqQSYHqO1rHVbvfg74RzP7AMHiV63hxzV5rUNzJvzuBOu74O4bCEr868LJOWNREzfPBP8IrA6nxh4AngVKjD80Zq3wwb8FOJKgauCzwDBwgZmdDeDuo8DN4U/l9aw24YveAhSBPyZYu6LDzNa4e9ndXyJYIe+jEKyNkUpio/X7wEnAGe7+I+CfCB+AULPX+kiCqs4jgC8TrHj4U+DtZra6Fq+1mV1oZv8C3G5mV4VvF4BRM6sEwNuB9zEeECNXa4HgXwiKzFcDuPsW4D/w+mg7W40S1I1+xN3vJJiyeyXwaeCrMNag9G2g18yOSy2lEZqQ211C8IU4gmAq8xsZn+22nmBm232T1sOelcLgdwJwm7u/ambnAo3A1wiqPGvyWgPzgXZ3/6i7/xOwj2BW4luBW6C2rnW4nvutBEHvGwS5/msIMrTvAI4N20EeJfj+fyD8u8hLQTUVCMJlMx8ELjWztWbWTpCLLKeZrohsAR6o9JogqPpa4e53A/Vmdm2YK1wOlN19prOzHlbMrC48r93AXuD7BF+IJ4HTzez9YVXJXGBuuC72rBbmchcBv2tm1wJ/DfwtQY5wtZn9QbhpTV1rd3+Z4AF/twUrI55HEPCHgfPM7IrZfq3DKr/Kc/doglqLf3T3x4D/ShDwdgJbCUoBJ4Xb/i/ChcTiKAXVVCAAcPefAH8OXAp8D/i2u/8s3VRVz933uXsp/CIAXAT0hr9/CDjZzL4L3EvQNlITXewmVHmcRrDI0feA0wnO82+AK83sgfD3n0JtnDfBw78DWOXuZxGU/F4iyBCcTrAoVE1d69Ba4CfALnd/E8H/QwvBqoa/G17rrzILr7WZfYhgzfZbwrcGgHMJppPG3X8F3E9QQriVoIT0eTP7OMH1fya2tM3yKrYDChtW3N1roTQwJiwROEGd8bXuvsPMjifIMZ8KvFBZArSWmNmNBLmj1UA/QS7xne5eNLPLgH8Nc5Q1wcxyBA+8M9z9zPC99QTVnLcBFwDba/RaXw2c7u6fCF9/iSAAPkiwANasu9ZmNh/4n8BjBKs5Xunu283sHqDJ3a8Mt2sFfgC8B+gG3gv8JnCfu/84rvTVXImgwt2Hay0IhEYJ6ot3E1SNfJegW92ou/9LLT4YQnXAYoK+879F8GD4EwB33zTbHgwH4+5FYANBtd97zexk4Apg2AM/rOFrvQNYbmbnmNligrXM68JS8ay81mHnlevc/SsE1ZuVUsFHCRrDzw1f7yNYsdHcfcjd73X3a+MMAlDDJYJaZmbnEBSffwL8nbt/PeUkxc7M5rj7YPi7AYvdvTvlZMXOzN4K/EeCNb3vDDsK1LSwNPQR4N0Ewf82d6+ZMRNhd/ZNwGfc/Z/CrtG/QzBwbgVBtfalSbaBKBDMQhYMPf994C+9BkdNvxEza6jRkt4bCgcRjhx8y9phZisJBtENp52WqJnZfwE+4O5vC19fSlDddwywIelSjwKBiEiCKj3hzGwjwcCxUYKuwc+mNS6iZtsIREQOR2EQmEtQ7bUO2OHuv0hzcJymaxYRSd5HgaeBiw6H6l1VDYmIJGzCQMnDggKBiEjGqY1ARCTjFAhERDJOgUBEJOMUCEREMk6BQEQk4xQIREQyToFARCTj/j8dUlsrz4uIsgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sdj7GG5UzYsg",
        "outputId": "07613a61-f6fc-4340-bd9f-48eae7f76511"
      },
      "source": [
        "import pandas as pd\n",
        "DF_p_led = pd.read_csv('/content/drive/My Drive/Stage/PointGreyLEDStatus2019-10-14T09_35_30.csv', header=None)\n",
        "DF_p_frame = pd.read_csv('/content/drive/My Drive/Stage/PointGreyFramenumber2019-10-14T09_35_30.csv', header=None)\n",
        "DF_p_time = pd.read_csv('/content/drive/My Drive/Stage/PointGreyTimestamps2019-10-14T09_35_30.csv', header=None)\n",
        "\n",
        "print(len(DF_p_led))\n",
        "print(len(DF_p_frame))\n",
        "print(len(DF_p_time))\n",
        "\n",
        "print(DF_p_frame.diff())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "861229\n",
            "861229\n",
            "861229\n",
            "          0\n",
            "0       NaN\n",
            "1       4.0\n",
            "2       1.0\n",
            "3       1.0\n",
            "4       1.0\n",
            "...     ...\n",
            "861224  1.0\n",
            "861225  1.0\n",
            "861226  1.0\n",
            "861227  1.0\n",
            "861228  1.0\n",
            "\n",
            "[861229 rows x 1 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "qbQa93Zuwbkv",
        "outputId": "a8eaabe7-697f-4e40-f91f-5cba710466f8"
      },
      "source": [
        "import pandas as pd\n",
        "from scipy import signal\n",
        "import matplotlib.pyplot as plt\n",
        "whole_list =  list(map(int, list(whole)))[0:100000]\n",
        "trial_list = list(map(int, list(trial)))[0:100000]\n",
        "\n",
        "from itertools import repeat\n",
        "trial_list = trial_list + list(repeat(0, len(whole_list)-len(trial_list)))\n",
        "\n",
        "led_dict = {'whole': whole_list, 'trial':trial_list}\n",
        "led_df = pd.DataFrame.from_dict(led_dict)\n",
        "\n",
        "file_path = '/content/drive/My Drive/Stage/synch_test/'\n",
        "ADC_file = 'Trial1/100_ADC1_0.continuous'\n",
        "LED_file = 'PointGreyLEDStatus2018-07-30T11_24_12.csv'\n",
        "TIME_file = 'PointGreyTimestamps2018-07-30T11_24_12.csv'\n",
        "data = load(file_path+ADC_file)\n",
        "DF_ADC_samples, DF_ADC_timestamps = load_data_into_dataframe(data)\n",
        "\n",
        "DF_ADC_samples.rename(columns={0 : 'value'}, inplace=True)\n",
        "\n",
        "DF_ADC_samples.iloc[0:10000].plot.line(subplots=True)\n",
        "group_size = 1000\n",
        "lst = [DF_ADC_samples.iloc[i:i+group_size] for i in range(0,len(DF_ADC_samples)-group_size+1,group_size)]\n",
        "\n",
        "bla = []\n",
        "for df in lst:\n",
        "  m = df[\"value\"].mean()\n",
        "  bla.append(m)\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plct\n",
        "\n",
        "#whole_list =  [1,1,1,1,1,1,1,1,0,0,1,0,1,0,1,0,0,0,1,1,1,0,1,0,0]\n",
        "#trial_list =  [1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
        "Fs=100\n",
        "ts = np.arange(0,len(whole_list))\n",
        "\n",
        "plt.figure(figsize=(20,8))\n",
        "plt.subplot(3,1,1)\n",
        "plt.plot(ts,trial_list)\n",
        "plt.subplot(3,1,2)\n",
        "plt.plot(ts,whole_list)\n",
        "\n",
        "\n",
        "corr = np.correlate(whole_list, trial_list,'same')\n",
        "#corr = np.correlate([1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[1,1,1,1,1,1,1,1,0,0,1,0,1,0,1,0,0,0,1,1,1,0,1,0,0],'same')\n",
        "\n",
        "print(np.argmax(corr)) #this should be where correlation reaches its maximum value, and where the functions are most \"similar\"\n",
        "\n",
        "plt.subplot(3,1,3)\n",
        "plt.plot(corr, 'r')\n",
        "\n",
        "print(led_df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading continuous data...\n",
            "91487\n",
            "       whole  trial\n",
            "0          0      0\n",
            "1          0      0\n",
            "2          0      0\n",
            "3          0      0\n",
            "4          0      0\n",
            "...      ...    ...\n",
            "99995      0      0\n",
            "99996      0      0\n",
            "99997      0      0\n",
            "99998      0      0\n",
            "99999      0      0\n",
            "\n",
            "[100000 rows x 2 columns]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABIoAAAHSCAYAAACD9CDIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzde5Rd512n+ed3zqn7RbcqWbIulmzLcRTn5ii2Q4AEEjp2oO2eJtB2wxBor3hNE0Masph2oMewAjNNYBZpssakMZ1MAg1xTKAZDRFtQkiTgY4dyzhxIju2FdnWxbqUqlT3Ovd3/jhblVK5ZFdaW1Uq6fmsVUtn7/2e/b6nzn73u+urfd4TKSUkSZIkSZKkwnI3QJIkSZIkSRcGgyJJkiRJkiQBBkWSJEmSJEnKGBRJkiRJkiQJMCiSJEmSJElSxqBIkiRJkiRJAJSWuwEvZ2BgIG3btm25myFJkiRJknTReOyxx06mlAYX2nZBB0Xbtm1j7969y90MSZIkSZKki0ZEvHC2bbl99CwiPhkRJyLim2fZHhHxsYjYHxFPRMT1edUtSZIkSZKkc5fnHEWfAm5+me23ADuyn7uAj+dYtyRJkiRJks5RbkFRSunLwMjLFLkN+MPU8jCwOiI25lX/hS6ldMbjsy0vtO1/5HnN5pnbms2zb3u5ds6va6HnvVK7zlbfd/Na5+/ju3mtC5WTJEmSJEkvtZRzFG0CDs1ZPpytO7qEbVgWv/JfvsEfP3JwuZuhzPO/+cPL3QRJkiRJki5IeX70LBcRcVdE7I2IvUNDQ8vdnFwYEkmSJEmSpJVgKYOiI8CWOcubs3VnSCndn1LalVLaNTi44De1SZIkSZIk6TxYyqBoN/BT2bef3QSMpZQu+o+dSZIkSZIkrRS5zVEUEZ8B3g4MRMRh4FeBNoCU0n8E9gDvBvYD08DP5FW3JEmSJEmSzl1uQVFK6Y5X2J6A9+dVnyRJkiRJkvJ1wU1mLUmSJEmSpOVhUCRJkiRJkiTAoEiSJEmSJEkZgyJJkiRJkiQBBkWSJEmSJEnKGBRJkiRJkiQJMCiSJEmSJElSxqBIkiRJkiRJgEGRJEmSJEmSMgZFkiRJkiRJAgyKJEmSJEmSlDEokiRJkiRJEmBQJEmSJEmSpIxBkSRJkiRJkoAcg6KIuDkino6I/RFxzwLbt0bElyLi8Yh4IiLenVfdkiRJkiRJOne5BEURUQTuA24BdgJ3RMTOecX+HfBgSumNwO3A7+VRtyRJkiRJkvKR1x1FNwD7U0oHUkpV4AHgtnllEtCfPV4FvJhT3ZIkSZIkScpBKaf9bAIOzVk+DNw4r8yvAX8dET8H9ADvzKluSZIkSZIk5WApJ7O+A/hUSmkz8G7gjyLiJfVHxF0RsTci9g4NDS1h8yRJkiRJki5teQVFR4Atc5Y3Z+vmuhN4ECCl9BWgExiYv6OU0v0ppV0ppV2Dg4M5NU+SJEmSJEmvJK+g6FFgR0Rsj4h2WpNV755X5iDwDoCIeDWtoMhbhiRJkiRJki4QuQRFKaU6cDfwEPAUrW832xcRH46IW7NiHwTeFxFfBz4D/HRKKeVRvyRJkiRJks5dXpNZk1LaA+yZt+7eOY+fBN6aV32SJEmSJEnK11JOZi1JkiRJkqQLmEGRJEmSJEmSAIMiSZIkSZIkZQyKJEmSJEmSBBgUSZIkSZIkKWNQJEmSJEmSJMCgSJIkSZIkSRmDIkmSJEmSJAEGRZIkSZIkScoYFEmSJEmSJAkwKJIkSZIkSVLGoEiSJEmSJEmAQZEkSZIkSZIyBkWSJEmSJEkCDIokSZIkSZKUyS0oioibI+LpiNgfEfecpcyPR8STEbEvIv4kr7olSZIkSZJ07kp57CQiisB9wA8Bh4FHI2J3SunJOWV2AB8C3ppSOhUR6/OoW5IkSZIkSfnI646iG4D9KaUDKaUq8ABw27wy7wPuSymdAkgpncipbkmSJEmSJOUgr6BoE3BozvLhbN1c1wDXRMQ/RMTDEXHzQjuKiLsiYm9E7B0aGsqpeZIkSZIkSXolSzmZdQnYAbwduAP4g4hYPb9QSun+lNKulNKuwcHBJWyeJEmSJEnSpS2voOgIsGXO8uZs3VyHgd0ppVpK6TngGVrBkSRJkiRJki4AeQVFjwI7ImJ7RLQDtwO755X5C1p3ExERA7Q+inYgp/olSZIkSZJ0jnIJilJKdeBu4CHgKeDBlNK+iPhwRNyaFXsIGI6IJ4EvAb+UUhrOo35JkiRJkiSdu1JeO0op7QH2zFt375zHCfjF7EeSJEmSJEkXmKWczFqSJEmSJEkXMIMiSZIkSZIkAQZFkiRJkiRJyhgUSZIkSZIkCTAokiRJkiRJUsagSJIkSZIkSYBBkSRJkiRJkjIGRZIkSZIkSQIMiiRJkiRJkpQxKJIkSZIkSRJgUCRJkiRJkqSMQZEkSZIkSZIAgyJJkiRJkiRlDIokSZIkSZIE5BgURcTNEfF0ROyPiHteptyPRkSKiF151S1JkiRJkqRzl0tQFBFF4D7gFmAncEdE7FygXB/wAeCRPOqVJEmSJElSfvK6o+gGYH9K6UBKqQo8ANy2QLlfBz4ClHOqV5IkSZIkSTnJKyjaBByas3w4WzcrIq4HtqSUPp9TnZIkSZIkScrRkkxmHREF4HeADy6i7F0RsTci9g4NDZ3/xkmSJEmSJAnILyg6AmyZs7w5W3daH3Ad8N8i4nngJmD3QhNap5TuTyntSintGhwczKl5kiRJkiRJeiV5BUWPAjsiYntEtAO3A7tPb0wpjaWUBlJK21JK24CHgVtTSntzql+SJEmSJEnnKJegKKVUB+4GHgKeAh5MKe2LiA9HxK151CFJkiRJkqTzq5TXjlJKe4A989bde5ayb8+rXkmSJEmSJOVjSSazliRJkiRJ0oXPoEiSJEmSJEmAQZEkSZIkSZIyBkWSJEmSJEkCDIokSZIkSZKUMSiSJEmSJEkSYFAkSZIkSZKkjEGRJEmSJEmSAIMiSZIkSZIkZQyKJEmSJEmSBBgUSZIkSZIkKWNQJEmSJEmSJMCgSJIkSZIkSRmDIkmSJEmSJAE5BkURcXNEPB0R+yPingW2/2JEPBkRT0TEFyPiirzqliRJkiRJ0rnLJSiKiCJwH3ALsBO4IyJ2ziv2OLArpfQ64HPAb+VRtyRJkiRJkvKR1x1FNwD7U0oHUkpV4AHgtrkFUkpfSilNZ4sPA5tzqluSJEmSJEk5yCso2gQcmrN8OFt3NncCf5VT3ZIkSZIkScpBaakrjIifBHYBbzvL9ruAuwC2bt26hC2TJEmSJEm6tOV1R9ERYMuc5c3ZujNExDuBXwFuTSlVFtpRSun+lNKulNKuwcHBnJonSZIkSZKkV5JXUPQosCMitkdEO3A7sHtugYh4I/D7tEKiEznVK0mSJEmSpJzkEhSllOrA3cBDwFPAgymlfRHx4Yi4NSv220Av8KcR8bWI2H2W3UmSJEmSJGkZ5DZHUUppD7Bn3rp75zx+Z151SZIkSZIkKX95ffRMkiRJkiRJK5xBkSRJkiRJkgCDIkmSJEmSJGUMiiRJkiRJkgQYFEmSJEmSJCljUCRJkiRJkiTAoEiSJEmSJEkZgyJJkiRJkiQBBkWSJEmSJEnKGBRJkiRJkiQJMCiSJEmSJElSxqBIkiRJkiRJgEGRJEmSJEmSMgZFkiRJkiRJAnIMiiLi5oh4OiL2R8Q9C2zviIjPZtsfiYhtedUtSZIkSZKkc5dLUBQRReA+4BZgJ3BHROycV+xO4FRK6Wrgo8BH8qhbkiRJkiRJ+cjrjqIbgP0ppQMppSrwAHDbvDK3AZ/OHn8OeEdERE71S5IkSZIk6RyVctrPJuDQnOXDwI1nK5NSqkfEGLAOOJlTG6RF+Zd/8PByN0GSJEmStAL9izdv4bY3bFruZpxXeQVFuYmIu4C7ALZu3brMrcnHv3rrdj75D88tdzOUqTWay90ESZIkSdIK1Gim5W7CeZdXUHQE2DJneXO2bqEyhyOiBKwChufvKKV0P3A/wK5duy6Kd+Def7qTe//p/CmbJEmSJEmSLix5zVH0KLAjIrZHRDtwO7B7XpndwHuzx+8B/jaldFEEQZIkSZIkSReDXO4oyuYcuht4CCgCn0wp7YuIDwN7U0q7gU8AfxQR+4ERWmGSJEmSJEmSLhC5zVGUUtoD7Jm37t45j8vAj+VVnyRJkiRJkvKV10fPJEmSJEmStMLFhTxNUEQMAS8sdztyMgCcXO5GSCuAfUVaHPuKtDj2FWlx7CvS4lwsfeWKlNLgQhsu6KDoYhIRe1NKu5a7HdKFzr4iLY59RVoc+4q0OPYVaXEuhb7iR88kSZIkSZIEGBRJkiRJkiQpY1C0dO5f7gZIK4R9RVoc+4q0OPYVaXHsK9LiXPR9xTmKJEmSJEmSBHhHkSRJkiRJkjIGRUsgIm6OiKcjYn9E3LPc7ZHOt4jYEhFfiognI2JfRHwgW782Ir4QEc9m/67J1kdEfCzrI09ExPVz9vXerPyzEfHeOevfFBHfyJ7zsYiIpX+lUj4iohgRj0fEX2bL2yPikez4/mxEtGfrO7Ll/dn2bXP28aFs/dMR8a456x2DdFGIiNUR8bmI+FZEPBURb3FckV4qIn4hu/76ZkR8JiI6HVckiIhPRsSJiPjmnHXnfRw5Wx0XMoOi8ywiisB9wC3ATuCOiNi5vK2Szrs68MGU0k7gJuD92XF/D/DFlNIO4IvZMrT6x47s5y7g49A6qQK/CtwI3AD86pwT68eB98153s1L8Lqk8+UDwFNzlj8CfDSldDVwCrgzW38ncCpb/9GsHFn/uh14Da2+8HtZ+OQYpIvJ7wL/NaV0LfB6Wn3GcUWaIyI2AT8P7EopXQcUaY0PjisSfIqXntuXYhw5Wx0XLIOi8+8GYH9K6UBKqQo8ANy2zG2SzquU0tGU0j9mjydoXcxvonXsfzor9mngn2WPbwP+MLU8DKyOiI3Au4AvpJRGUkqngC8AN2fb+lNKD6fWRGt/OGdf0ooSEZuBHwb+U7YcwA8Cn8uKzO8rp/vQ54B3ZOVvAx5IKVVSSs8B+2mNP45BuihExCrg+4FPAKSUqimlURxXpIWUgK6IKAHdwFEcVyRSSl8GRuatXopx5Gx1XLAMis6/TcChOcuHs3XSJSG7hfmNwCPAZSmlo9mmY8Bl2eOz9ZOXW394gfXSSvQfgP8VaGbL64DRlFI9W557fM/2iWz7WFb+u+1D0kqzHRgC/u9ofUzzP0VED44r0hlSSkeA/xM4SCsgGgMew3FFOpulGEfOVscFy6BI0nkTEb3AnwH/JqU0PndblrT7tYu6pEXEjwAnUkqPLXdbpAtcCbge+HhK6Y3AFPNu3XdckSD7CMxttMLVy4Ee/BiltChLMY6slLHKoOj8OwJsmbO8OVsnXdQioo1WSPTHKaU/z1Yfz27LJPv3RLb+bP3k5dZvXmC9tNK8Fbg1Ip6ndfv+D9Kah2V19pEBOPP4nu0T2fZVwDDffR+SVprDwOGU0iPZ8udoBUeOK9KZ3gk8l1IaSinVgD+nNdY4rkgLW4px5Gx1XLAMis6/R4Ed2TcNtNOaFG73MrdJOq+yz7Z/AngqpfQ7czbtBk5/M8B7gf9nzvqfyr5d4CZgLLs98yHgn0TEmux/yP4J8FC2bTwibsrq+qk5+5JWjJTSh1JKm1NK22iND3+bUvoJ4EvAe7Ji8/vK6T70nqx8ytbfnn17zXZaEyh+FccgXSRSSseAQxHxqmzVO4AncVyR5jsI3BQR3dmxfLqvOK5IC1uKceRsdVywSq9cROcipVSPiLtpHVBF4JMppX3L3CzpfHsr8D8D34iIr2Xrfhn4TeDBiLgTeAH48WzbHuDdtCZKnAZ+BiClNBIRv07rogTgwyml0xPQ/Sytby7oAv4q+5EuFv8WeCAifgN4nGwC3+zfP4qI/bQmY7wdIKW0LyIepPXHQB14f0qpAeAYpIvIzwF/nP1xeoDWWFHAcUWalVJ6JCI+B/wjrfHgceB+4PM4rugSFxGfAd4ODETEYVrfXrYUf5+crY4LVrQCY0mSJEmSJF3q/OiZJEmSJEmSAIMiSZIkSZIkZQyKJEmSJEmSBBgUSZIkSZIkKWNQJEmSJEmSJMCgSJIkSZIkSRmDIkmSJEmSJAEGRZIkSZIkScoYFEmSJEmSJAmA0nI34OUMDAykbdu2LXczJEmSJEmSLhqPPfbYyZTS4ELbLuigaNu2bezdu3e5myFJkiRJknTRiIgXzrbNj55JkiRJkiQJyDEoiohPRsSJiPjmWbZHRHwsIvZHxBMRcX1edUuSJEmSJOnc5XlH0aeAm19m+y3AjuznLuDjOdYtSZIkSZKkc5TbHEUppS9HxLaXKXIb8IcppQQ8HBGrI2JjSuloXm24UDWaiR/9+H/na4dGASgVgnozAXDD9rV89bmR2bJzt21a3cWR0ZmX7O/uH7iabx2b4G+eOg7AD167nr/91gkAXrtpFbVGk28dm3jJ8169sZ++jhJffb5VX39nifFyfXb7lQM9HDg59ZLnves1l/HcySmeOT4JwBXrunlheHp2++WrOnlxrPyS563raaeREqPTtZe8NoD2UoFqvfmS5/1vP7KT3/j8k6Ss6GX9HRwfrwDwtmsG+btnhhb8ff3o9Zv5s388PLttsK+DoYnW8zau6uTonDb2dpSYrLRe+53fu51P/P1zZ7R7eKoKwLUb+s74Xc5t843b1/LInPdu7ra+zhIT5TofuuVadm1bwx1/8AgbV3XyS+96Ff/H55/ixbEyv/DOazg6NsNn9x7iVZf1sbq7jYcPtPb3vu/bTkRw/5cP8NpNqygUgsMj0wxPVfnh121k35Exnh+e5vqtq/nHg6N0lApU6k3e+5Yr6Ots4//60n5edVkfq7raOHBykpOTVW7cvpZqo8njB0d57aZVfOPIGL0dJaaqdbav6+GKdd186ekhrhrs4dtDU/S0FwGYqjZY093Gqex9BIho/e5/7gd3cHBkms89dpg3b1tDEOx9YYSOUpHXb1nFRLnOvhfHzzhGejtKbFnbzequNr5yYHj2dwXQ015kqto467F44/a1NJqJvS+cAl567F29vpf9JyZf8rxCwLaBHg4MvXSfcOYxdtoV67r59duu4/1//I9MVOr8yxu38v89O8ShkRk29HfylqvW8fknjlJtNGfbDdBRKvCTN10xe0y1FYPu9hJjM7WXHF/z2/jz79jBf/ibZwFY29NOsRCzx/Abt67m8YOjs+U72wqUa63j7fuvGeTLc/rF6u622X43v77u9iLT1QbXbuhj17Y1/OeHDwKt88P+ExPUGq3+9LrNq3ji8Njs8yKY7ZPfe/UAf7//5IK/y61ruzk4Mr3gtrdevY5/2D8MQF9HiYnKd84/8/vaXK+5vJ99L44vuO1s58k//9nv4V//58c4Pl7hf3nbVfz1vmMcODlFX0eJH3n95fy/X3+RyUr9jNfZVgzu+v4r+f2/O0C9mdi4qpPJSn32+Dzba+tpL/KzP3A1v/3Q0wBsH+hhptrg2Hjr2Jz/3s09V8x/7+a+X23FmH0/5i7vWN/LD167nt//8gEAdl2xhiePjjOdHYNvumINj2V9ZL7559C55p/bT1vb085rN62afd7888HLvXfzX/ti6vvd29/AR7/wDM8PT9PTXmTL2u7Z/X/fjgGeOjrBycnKS573L3Zt4S+feHG2L849z5cKQaI1Hs/3b975nX4HZ54PXr9lNV8/tPB7945r1/PFbOwFGOjtmG3X3LrhzOP9A+/Ywe9+sVXf5dkxdnosnj+uzB3j3vnq9fzNUycW3Ha2cXihds71cu/d6fFlIddc1sszxyf5s3/9Fh4/OMpvfP4pfvi1G9l5eT+f+PvnGJmq8u//+Wv5q28e48vPDPFjb9rM3zx1nGq9Sb2Z+MA7d/CNw2P81TeP8c/ecDkPHxjh1HSVvs4Sl/V3MjxZ5dh4+YxrpMv6O/jn12/m4Mg0n3/iKLdct4Gnj09wYGiKzWu66O0oERE8dXT8jNc12NdBvdHkqsFe9r5w6owx5/SxvNCxONDbzi+/+9V8+isv8PVDo3zv1QNMVOp8/dAoG1d1smVtNycnKhw4OXXGOXd1dxu7rljLt46Nc/hU69x0+ng4/e+a7jYmyvUzronaisFP3HgFn/nqQRrNRL2ZznhfV3e3MT5TY4FDeHbcPm3uazybW19/OTdduY5f/i/foBBw+w1b+et9xzk5WWHXFWtY3d3O3zx1fHasOf3vay7vZ8f6Xv7iay8C0F4s0FEqzB7fW9Z2cWjkO+fk09cn127oY+fGfv788SNA67rs+Hh59vXs3NjPk0cXPs9/z1Xr+O/fHl5w29nGgDtu2Mq3hyZnj5/rNvXzzSPjs7+fjas6Z69r5++vp6M4u+30mHna+r4OTky89PzztmsG+frh0dnjYG67SoWgUIgFr3mvHOzhuZNTs+Prqq622euFs3nPmzbz1qvX8Quf/TrQOqc88OhBjo9XeNMVa7isv4M93zg2W/70e3fdpn5evaGfP32sda28tqedlNLs+Xz+tfLpc8x1m/q57vJVPPDoodk2Hz41M/t65v5u55t/Tptr+0Drtc/3gXfs4KvPjfCVA633fO44smN9L4UInj7+0vPW6zavotZIPJUdR/PPw2c7Vm65bgOPvXBq9n2df004fz+nDfZ1cGqqOtuPTx/rr+R3b38DH3jga0DrvP6NI2Oz9f3w6zby+Se+86fx3OPv9jdvmX0P4MzrhfnnsLlt/okbt/LHj7Su9XraizQTzNRa+zz998BCbrpy7ezfJfOdrR9cOdjD5jXds9c188eYlxtz3rBl9ezfyl1txdk2nt7vQtfwb962huPjldlrs/nnn7Ndc6/pbqNab85eLxSCM86txUKwfaCHT/+rG/i13fv4wpPH+d//p+v4u6eH+Osnj3Pdpn7+5H030d/ZtuBruVhESguMOP+jO2sFRX+ZUrpugW1/CfxmSunvs+UvAv82pbR3Xrm7aN1xxNatW9/0wgtnnV9pxXjgqwe558+/sdzNkCRJkiRJ5+Ddr93A7/3Em5a7GecsIh5LKe1aaNsFN5l1Sun+lNKulNKuwcEFv6ltxXml/xmQJEmSJEkXvqEF7qi62CxlUHQE2DJneXO2TpIkSZIkSReApQyKdgM/lX372U3A2KUwP5EkSZIkSdJKkdtk1hHxGeDtwEBEHAZ+FWgDSCn9R2AP8G5gPzAN/ExedUuSJEmSJOnc5fmtZ3e8wvYEvD+v+iRJkiRJkpSvC24ya0mSJEmSJC0PgyJJkiRJkiQBBkWSJEmSJEnKGBRJkiRJkiQJMCiSJEmSJElSxqBIkiRJkiRJgEGRJEmSJEmSMgZFkiRJkiRJAgyKJEmSJEmSFiWI5W7CeWdQJEmSJEmStAiJtNxNOO8MiiRJkiRJkgQYFC2JuPjvTJMkSZIkSRcBg6IlkC7+O9MkSZIkSdJFwKBIkiRJkiRpEZzM+rsQETdHxNMRsT8i7llg+9aI+FJEPB4RT0TEu/OqW5IkSZIk6XxzMutFiogicB9wC7ATuCMids4r9u+AB1NKbwRuB34vj7pXAucokiRJkiRJK0FedxTdAOxPKR1IKVWBB4Db5pVJQH/2eBXwYk51X/Cco0iSJEmSJK0EeQVFm4BDc5YPZ+vm+jXgJyPiMLAH+LmFdhQRd0XE3ojYOzQ0lFPzJEmSJEmSzo1zFOXrDuBTKaXNwLuBP4qIl9SfUro/pbQrpbRrcHBwCZsnSZIkSZJ0acsrKDoCbJmzvDlbN9edwIMAKaWvAJ3AQE71S5IkSZIknVdOZr14jwI7ImJ7RLTTmqx697wyB4F3AETEq2kFRZfEZ8uczFqSJEmSJK0EuQRFKaU6cDfwEPAUrW832xcRH46IW7NiHwTeFxFfBz4D/HRKl8Y0z5fGq5QkSZIkSStdKa8dpZT20Jqkeu66e+c8fhJ4a171SZIkSZIkLSUns5YkSZIkSdIlw6BoCThHkSRJkiRJK5+TWSsXzlEkSZIkSZJWAoMiSZIkSZKkRXCOIkmSJEmSJF0yDIqWgHMUSZIkSZKklcCgaAk4R5EkSZIkSSufk1lLkiRJkiTpkmFQJEmSJEmStAhOZq1cOEeRJEmSJElaCQyKJEmSJEmSBBgULQkns5YkSZIkSSuBQZEkSZIkSZIAg6Il4RxFkiRJkiRpJcgtKIqImyPi6YjYHxH3nKXMj0fEkxGxLyL+JK+6JUmSJEmSdO5KeewkIorAfcAPAYeBRyNid0rpyTlldgAfAt6aUjoVEevzqHslcI4iSZIkSZK0EuR1R9ENwP6U0oGUUhV4ALhtXpn3AfellE4BpJRO5FS3JEmSJEnS+XcJTC2TV1C0CTg0Z/lwtm6ua4BrIuIfIuLhiLg5p7oveM5RJEmSJEnSReAS+MRQLh89+y7q2gG8HdgMfDkiXptSGp1bKCLuAu4C2Lp16xI2T5IkSZIk6dKW1x1FR4Atc5Y3Z+vmOgzsTinVUkrPAc/QCo7OkFK6P6W0K6W0a3BwMKfmSZIkSZIk6ZXkFRQ9CuyIiO0R0Q7cDuyeV+YvaN1NREQM0Poo2oGc6r+gOZm1JEmSJElaCXIJilJKdeBu4CHgKeDBlNK+iPhwRNyaFXsIGI6IJ4EvAb+UUhrOo35JkiRJkqTz7hKYgzi3OYpSSnuAPfPW3TvncQJ+Mfu5pDiZtSRJkiRJF4FL4BNDeX30TJIkSZIkSSucQdEScI4iSZIkSZK0EhgUSZIkSZIkLcYlMLWMQdEScI4iSZIkSZK0EhgUSZIkSZIkLcYlMLWMQZEkSZIkSZIAgyJJkiRJkiRlDIokSZIkSZIW4xKYg9igSJIkSZIkSYBBkSRJkiRJ0uI4mbUkSZIkSZIuFQZFkiRJkiRJi+EcRZIkSZIkSbpUGBRJkiRJkiQJyDEoioibI+LpiNgfEfe8TLkfjYgUEbvyqluSJEmSJOm8czLrxYmIInAfcAuwE7gjInYuUK4P+ADwSB71SpIkSZIkKT953VF0A7A/pXQgpVQFHgBuW6DcrwMfAco51StJkiRJkrQ0nMx60TYBh+YsH87WzYqI64EtKaXP51SnJEmSJNz6QF8AACAASURBVEmScrQkk1lHRAH4HeCDiyh7V0TsjYi9Q0ND579xkiRJkiRJAvILio4AW+Ysb87WndYHXAf8t4h4HrgJ2L3QhNYppftTSrtSSrsGBwdzap4kSZIkSZJeSV5B0aPAjojYHhHtwO3A7tMbU0pjKaWBlNK2lNI24GHg1pTS3pzqlyRJkiRJ0jnKJShKKdWBu4GHgKeAB1NK+yLiwxFxax51SJIkSZIk6fwq5bWjlNIeYM+8dfeepezb86pXkiRJkiRJ+ViSyawvdXEpfH+eJEmSJEla8QyKlkAiLXcTJEmSJEmSXpFBkSRJkiRJkgCDIkmSJEmSJGUMiiRJkiRJkgQYFEmSJEmSJCljUCRJkiRJkiTAoEiSJEmSJEkZgyJJkiRJkqTFSMvdgPPPoGgJBLHcTZAkSZIkSXpFBkVLIF0KkaMkSZIkSVrxDIokSZIkSZIEGBRJkiRJkiQtyqXwiSGDoiXgHEWSJEmSJGklMChaApdC4ihJkiRJkla+3IKiiLg5Ip6OiP0Rcc8C238xIp6MiCci4osRcUVedUuSJEmSJJ1vl8InhnIJiiKiCNwH3ALsBO6IiJ3zij0O7EopvQ74HPBbedQtSZIkSZK0FC6FTwzldUfRDcD+lNKBlFIVeAC4bW6BlNKXUkrT2eLDwOac6r7gXQqJoyRJkiRJF7t08edEuQVFm4BDc5YPZ+vO5k7grxbaEBF3RcTeiNg7NDSUU/MkSZIkSZL0SpZ8MuuI+ElgF/DbC21PKd2fUtqVUto1ODi4tI07Ty6FW9MkSZIkSdLKV8ppP0eALXOWN2frzhAR7wR+BXhbSqmSU92SJEmSJEnKQV53FD0K7IiI7RHRDtwO7J5bICLeCPw+cGtK6URO9a4IzlEkSZIkSdLKdyl8XiiXoCilVAfuBh4CngIeTCnti4gPR8StWbHfBnqBP42Ir0XE7rPsTpIkSZIkScsgr4+ekVLaA+yZt+7eOY/fmVddK41zFEmSJEmSpJVgySezliRJkiRJ0oXJoGgJOEeRJEmSJElaCQyKJEmSJEmSFiGli39qGYMiSZIkSZIkAQZFS8LJrCVJkiRJ0kpgUCRJkiRJkiTAoGhJOJm1JEmSJElaCQyKJEmSJEmSBBgULQnnKJIkSZIkSSuBQZEkSZIkSdIiXAq3gRgULQHnKJIkSZIkSSuBQZEkSZIkSZIAgyJJkiRJkiRlDIokSZIkSZIWIV0CkxTlFhRFxM0R8XRE7I+IexbY3hERn822PxIR2/KqW5IkSZIkSecul6AoIorAfcAtwE7gjojYOa/YncCplNLVwEeBj+RRtyRJkiRJkvKR1x1FNwD7U0oHUkpV4AHgtnllbgM+nT3+HPCOiLgkvg7sueGp5W6CJEmSJEnSK8orKNoEHJqzfDhbt2CZlFIdGAPWzd9RRNwVEXsjYu/Q0FBOzVtezx6fWO4mSJIkSZKkc7Sqq225m3DelZa7AfOllO4H7gfYtWvXRTFN1G+95/X8/bNDHByZJiJY19POVLXBRLnGljXdHByZ5tDINOt621nT087oVI3nhqd42zWDnBgvc+DkFKVCcM1lfRwdK/MD1w5yYrzCi6MzNFJi0+puTk1XeXF0htdtXkWl1uTERIV6M9FRKtBWDF4cK/PGLauZqTY4Nl6mUm8y2NdBe7FAudZgVXcbnaUiI1NV6s1EZ1uBQgRjMzVevbGf0ekqp6arFCJY1dXGdLXBqekqG1d1AjA6XaNcazLQ106tnjg5WeGaDX1MVeqMzdSYrtTZsrabyUqdk5MVNq7qohDBeLnG8GSFKwd7mak2GJup8catq9n34jgvDE9x5WAvbYVgdKbGC8PTfM9V6xieqnJgaJJVXe1sWNXB2EyNA0NT3HzdBg4OT7N/aJL1fR1c1t/Jyckqzxyf4Pt2DDBRrvNMFtq9fvNqhiYqvDg2w83XbWD/iUm+eWSM125azWBfBxPlGiNTVV6zaRWnpqocHSvT015kfX8Hp6ZrnJqq8satqzk2VuHQqWnW93XQ39XG6HSNY2MzvOmKNQxPVdm2rofejhLPnJigGMH2gR4OjkwzWamzfaCHqUrr99hRKtDf1cbIVJVyrcGWNd00U+L4eJnu9hLT1TrVepPJSoPNa7oYzX6nA30dHBiaZH1fJ9PVBtsGumkvFnj2xCQDvR3UG00q9SblWoM1Pe0UIjgxUWZDfyffHppksK+DeqN1nKzpaeeF4WnW93dwaGSG1d2tE2AAq7vbmKk2WdVV4oWRaS7r76TRTFy+upNqvcnRsTL9nW20FYOJSp0g6G4vUggYnqrS39nGZKVOsRAUC0F7sUBbqcDwZIWutiL1ZiKlRFuxwHS1wdqedk5NVylGUGs0aSsWKBULrO1pY6rSYKJcZ7xco7ejxGSlTluxQLXepK+zxFSlzuhMjb7O1umtVAiKhVY/qNSbHDk1w/aBHiYrdSJgbLrGhlWdNFLi8RdGed3mVdSbiVVdbbx6Yx9fOzRGrdFk20APQxMVxmdqrOluz46FKvVGorejRLXR+j13lIoM9LZzYqJCo5loLxUoRjBVrVMqBF3tJcq1Bs1mYqbWYHV3qz8VIrhiXTcHh6dpJuhsKxABM9Um9WaTgd7WsZ4STNfqrOtpZ6bapFJvtJ43Ms1Euc5MtcH6/g6mqw2OnGqdE05OVqk1mhwcmea6TauYqTYY6G2nt6PE88PTVOpNtqztYnymTrnWoFQI1va2c2K8wky1QXupQHd7kZlag7ZigfV9HRwbLzMyVWV9XwcRQQD1ZuKKdd2MTFUZm2m9Px2lAjO1Buv7WueKU9NVJst1NqzqZKbaoFxvUCwUWNXVxmS5zomJMhtXddJMMF1t0N9Voj07Ll4cneGKdT3UGk2mKnUIGOhpvdbnTk5y9fpemgk6SgWu2dDHM8cmODpWZufl/Rw5NcNYdlys6W69PxPlGpvWfOd1d7cX2biqk8OnZpis1Fnf10mj2WRspkZne5G2QoFqo8l0tU6tnhjoa2eiXKerrchVg708e2KCqWqDDf2dNFOi0Ux0thXpaS8yUa63jnOgt6O1fPp3efpYOb1tslynkRLrejqYrNRpNBOj01U2relislxnbU87fZ1tHBmdoVJvsGFVF9OVOlOVOh1tRfo7S4xM1Wb7TnupwEy1TnupwNqeDp49McHYdI21Pe0UC8Gp6SrNBNsHejgxXmZkukYhyM6hFa5Y20OpGBwfLzM8WeWq9b3MVOucnKzS01FkfV8nQxMVjo7NsHFVF+2lAkMTFdb2tLO6u40TExWePznFjdvXEQET5RrFQoGB3nbGZ2q8OFZm69puoPWe37h9LftPTM6OMZV6k+lqg+lqg+0D3YxO1xibqdHf1UZKUK03ma7VuXqwl5OTVcbLNbrainS1FZms1CnXGwz2dlCpN5kot/r96XFsulLnVRv6ePbEJEdOzbBpTRedpQKnpmscGy/zhi2rOTVV5fCpGTraCmxZ083IdJUjp2Z42zWDHBmd4eDINL0dJTav6eLkZIVvn5jiLVetY6Jc5+DIFBPlOm/etpYTE2VKxQKvubyf/ScmmarUZ/vj2EyNCNiwqouRySrDU63fX1dbkVPTNQC2revm2HiZoYkK6/s6aS8VGJ2uUqk3uXKwh+HJKicmyvR3tdHf2cbodJXV3a334Ph4hRPjZa5e30ul3mR4skpbKdjY38nIVJUXx8psWt1FWzE4OVmlr7NEd3uJQ6em+cq3h/mhnZfRaCb2vThGe6nI9VtXM1Gu85Yr1zE8VeHpY5NsXdtFe6nIkdFpao3Ezo39HB8vc3SszFXrezl8appiRHasdTNVaXBwZJqrBns4MloGoKutyEBfO/VGYnS6xsZVnTw3PEV/ZxsRcFl/B5Vak2PjZQb7OpipNqjUm7QXCxQLQakYjExVWdfbwdB4mY62Ip1trbGoVAiGp6qs6mpjolynENDdXmJ0psba7jbGZup0tBWYLNfpKBXobCuyZW0XR0bLnMjqm642GJ2u0d9VorOtyHSlzkytQWdbkZlqg3oz0d9Zor+rbXZ8aKbWeWCiXKe3o8RMrXVOTQmOjs1wWX9rHG0rBlcO9vL88BSFCCq1Jv1dJVKC9lKB9mLr/FOIYHSmyprudmaqDSJgsK+D0ekahQhGplrHx1S1Tq3RpL+zjVqjSbFQ4IXhKa4a7GW62mCgr53+zjaePzn1nWNvqjI71heLBU5NVWfH7UJAudakq63Iqq42njk+QbHY+mBAMYJyrUkiMdDbwdBEhY5SgSOjM2xe002t0WRNdxvd7SUOjkwDrT5Yrjdmf0enn1dvNqk3Ev1dbbPbLuvv5MREmYlyncG+Dno7SjSaifGZGlvWtq6Bp7Jrge72ItV6k6vW986ORYUI+jpLTJZbv5PV3e0kYKpSp1Jv0N3e2l+t0WRdTzv1ZmKq0mCq2jrfVmpNao0mpWLr2ma62mBkqspl/Z3UG4npWp0d6/s4MDQ5e318xboexmdqHB2b4VWX9VFpNJmpNvj20CSv27x69vH3XDUw2/4DQ1O8edsaJip1vnlkjDdsWQ3ARLnOo8+PcPN1GxifqXPVYA99nW08dXScejNx/RVrODY2w0S5zqbVXdSaidGpKgB9na3rk8ly65qxs63IsbEyxULQVgxSav0e+rvaKBWC8XKdUjGYKNdZ093GZKV1vdDVVuTF0TIR0N3eum6brjZa16+dJYanWtds5XqDno4S5VqTjuy4PXyqdZ3RUSrQkZ2XZ6oNrhrsycbiOtVGk439nYzO1Pieq9bx4ugMx8bLdJZax9vYTI3RmRrbB1rXAEMTFQA6SkUAxmZqXHNZL1PVBmPTVTraiqztaaeaXQN3ZmPCRLnGRKXO2uwYmCjXuHZDPycnK0xVG9QbTdb2tDNVaTBZqTHQ20EzwUy19ffMxtVds+fsqwZ7mKjUma40GC/X2Lq2m/FyndHp1rFBdoydmKhwzWW9TJbrdGbXGU++OM6LY2Vu2LaWk5MVRqaqtBULXL66k+PjFY6Pnz4nFxieqhARvOqyPl4Ynub4RJnNa7poKxQ4MVGm1khcNdjD8FSVExMVAti6tpsTExX6OktcOdDDyclWX9i8pot61neaCTat7mRkqsboTJXu9iK9HW2Ml1tjzhVruzk6VubUdOt809FWYKJcZ7pS56r1rfH25GSFvo7snDdV5ZoNfVRqDYYmKszUGmxZ2834TI2RqRobVnXQViwwMlVlqtKYva589sQkb9yymkIEx8ZneGF4mjdsWU251uSZ4xNsWdvFYG/ruvPpYxO8YctqIoJvHBnjR163keHJ1t9pJybKXL91DWMzNR4/NMpbrmxdcxwdneG5k1PcdNU6KrUmDx8Y5rY3tO5nGZ2pMjxZ5doNfUyU67wwPMXV63spFIK2Qmu8fm54ihPjFa65rI+R6Sqj2Xnlp9+6Laek4MIVKYcpuyPiLcCvpZTelS1/CCCl9O/nlHkoK/OViCgBx4DB9DIN2LVrV9q7d+85t0+SJEmSJEktEfFYSmnXQtvy+ujZo8COiNgeEe3A7cDueWV2A+/NHr8H+NuXC4kkSZIkSZK0tHL56FlKqR4RdwMPAUXgkymlfRHxYWBvSmk38AngjyJiPzBCK0ySJEmSJEnSBSK3OYpSSnuAPfPW3TvncRn4sbzqkyRJkiRJUr5ymaPofImIIeCF5W5HTgaAk8vdCGkFsK9Ii2NfkRbHviItjn1FWpyLpa9ckVIaXGjDBR0UXUwiYu/ZJoqS9B32FWlx7CvS4thXpMWxr0iLcyn0lbwms5YkSZIkSdIKZ1AkSZIkSZIkwKBoKd2/3A2QVgj7irQ49hVpcewr0uLYV6TFuej7inMUSZIkSZIkCfCOIkmSJEmSJGUMiiRJkiRJkgQYFC2JiLg5Ip6OiP0Rcc9yt0c63yJiS0R8KSKejIh9EfGBbP3aiPhCRDyb/bsmWx8R8bGsjzwREdfP2dd7s/LPRsR756x/U0R8I3vOxyIilv6VSvmIiGJEPB4Rf5ktb4+IR7Lj+7MR0Z6t78iW92fbt83Zx4ey9U9HxLvmrHcM0kUhIlZHxOci4lsR8VREvMVxRXqpiPiF7PrrmxHxmYjodFyRICI+GREnIuKbc9ad93HkbHVcyAyKzrOIKAL3AbcAO4E7ImLn8rZKOu/qwAdTSjuBm4D3Z8f9PcAXU0o7gC9my9DqHzuyn7uAj0PrpAr8KnAjcAPwq3NOrB8H3jfneTcvweuSzpcPAE/NWf4I8NGU0tXAKeDObP2dwKls/UezcmT963bgNbT6wu9l4ZNjkC4mvwv815TStcDrafUZxxVpjojYBPw8sCuldB1QpDU+OK5I8Cleem5finHkbHVcsAyKzr8bgP0ppQMppSrwAHDbMrdJOq/+//buPE6Oqtz/+PcxAVkFhBAhYQkaBfRigIBRBEUg7ATuT2S7bKJRQRBFkE3DpoCE7QrkyhKWsMQQFAKEJYAsgoSsEJIQEpKQheyB7Ps8vz9OldU90zPTW3X3zHzer1e/6lSd6qrT1VVdVU+fc8rd57j76Ci9TOFivpPCvv9gNNuDko6P0r0kPeTB25K2NrMdJB0uaZi7L3b3TyUNk3RElPcFd3/bQ4/8D2UsC2hRzKyzpKMl3RuNm6QfSBoczVL/WImPocGSDonm7yVpoLuvcfdpkqYonH84B6FVMLOtJB0k6T5Jcve17v6ZOK8AubSXtKmZtZe0maQ54rwCyN1fl7S43uRKnEcaW0fNIlCUvk6SZmaMz4qmAW1CVIV5b0nDJXV09zlR1lxJHaN0Y8dJU9Nn5ZgOtES3SbpEUl00vq2kz9x9fTSeuX//55iI8pdE8xd6DAEtTRdJCyTdb6GZ5r1mtrk4rwBZ3H22pL6SZigEiJZIGiXOK0BjKnEeaWwdNYtAEYDUmNkWkp6QdKG7L83MiyLtXpWCATXCzI6RNN/dR1W7LECNay9pH0n93H1vSStUr+o+5xVAiprA9FIIru4oaXPRjBLISyXOIy3lXEWgKH2zJe2UMd45mga0ama2kUKQ6BF3/3s0eV5ULVPRcH40vbHjpKnpnXNMB1qaAyQdZ2bTFarv/0ChH5atoyYDUvb+/Z9jIsrfStIiFX4MAS3NLEmz3H14ND5YIXDEeQXIdqikae6+wN3XSfq7wrmG8wqQWyXOI42to2YRKErfCEldoycNbKzQKdyQKpcJSFXUtv0+SRPd/ZaMrCGS4icDnCnpqYzpZ0RPF+ghaUlUPfMFST3NbJvoH7Kekl6I8paaWY9oXWdkLAtoMdz9Mnfv7O67KpwfXnH30yT9U9IPo9nqHyvxMfTDaH6Ppp8cPb2mi0IHiu+IcxBaCXefK2mmmX0tmnSIpAnivALUN0NSDzPbLNqX42OF8wqQWyXOI42to2a1b34WlMLd15vZLxV2qHaS+rv7+CoXC0jbAZJOlzTOzMZG0y6XdIOkQWZ2jqSPJf0oyhsq6SiFjhJXSjpbktx9sZldq3BRIknXuHvcAd25Ck8u2FTSc9ELaC1+J2mgmV0naYyiDnyj4QAzm6LQGePJkuTu481skMLNwHpJ57n7BkniHIRW5HxJj0Q3p1MVzhWfE+cV4D/cfbiZDZY0WuF8MEbS3ZKeFecVtHFm9pik70vazsxmKTy9rBL3J42to2ZZCBgDAAAAAACgraPpGQAAAAAAACQRKAIAAAAAAECEQBEAAAAAAAAkESgCAAAAAABAhEARAAAAAAAAJBEoAgAAAAAAQIRAEQAAAAAAACQRKAIAAAAAAECEQBEAAAAAAAAkSe2rXYCmbLfddr7rrrtWuxgAAAAAAACtxqhRoxa6e4dceTUdKNp11101cuTIahcDAAAAAACg1TCzjxvLo+kZAAAAAAAAJBEoAgAAAAAAQIRAEQAAAAAAACQRKAIAAAAAoPV4+GHJTFq4sNolQQtFoAgAAAAAgNbizjvDcPLk6pYDLRaBIgAAAAAAAEgiUAQAAAAAAIAIgSIAAAAAAFoL92qXAC0cgSIAAAAAAABIIlAEAABQGRs2VLsEAIC2wKzaJUALR6AIAAAgbQMGSO3bS9OmVbskAAAATSJQBAAAkLaBA8NwwoTqlgMAAKAZBIoAAABam2uukXbfvdqlAABUQ6U7s169Wlq6tHLrW7BAWr68cutrgwgUAQAAtDZ9+kiTJlW7FACAtmC//aSttqrc+rbfXtpjj8qtrw0iUAQAAFApPLIYAJC2Sndm/f77lV2fJM2aVfl1tiEEigAAAMphwwbpqacIBgFoHUaNkk4/Xaqrq3ZJ0rF8ubRuXWXWtWyZ1KNHcf3UrVmT33yjR0vTp+fOe/ZZafLkkJ4/X3r00dzzXXUVT0yDpDwCRWa2k5n908wmmNl4M/tVNP2LZjbMzCZHw22i6WZm/2tmU8zsPTPbJ2NZZ0bzTzazM9P7WAAAABV2883S8cdLTzzR+DyFXoAPHy795CcEn9By3Xln2O9ba7ChNTv+eOnhh6XZs6tdknRsuaV02GGVWddLL4Xf8yuuKOx9w4ZJm2wivfVWw7z33pN22UVatCiM77uv1KVLSNc/ZxxzjPTVr4b0scdKp50mzZvXcJlXX11Y+dBq5VOjaL2ki9x9T0k9JJ1nZntKulTSy+7eVdLL0bgkHSmpa/TqLamfFAJLkvpI+pak/SX1iYNLAAAALcK4ceGmd/jwhnkffxyGuS6+i3XoodJ999FpJ1qu3/wmDCtVcwMoxGuvVbsETXvppTB8442GeddfL82YIb34YmHLjJtsrV9fWtnK4cUXpeeeq3YpkEOzgSJ3n+Puo6P0MkkTJXWS1EvSg9FsD0o6Pkr3kvSQB29L2trMdpB0uKRh7r7Y3T+VNEzSEWX9NAAAAGkaOjQM//734t5PzSAAQNpaSvOxww+Xjjqq2qVADgX1UWRmu0raW9JwSR3dfU6UNVdSxyjdSdLMjLfNiqY1Nh0AAKD1iINBvXpJF12U3noGDpQOPji95QMAWp+W8ofF6tXS2rXVLkWblXegyMy2kPSEpAvdfWlmnru7pLLscWbW28xGmtnIBQsWlGORAAAAlTdkiHTLLdnTcv3LO2BAmL50acO8WK4L+1NOkV59NRnfsKHl3AAArcXixdzMIj1N/aYXm1escePC+apSNt1U2m23yq0PWfIKFJnZRgpBokfcPa5rPS9qUqZoOD+aPlvSThlv7xxNa2x6Fne/2927u3v3Dh06FPJZAAAAKmvt2oY3iflU+R80KDxRSJL+/OcwjPs4KsaKFVL79tK11xa/DACF23Zb6f/9v2qXApW0cmXL6Zw91/mo/rRx46QlS5pf1l57SWecUXgZpk8v7mlvUtKRel1dKCcqJp+nnpmk+yRNdPfMv8WGSIqfXHampKcypp8RPf2sh6QlURO1FyT1NLNtok6se0bTAAAAWqYtt5S23z7/+eN/eU86SerePf/3NRd8+uyzMPzrX/NfJoDyeOaZapcAlbJ6tbT55tJvf1u+ZU6ZEn7j33yzYV793/7rrpPuuit3Xi751Czaay/pkEOan6++kSOlNWuan69LF+nrXy98+ZluvjmU8+23S1sO8pZPjaIDJJ0u6QdmNjZ6HSXpBkmHmdlkSYdG45I0VNJUSVMk3SPpXEly98WSrpU0InpdE00DAACorLo66ZJLpJkzm5+3KWvXNvwnNteFebk6Fv3b34qrebRyJc3SANS2ww8vLPBeDStWhOGDDzY9XyGGDQvDhx9umFf/d/v3v5fOO6/5ZTZ1zsl1LohruOZr2jRpv/2kCy5omFdXF2rK5lNLKV8jR4ZhKTVvUZB8nnr2L3c3d9/L3btFr6HuvsjdD3H3ru5+aBz0iZ52dp67f9nd/8vdR2Ysq7+7fyV63Z/mBwMAAGjUiBHSTTdJp55a3PsLDbqUGqSJ33/yyVKPHoUtf+bM8A/4HXeE8WHDpP79SytPpb31lnTVVdUuBYA0vfii1JL7qHWXLrssnWBGrsBP2n0UrV4d1pvrt3dxVN9j5MiGec89J/3ud9KFFxa2vnnzpCOPTJadr08/le67r7D3oFkFPfUMAACgVYj7l1i/Pnde377SsmUN8/KpGVRq7SGz3P/SxubOLWzdU6eG4eDBYdizp3TOObnf36uXdMAB+ZWzkg44QLr66mqXAqiuqVPDMf7WW9UuSdM++ih05t+SjRpVeMDivfekG26QTjwxjC9cGLZFLqedJnXrljvvmmukK6/MnlZs4CefPooaE58D77yzsHWuXh2GTT2gIZe+faXnn88d9Gnq8591lvSTn0jvvhvGzaSf/zz3vE88UXggz126/fbwfbYhBIoAAAAyPfOMdPHF0m9+0zCvnBfrTfnLX4p7f6n/Ig8ZUtxN6CuvhJskoNomTZLGj692KdIRN1EqZ7OnYi1cKL3+eu68r341BJ0r5ZxzpEsvLe8yu3eXvve9wt4T/wGxbl0YdukifeUr2fPEv9GPPpoENurr00f64x9Duqnf/mL/lChnM+RCl7VkiXTccaH2ULnEf57EASqp8f76fvjDwvoHlELQ8MILQ0CqDSFQBAAA2q5cF7mrVoVh/G/oSSc1rGVTaP8PxZal1GWWexmNOeQQ6ZvfTG/5QL523136xjeqXYraMmRI+M368MPm5501K3dtSknabDPp+utD+pBDGg+kVPqJYP37SzfemDvv4INDf3RSCFI88EDu+e68M2yjlSuTae+/X1q5li9P0mkEdfL5TW9qnjTOY83p3196+ulQ86oQ5ernTyq8ZlDcYfenn5avDC0AgSIAAICmDBrUsJZNpfooKvTiuJiL6fiJaS2Fu3TRRdLkydUuScu0enWosZD573s+/vlPaYstcu8vdXXZzUyefjp0dpvL889L8+eH9OzZ0ssv557vpz8NgQmU7m9/C8MRIxrmPfBA+N2Ia2XstJP0rW/lXs6qVdLll4d0S6lB+OqroT86KdQ82lz62wAAIABJREFUOvtsafTohvPFgYtFi9IpR6mBl1qtWVTOAE5TKv0whuXLQ9C0DSNQBAAA2q74InfMGOnZZ/Ofv9R58n1fOS+Ocy1/7Fhpm22kRx5p/v0LFyZ9dsybF/rhyPzHvFImTZJuuUU6/vjKrzsNdXXhuyn0H/Zi3XZb6APl9tsLe98114QnPo0Z0zDvyiulrbZKgkjHHZe7Vs+6daGz2kMPDePduiXp+u69N6nd1xrNmydt2FDtUiT9wWQGXidOrE5Z0jZnThiWc796883whK98Ffs7X2zNolL6KKpFhZb9k0/y7+j6b39LgtgHHhiCppna2JNDCRQBAIC2K77w22cf6Zhj8p+/0HnK0cF1GhepcR8ZL77Y/LwdOkjbbhvSV10VOsceMKC09W/YkPtzuUvnny+9807uPKnyzVvSEgcLfv/7yqwvvkkutEZRUx57LAwzm2ZkNuGJxd/ZpElhWEudww4fXrlaagsWSF/6UnhCVktQzmaxs2fnrtFTbU19jrfeavwJXt/9bnjCVznWU4kapM2VoZzSWE+hyzzqqNDRdRwAasz8+eGposcdF8bHji2ufK0IgSIAAND2pPGPaqnLTLuPomKMHZvuutu3D01B6lu9WrrjjqY7k21j/+7WjEo1u6y0Hj1CJ8y5fPe7hdUaidXV5f78cYDs6acLX2ZLt8su0r77Fv6+yy/P/YCBSjjggMJq4BX7Z0GhQaSW2EdRU9Lo3y/uNDtX7b1165KmhnEH5DNnNr6sllwTqwgEigAAQNuTRs2gNC6sq9n/w+DB0t57SwMHFr6sxv69nTIlPJ0oc33xE5wWLpReeqmwdbUGLSWQko/mgp0t9bO++WZSa+Teext2ehxbuTKplbR+vdSuXdKRcjk0tf0+/TQ8cn3JkvKtL5bP79CAAaG2UHPq37DPn5/ffnH99dKttzY/X1O/P01J+7c2zX2/2OZlaR+PtfiHTKZTT5W22y7/+Vvq71eRCBQBAACUU1p9FFX6IjXup2TChMLed8stUseOISgkhb5qrrgipI87LjydKM7L1LOndNhh0tq1ybR8/g0fP1565pnCytictWuTf5iRrVLNY5ry739LTz1V/uXm49prw3DBgoZ5J50UaiWtW5fsP3fc0XC+5o7lV17JPg7y0bdveOR6rvXVN2VK+R5P7h46Mj/jjHD8FuLDD8NvRa7aOqtXh+anhT5p6v77wzJz9aVVKeU8BxSrnE3cil1PoSq93QYPTnf5LRyBIgAA0PYU2yl1qVX903hfOZZfzgvh558Pw/ipV+PHS3/6U0g3FXwZPz4M890O8Xzf+IZ07LGFl7Mpm24qde5c3mU2ppZuQubPl3784/z6L5oxo2HtleY+S3P5Y8bkt+7vfKc2OzOP+/oqpf+sUaPCo+cvvrhhXv3tlxmQKkTXrqGPpHw1d0zGtYTizqLrv/f223MHpj76KAxfeKFh3kMPhaBXoX13xU/RyxXgrmaNkDSP8zSarJVDa2mmWqvlShmBIgAAgFwKvThM49/Q1vro4XzXV+oTe4YMya+GRX11dcU1X6m0Z54p7xOcLr441MjIp7nhLruEpomZGuuYvKn82Jw5oVP53r3zK2sxxo1LOtIuVXP7cLGB2bj/og8+CMMXXgiPsM+1zM6dpS22yL2cFStq46lxEyeGjqBPOqmw98XBp1x9y0yYELZhpTofb63K0ZS6FpqXtdFATtoIFAEAAGSqdOCmUhe5tdrcoND1FLLuXr1C85VaVuy2HD061KT61a/KWx4p/30lrjWWj8zPmWv5S5eG4fDh+S8zLoNZ6EeoOXvtJe2+e2HLL0Qax8URRzTs8D3efvPnN95EbYstpB12KE8ZSvntiGs85WpCVuz2ip+2mKvpUK02uap0H0X5rLeafRRVqlzl+J5qqdZnBREoAgAAbUNdXX7NNFpi07E0n36ThlKbKDVl0qTSmv60FPGNd9x8p1ijRoWaNrk884w0YkT+y8q3Blihx8qsWWE5Dz3UMC/uAD2udZNpxYpQmyVXx9NNefrpsL74iUi5FLoPv/lm0r9RqfLdfnHTwDR/m1pLbY40mxW3VS3xTxD8B4EiAADQNpxwgrTxxtUuRUPlvMit1AVzqesp5elYTa17woRQY6RcN+SV0Ny2nDMnPEGrEKtXhyZccTOmXOJt3L17qGmTy7HHSvvvn/t9TS2z3OKO1eOaJPm6+ebQP85ttxX2vr59w/D99xufp7maB/Xzv/td6Q9/yL2s//7v4vtNq4S0vtdiP08tBL0LrS1Tre+uFpo2F7ruQoPLtbA/tEIEigAAQNswZEjDaeW8oK+FC9JiL5jTaEZR6LYtx/abOTMM33qr+Xnnzg01Vcrp5z/PXbOlKU1ty0WLpB13zN2xcVMefVS65x7p0kvD+MiR+W2TYtXqTXNcgzBXPzfu+T3OvRDFftZ//KP45eejFn6bckkj6NjatlG1lCM4WM5zTrGq2ddSC0egCAAAoJoKfcpX2uspRDk7HW2uD5tS17N+vXTNNdKyZWF8hx2knXYqbT31/fWvDfuTid1wQ3isuxSaQsWdFdc3YEB46pUkLV4chs88U1g54s8fD/fbTzrggOx5yhkQraX+WvJ1332hM+iRI4t7f75Bz0L72cpn29RSU9Fy/H4V+nmq1Vdcaws6lOM4rFQNw1L/ZGgp30kNaTZQZGb9zWy+mb2fMe0qM5ttZmOj11EZeZeZ2RQzm2Rmh2dMPyKaNsXMLi3/RwEAAChQpS8e0/6XvNinhKXR6Wgp2zaNQMKgQVKfPkktm0wbNoQaOIU28SrEZZeFx7pL0o9+JO2xR+6OiM84Q3rllfTKUapa7cOrUK+9FoZxs7ZCVbOT4FrblqUqtCZWpWpOFquanTHXgjQCN2n20VeOMrRC+dQoekDSETmm3+ru3aLXUEkysz0lnSzp69F77jKzdmbWTtKdko6UtKekU6J5AQAA0jNjRtMdWNfCTW/awapqddJajaZnTS1rzZowzNWxcb9+oU+fO+8sXxma8vLLYbhhQ+u4+ahmjZBa336FBm/T6LenVNWoNdQSm56l+d3VwmPo811GmuecUo6dWv+tqDHNBorc/XVJi/NcXi9JA919jbtPkzRF0v7Ra4q7T3X3tZIGRvMCAACkY+lSaZddpJ/9rGFeLXYs2tz7Sr3ITaPvpbRu+PNZbrH/MOfKi59utTjHJW9dXXb/PiNHSlOmNF++QuXzmefMkcaMyZ3Xt6/0k5+Ut0y5lKPpWaGqdYNXbOCjlBvWWmx61pS0g0jlrCVZam2jzHkKbVJYqrSW3dhy0+qjqNK/7YWstzm1dNxVQCl9FP3SzN6LmqZtE03rJGlmxjyzommNTQcAAEjHihVh+NxzDfNa2uPky6HSzTWqsY3S6Nvo5ptD/z7xY9j320/q2rW09eS77vp2203aZ5/ceRdfHPrdSVtaTTha0n4mpd/0rJwdhFcqyJxGjZpi95Vq1h4pZw27Wj/XpPHd1aqWXPYiFBso6ifpy5K6SZoj6eZyFcjMepvZSDMbuWDBgnItFgAAoKFKN/OohaZulVp+octsrgPgpuYvdj35mDAhDGfObHq+YhWynVavrs56pdJvZt3TCQaleWwUWyOimOXWV+lgUKFlKXS+Wm06VeyyKvUUyZYYnCi1j6I08krZjrUerEtJUYEid5/n7hvcvU7SPQpNyyRptqTMx0d0jqY1Nj3Xsu929+7u3r1Dhw7FFA8AACA/hd7YVPOCMY3mKi35aTFpBN3KeaNx991heUuX5jd/rW7nWNr9ctVCJ8T5rLeaneJWutZVqc2E8p0vjZv/Wuj7rVCFbodaCCJVuo+iUqTZzK4VKipQZGY7ZIyeICl+ItoQSSeb2efNrIukrpLekTRCUlcz62JmGyt0eD2k+GIDAACUoNgboDSacFTzSTy1eEHf3PpqPaASu/XWMJyd87/R6qu17djSmp6VQzlrTqQRaCtnrbNi1dKTytyrF1yv1T6KainIUo7AYS19nipr39wMZvaYpO9L2s7MZknqI+n7ZtZNkkuaLulnkuTu481skKQJktZLOs/dN0TL+aWkFyS1k9Tf3ceX/dMAAADUV6mATz7LTuOGrbmbl0rftJRjfWk2xShn7YrnnpOOOkqaN0/afvvSylXoflrqdqhGEKClBYPS6Iw33/lqqe+Xctxk10LTs0rVBmvJgblyqMWaqgSACtZsoMjdT8kxudGe8tz9j5L+mGP6UElDCyodAABAtZTzH/o0+lzItzyl3rxxQR/kKtdtt4XhmDHS4YeXb7m1qByBqWr0L1KKtAIyaR5TaTSZrWZzu0rtF+Xoo6jS32st9G1US03PSnnaYD7LaCm/1WVSylPPAAAAal85L+grHVjJvDBNo4PnYi/oq3njWc4aHbWk0P201O+g1m6IWuJ3Vl++x2upx3ItBU8y11frTa7y3W75NMeqdE3VNN6XtnKej0oNPpX63bdBBIoAAEDL9uij0kcfhfS0adIjj2Tnl7MZQKU7oS2HWr95K3Q91dqWtdbMpdLbobU2PWtJHdoXq1K/W4Vuy0r3+VYLQYA0ylBKMD/NPoqq2dl4GlpimUvQbNMzAACAmnbaadKWW4anS+2/v7RwYZhWLZW6Ech3fWk2PavVIFStlquSyy1FNW6Iaq2GUynrrWaH7NXoCyjNWn6V2hdrvelZvmVIQxp9JxUa3CpHbV6anhWEGkUAAKDlW7YsDBcuLN8yi70RmD1beu21wt/XXBmq1ayhnP/Ql+NJTrXU9Cztf8wrHaSLpVGTq5SAZqzaHSK/955UV5ffvNWq2VLppqLulf88TUmzyV6+TZQ2bJDGjSu8DIWqhcBFobW6Kv0giHw09722sZpEMWoUAQCA1qmS/eh8+KG0alVI33hjeMWefTbUdiqn229P0mvXSuvWNf+eWn+iTjmCE7V0Qd9Y2R9/PNxIxvOU88lXUmh+Ge9vEyZIv/99kvf009Inn+S37jSkHWwq9zJnzQrBodg3v5mkr71W2myzZHz+/NzLKDZwsWKF9Oqryfinn4ZjvZhl1UJtnkoFPRv7rH37SgsWNP9+s/wCHe7Z+8bPfpYcd++/L+21V5L3pz9J06cn7yv3flxKs75K17JJs3+uUlCTqAECRQAAoO0q17+HX/ta43nHHJOkb7hB+stfkvEFCxq/CF2xIklfeaXUPuOy7cILk/SmmybpmTOzy79mTRKUaMrq1dKkSdnT4nKNHSv9scEDbYO0mnjVUsCn1JuWt95K+tCSpB/9KEnvtFNy879unXTmmUneBx+E7zOXmTOlzz4L6enTpUsuSfJ22y1Jv/12eMWOOy5JX311eGW+b9GiZJnDhiV5AwcmtfaWL8/vhrsp5Wy2U+6buI8/TmoNnX564/P175893rFjks7cdvXLt3attH59SK9YIT38cJJnJn3hCyH9r39JBx+c5H3xi0n6jTekk09Oxu+5J9nH1q0LQaXY6NFJEHvJkvD5Yk8/La1cGdLz5klTpmSXNd8aNKXW8kujZt6IEclnk6SLL07Sxx0XfhtjF1yQ7PsrV4Z9PJfXX0/e99ln2YHDu+9uvCxXXJGkhwyRPpfRqOeNN5LgbbHSCNaU0nyy1N/voUNDkDuWuQ+vWRP243zl81mPOio5Ty5fLl1/fZL35JPZx0wbQqAIAAAgU5r/Hr71Vvb49tsn6dGjs2/At9giSd91V3Hr22STJP3GG9kX8I89ljSPePzx8Iplzvfss+GVK2/AAOmllwov18KFoV+pTPFyp0ypbKAoc12TJ0v335+Mv/VW07XBGttXjj46uen+yU8af//ixdnjDz2UpPfYI0nX3yY775ykp06Vbrqp8XXka9q0JJ0ZQJKkU05J0r16Zedlluvxx6Vttml+XcXWHqu/X3zwQXZQpBj1l7nrrqUtT8reRqtWSdttl4x//vNJesaMhsGofGofTpiQfSPdu3eSrh8A3HffJH3eeeEVywwcnnhi9joyt8s772TvH5kBmPo1ZDLzPvkk2cfXrpX++c8kr34woBwOPDA57m67rfH56h93mcH7+kH/bbdN0ocdVlr5cjnooCT9xhvZ/evdc0+y3T/4IPzexq6/Puw/UghgxYEuKWzXfJtI5sM9qRElSX36JOtbsCCMxwYNCjWq4vfFAZ5YvF8tWiS98krD6VL2dCl7H/6f/8nOGzkyqVG7bFnjzc/vuCO7BmDm+l58MUkvXy5dfnkyfsIJSXrixHDMPP549nHcShEoAgAAbVct1Vw5++zKru/UU0tfxptvZo9nbs+f/Sz7BvKnP03+Of/udxt/3957J+m1a7PzLrww+be/kIDen/4UbrQk6YEHwiuXxx4Lr9gBByTpZcuSGzMp3GzGZfjFL7Jrbg0dmn/ZWpNLL80ez/zupkzJrkmRr3HjpDlzQnrYMGnjjZO8zGDaww+Hmk+xO+4IN3ZSqIUwd27u5Z9xRnilKfMmviWqH0TafPMkPWhQEpRYsiQ7r1OnJD1pkvSDHyTjRx+dpJ97Lvt7veeeUJNRCsdVY8HAYcOyA1j/+lfzn6VQ9YNKaRo3Lrtvo8wAYP/+2bXYMoMZ/fplL+frX0/SffpkB+WffLLxWqaZQf+bbkp+3xYtkrp0SfKuuSZJz5qVPX7SSdnrygzO3HKLNGpUSD/8cHZtumLtt1+SPvTQ7LzM35/GasUWYunSUAvv7bel732v9OXVOPMabnPXvXt3HzlyZLWLAQAAallmnxGZ6blzpR12CLV25s3Lzhs+XOrRI1xkvvNOdt7jj4fmQT/8YUhn5t10U2jm07lzaAqS2UcFqqtHj/DP8qhR0p57hn+eM2+mUFs6dgzHpZQ0s4trbHzuc+WtEVHfF74Qajrk07cXakvHjiGAXGpNMkj77BMCdJnNU9G8V19tNYEiMxvl7t1z5VGjCAAAtD5pP5561qzwQu3IvNmZMIEgUa2Lg0SStPvu2XlpBomk8ncuj8rJ3G9QmtGjq10C1LAi6n8CAAC0IKU8EQYAAKCNIVAEAABalo8/brqj0vrKFRR66aXSn04DAABQ42h6BgAAat/NN4f+hA46SDr88NAx6imnZD+SOlMaTc/SeOINAABAjSFQBAAAat9vfxuG7uHJOlL+/ZgU2x8RzdMAAECmNnJtQNMzAABQG846S+rbtzzLyryQayMXdQAAAOVAjSIAAFAbHnwwDOPaQ9X2xBNSu3bVLgUAAEBFNVujyMz6m9l8M3s/Y9oXzWyYmU2OhttE083M/tfMppjZe2a2T8Z7zozmn2xmZ6bzcQAAQKvw6afSPfc0P1/atYXSfkw3AABAjcmn6dkDko6oN+1SSS+7e1dJL0fjknSkpK7Rq7ekflIILEnqI+lbkvaX1CcOLgEAADRw9tlS797SmDHFvd89v76J6geaXnlFmjatuHUCAAC0As02PXP3181s13qTe0n6fpR+UNKrkn4XTX/I3V3S22a2tZntEM07zN0XS5KZDVMIPj1W8icAAACtz7x5YbhqVfHLKKa20SGHFL8+AACAVqDYzqw7uvucKD1XUvxs2k6SZmbMNyua1th0AACA4pXa9KzYJ6IBAAC0UiU/9SyqPVS2DgLMrLeZjTSzkQsWLCjXYgEAQGvTVJAo36ZnI0YQLAIAAMhQbKBoXtSkTNFwfjR9tqSdMubrHE1rbHoD7n63u3d39+4dOnQosngAAKBNSLszawAAgNjjj0sTJ1a7FKkrNlA0RFL85LIzJT2VMf2M6OlnPSQtiZqovSCpp5ltE3Vi3TOaBgAA2qq1a6WBA9MJ9jS1zH79pKeeajwfAAAgl7vukp58stqlSF2znVmb2WMKnVFvZ2azFJ5edoOkQWZ2jqSPJf0omn2opKMkTZG0UtLZkuTui83sWkkjovmuiTu2BgAAbVSfPtINN0hbbikdfXTl1nvuuZVbFwAAQAuTz1PPTmkkq8FjQaL+is5rZDn9JfUvqHQAAKD1mhk952JxCf8d5VMbyV2aM6f5+QAAAJrTBvo2LLkzawAAgLz16ycNGJD+ejIDSAsWSDvumP46AQBA69cGAkXN1igCAAAom7jZ1+mnl2d5dGYNAAAqqQ0EiqhRBAAAWp/ly6V586pdCgAAgBaHGkUAAKD12W67apcAAAC0RtQoAgAAqIKmLsIy82h6BgAAKolAEQAAQIGmTZM++iik58+Xjj66tCeb5WvRovTXAQAA2jYCRQAAAAXabTfpK18J6ZtvloYOle65p/H5S7ngyqxRRHMzAACAkhEoAgAAAAAAyMeAAdUuQerozBoAAKQvjb6Exo+XPvig/MsFAABozLvvVrsEqSNQBAAA0pNPs7Jig0g9exb3PgAAADSKpmcAACA95a5JNGdOeAEAACAVBIoAAEBp3KVBg6T169NZ/po1SXrHHdNZBwAAACQRKAIAAKV6/HHppJOkP/+5YV6xTzT797/D8LTTpE02Kb5sAAAAKAiBIgAAULihQ6X77w/pBQvCcPbshvPl0/TMTFq+PBnfYYckPX160UUEAABA4QgUAQCAwh19tPTjH2dPK7Y/opEjpf32S8bnzi2+XAAAACgJTz0DAAClaap5WWN5774r/etfIX377eUvEwAAAIpSUo0iM5tuZuPMbKyZjYymfdHMhpnZ5Gi4TTTdzOx/zWyKmb1nZvuU4wMAAIAWoH5to27dpI8/rk5ZAAAA0KhyND072N27uXv3aPxSSS+7e1dJL0fjknSkpK7Rq7ekfmVYNwAASMvSpUm/QzNmhNpBjz7acL5im5wBAACg5qTRR1EvSQ9G6QclHZ8x/SEP3pa0tZntkGsBAACgBnz961LnziE9blwYPvJIccu6/PLin4AGAACAiik1UOSSXjSzUWbWO5rW0d3nROm5kjpG6U6SZma8d1Y0DQAA1KJZs/KbjwAQAABoK7baqtolSF2pnVl/191nm9n2koaZ2QeZme7uZlZQffQo4NRbknbeeecSiwcAAKrm/POlO+6odikAAADKZ+zYapcgdSXVKHL32dFwvqR/SNpf0ry4SVk0nB/NPlvSThlv7xxNq7/Mu929u7t379ChQynFAwAA5dZUf0T18wgSAQCA1mbXXatdgtQVHSgys83NbMs4LamnpPclDZF0ZjTbmZKeitJDJJ0RPf2sh6QlGU3UAABALWuqeVmct2iR9MILlSkPAAAAUlFK07OOkv5h4eKwvaRH3f15MxshaZCZnSPpY0k/iuYfKukoSVMkrZR0dgnrBgAAaZg4Ufryl6WNNy78vY8/Hl4AAABosYoOFLn7VEnfzDF9kaRDckx3SecVuz4AAJCy2bOlPfeUfv5zqV+//N6zerW0YUO65QIAAEDFlNqZNQAAaC0WLw7DN97I/z2bbppOWQAAAFAVJXVmDQAAWrh166QlS/Kf/913pRNPTK88AAAAqCoCRQAAtGW9eklbb509rX7H1SNGSNOnh/Qnn0iDB1ekaAAAAKg8mp4BANCWPfdcw2nu0po1yfj++1euPAAAAKgqahQBAIAgrkk0fry0ySbVLQsAAACqghpFAAC0NevXS+3aZTcx+93vpM8+q16ZAAAAUBOoUQQAQFuybp200UYhMJTpz3+W7r67OmUCAABAzSBQBABAW7J6dRj26xdqFgEAAAAZCBQBANAWLV8eahYBAAAAGQgUAQDQ2v3f/0lf+EJ4mhkAAADQBDqzBgCgtfvFL8Jw0iTpgw+qWxYAAIBMO+4offJJSPfsKa1ZI732WnXL1MZRowgAgLZijz2kE06odilQbj16SPvvn4xfdlmSvu8+6de/TsYfeyxJn3uu9MUvJuM33JCku3aVOnYsf1mBlu5730vSO+/c+HxbbJF+WWrdlluGAEDszDOT9LHHSltvnYzfeGPlyoXmHXhgkv7856UzzkjGn3pKOuWUkH7jDemll0K6R49Qc3nQIGnTTaVVq6QFC6Sf/1x6/vkQ/Bk+PPx5tXZtmPfRR6VZs6TZs6Vly6Tp06UXXpBefTVM++STMN+yZdKpp0oTJkgbNiRlWbMmKevee0u33ZbkZR6r5fTOO+kst9a4e82+9t13XwcAACUKl1m8WtLLzP2oo9z79HH/2c/c161zX7So4Xf73ntJevp099mzQ3rtWvd//MO9ri6Mv/+++8yZIf3RR+7XXx/yFi9232479/79Q95hh4X1f/ZZWIbk/rWvhbx99w3jb78d8uOyvv12ku7d2/2//zsZP/304j7/Zps1nvelLyXp73ynuOXvsEP1v2Ne6b5+85tkP7rgAvfzz3fv0MH9xBPd+/Z1v+ce9/33d5861f3ll92HD3efMiU5niZMCMeAezj+Vq9O8p580n3NmpAeNsz98ceTY0tyf/fdMP6Nb7h36xbSt90W8gYPzv5ddnc/9NCQvugi9xdfrOx2io/Xs85y32UX91decR80yP3b33Z/4w33cePcp00Lx/l777n/+9/u77zjPmNGOL4XLQrbRnLfe+/weW69NYyPHh22U+Zn3WOPkB4zxn3OnOy8OD19uvuddybjGzYk6eefzy7/E08k6X79svP++tckvXJl9r7x2GPJeFx+yf2KK9zvvz8ZX7UqSV92WfLZ6r/v3nvD76/k/r3vuU+eHL7rnXcO+8u6dWE/eeaZsD0nTXL/xS/Cb++qVe5XXul+7rnuDz0Upm+0kft557mPHx+WeeGF7iNHum+/fRi/8Ub3p58O6Ysvdn/99aQsr72WbOdXXw37o+R+7bXh+zrySPcePcI2nzgx5E2cGMZ/+1v3a64J6WXLwrli/fowPn58ch5Zvz6Mx5YsyT5G0rZwYdiGcVkuush91qwwfsUV4bzp7n711eHz3X57KHvm/rbffsl3F3+XX/lKyOvf333jjcP2WrEi5PXsmRz3rYSkke65YzE5J9bKi0ARAABF+uUv3YcMCTdI1b5hq+XXIYc0nnf++dnjL7yQpP/2t3CjE4+/9FJQ/8VcAAAMRElEQVSSPvpo94MOSsY7dUrSF1+cXMDfcIP7//1fuPGIAzXu4cY1DvBU2vr14QI8Nnx4Mr54cbhZjPXrF27o3N3/9Kdwk+meXIyfe24Yjz/7jBnuf/xjSHfrFm4sMi/af/Ur/89NXOb73JP9+LbbGuZ9//sh/eST2Tdu7u7bbhvS77zTcH1xet68ECiIx+MAmeS+fLn7Oeck43EgQAqBuMz9I76hk7IDaVK42c617r32CjeI8fjYsUn6yCPdTzklGe/bN0nvs08SXIg/e5w+9tiwf8bjjz6apP/rv7LHr7kmu5y/+U3jx8O3v52k44Bi/NpppyTdu3fjy3juuSR91VXZec8+m6RPOy1JH3RQuHmOxzMDA5984v7WWyF99dVh2z7yiPvAgcl+unx5UYdC2W3YEIIc8bE9YUJy/CxbFgIjcd7QoUkQ+MMP3S+5JAQe5swJ2/7118Pv+8svhxvkgQPdR40Kv2evv+7+l7+EfX7AgBCc+OijsKz6AeeVK0O50jZrlvuCBcl2GDcuyXv33STAvXq1+4gRSV7fvuH7dU9+q9evD0EtKQS5Fi/OPrZ++cuQXrgwfPe77x72LfdwzPbuHbbD+vXul18efm/dw/dx880hr64u/CbHeRMnuv/+90nesGHhdyOWma4V9YObbdl77yXH1tSp7h98ENJ1ddnHZBtEoAgAgLamuQBJa3/tvHOSnjHD/e9/d998c/dTTw0X9b/+dcibMyf5V/b73w95l1wSxlesSP7JPuaYsF3/8IcwvmFDuLj8wx+Sm55587JvcgYODDcx7uFGaMyYyu4D1TR+fFIbY9gw97vuCuk4YHLnnWE88wZv4MCQvv32hnkDBoT0sGEN8x55JKTjm+Fu3ZJg0+TJ7j/9afKP+MMPJ7U9pkwJwbrYT3+a5MXBJ3f3Bx4I6c02a7juE0/MHs9Md+kS0l/6UsO8OH3CCdmBqcy8m27KrgWRmTd6dMO8uGbI+vVJUOzXvw55d9/tfvDBIV1XF76PJUvC+JtvhkCTewi8bL11CFQsWhS2w157hRoQc+eGZcycGW5Cr7zS/brrwvLiIM+HH4ZxKQTp3JO8+Fg44ogQWHRPAkCrViWf7/zzQ/q119x32y0p8wUXhGCIe6hJ8MQTyXe3fHmbvtlrc+rqkt9n9xCEio9xAHlrKlBkIb82de/e3UeOHFntYgAA0LKsWydtvHG1S5G+E06QNtpIatdOuuMOaZNNpM8+k9q3l7bfvrBlrVoVltU+x3M+Fi8O/Y20hW1aCfPmhe/HLPT1sGyZdMghIeTx/PPS4YdLn/uc9PHHYZ64H5iPPpK+/OWQnj07fB8dOoTxDRvCfpCGujrpL3+RevcO/W5cdlnYz/r1C/1rnHaadP/90llnhT4yPvc5adQoaeJEac89pblzQ39Pm28ude8eOmh99VXp4IOlTz8N/bTceKO0++5Sr17hc551lvT66+HzX3dd6GvjwAOlJUukV16p/b7G6upC2c2qXRIAQCPMbJS7d8+ZV+lAkZkdIel2Se0k3evuNzQ2L4EiAADytG6dNHNmuOncdttql6Z0ZiFwcN114TPtuWfoeHmnncLN+uc/H27IgWpyl8aODQEiAABakKYCRTn+Nku1IO0k3SnpMEmzJI0wsyHuPqGS5QAAoNU59tjwpJCWpGtXafJk6e67pc6dQ0CoU6emnyQE1BIzgkQAgFanooEiSftLmuLuUyXJzAZK6iWJQBEAAMW6557aDBJdcIH0zW9Kf/hDeITu6tXSrruGx+USDAIAAKhJlQ4UdZI0M2N8lqRvVbgMlXfrrdLgwcW/vxzNA0tdRmsoQ2v4DJShPO+nDOV5P2WonTJMnVr4e77xDen996Wbbgr9rOy4o9Stm9Sli/TVr0pPPy394hehf5Wdd5Y++SQ0+9puO2ncuNCfyoYNoV+fDRukzTZrfF0//nH2OEEiAACAmlXpQFGzzKy3pN6StHNruZDcaKOmL6DzUY7OAEtdRmsoQ2v4DJShPO+nDOV5P2WojTJkBoouvVSaNk36znek//mf0EnzDjs03Z/Pb3/bcNqBB4Zhly5h2KlTkkdTGwAAgFarop1Zm9m3JV3l7odH45dJkrtfn2t+OrMGAAAAAAAor6Y6s67040JGSOpqZl3MbGNJJ0saUuEyAAAAAAAAIIeKNj1z9/Vm9ktJL0hqJ6m/u4+vZBkAAAAAAACQW8X7KHL3oZKGVnq9AAAAAAAAaFqlm54BAAAAAACgRlW0M+tCmdkCSR9Xuxxlsp2khdUuBNACcKwA+eFYAfLDsQLkh2MFyE9rOVZ2cfcOuTJqOlDUmpjZyMZ6FAeQ4FgB8sOxAuSHYwXID8cKkJ+2cKzQ9AwAAAAAAACSCBQBAAAAAAAgQqCocu6udgGAFoJjBcgPxwqQH44VID8cK0B+Wv2xQh9FAAAAAAAAkESNIgAAAAAAAEQIFFWAmR1hZpPMbIqZXVrt8gBpM7OdzOyfZjbBzMab2a+i6V80s2FmNjkabhNNNzP73+gYec/M9slY1pnR/JPN7MyM6fua2bjoPf9rZlb5TwqUh5m1M7MxZvZMNN7FzIZH+/ffzGzjaPrno/EpUf6uGcu4LJo+ycwOz5jOOQitgpltbWaDzewDM5toZt/mvAI0ZGa/jq6/3jezx8xsE84rgGRm/c1svpm9nzEt9fNIY+uoZQSKUmZm7STdKelISXtKOsXM9qxuqYDUrZd0kbvvKamHpPOi/f5SSS+7e1dJL0fjUjg+ukav3pL6SeFHVVIfSd+StL+kPhk/rP0k/TTjfUdU4HMBafmVpIkZ4zdKutXdvyLpU0nnRNPPkfRpNP3WaD5Fx9fJkr6ucCzcFQWfOAehNbld0vPuvrukbyocM5xXgAxm1knSBZK6u/s3JLVTOD9wXgGkB9Twt70S55HG1lGzCBSlb39JU9x9qruvlTRQUq8qlwlIlbvPcffRUXqZwsV8J4V9/8FotgclHR+le0l6yIO3JW1tZjtIOlzSMHdf7O6fShom6Ygo7wvu/raHjtYeylgW0KKYWWdJR0u6Nxo3ST+QNDiapf6xEh9DgyUdEs3fS9JAd1/j7tMkTVE4/3AOQqtgZltJOkjSfZLk7mvd/TNxXgFyaS9pUzNrL2kzSXPEeQWQu78uaXG9yZU4jzS2jppFoCh9nSTNzBifFU0D2oSoCvPekoZL6ujuc6KsuZI6RunGjpOmps/KMR1oiW6TdImkumh8W0mfufv6aDxz//7PMRHlL4nmL/QYAlqaLpIWSLrfQjPNe81sc3FeAbK4+2xJfSXNUAgQLZE0SpxXgMZU4jzS2DpqFoEiAKkxsy0kPSHpQndfmpkXRdp57CLaNDM7RtJ8dx9V7bIANa69pH0k9XP3vSWtUL2q+5xXAClqAtNLIbi6o6TNRTNKIC+VOI+0lHMVgaL0zZa0U8Z452ga0KqZ2UYKQaJH3P3v0eR5UbVMRcP50fTGjpOmpnfOMR1oaQ6QdJyZTVeovv8DhX5Yto6aDEjZ+/d/jokofytJi1T4MQS0NLMkzXL34dH4YIXAEecVINuhkqa5+wJ3Xyfp7wrnGs4rQG6VOI80to6aRaAofSMkdY2eNLCxQqdwQ6pcJiBVUdv2+yRNdPdbMrKGSIqfDHCmpKcypp8RPV2gh6QlUfXMFyT1NLNton/Iekp6IcpbamY9onWdkbEsoMVw98vcvbO776pwfnjF3U+T9E9JP4xmq3+sxMfQD6P5PZp+cvT0mi4KHSi+I85BaCXcfa6kmWb2tWjSIZImiPMKUN8MST3MbLNoX46PFc4rQG6VOI80to6a1b75WVAKd19vZr9U2KHaServ7uOrXCwgbQdIOl3SODMbG027XNINkgaZ2TmSPpb0oyhvqKSjFDpKXCnpbEly98Vmdq3CRYkkXePucQd05yo8uWBTSc9FL6C1+J2kgWZ2naQxijrwjYYDzGyKQmeMJ0uSu483s0EKNwPrJZ3n7hskiXMQWpHzJT0S3ZxOVThXfE6cV4D/cPfhZjZY0miF88EYSXdLelacV9DGmdljkr4vaTszm6Xw9LJK3J80to6aZSFgDAAAAAAAgLaOpmcAAAAAAACQRKAIAAAAAAAAEQJFAAAAAAAAkESgCAAAAAAAABECRQAAAAAAAJBEoAgAAAAAAAARAkUAAAAAAACQRKAIAAAAAAAAkf8PDqPo0KWCILIAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 1440x576 with 3 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qUKTWZ-VRglZ"
      },
      "source": [
        "### Timestamp synch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "D6z_mJLLwORO",
        "outputId": "71099ef5-baad-41b7-b75f-e6cca738194d"
      },
      "source": [
        "import pandas as pd\n",
        "import time\n",
        "from moviepy.video.io.ffmpeg_tools import ffmpeg_extract_subclip\n",
        "import scenedetect.frame_timecode as sce\n",
        "import deeplabcut\n",
        "import os\n",
        "\n",
        "\n",
        "def main():\n",
        "  # DLC variables\n",
        "  %matplotlib inline\n",
        "  ProjectFolderName = 'Stage/sk_test-sanne-2021-09-08'\n",
        "  VideoType = 'avi' \n",
        "  # video path \n",
        "  videofile_path = '/content/drive/My Drive/'+ProjectFolderName+'/videos/'\n",
        "  videofile_path\n",
        "  # config file\n",
        "  path_config_file = '/content/drive/My Drive/'+ProjectFolderName+'/config.yaml'\n",
        "  path_config_file\n",
        "\n",
        "  # make file variables\n",
        "  file_path = '/content/drive/My Drive/Stage/ADC_trial1/'\n",
        "  ADC_file = '100_ADC1_0.continuous'\n",
        "  LED_file = 'PointGreyLEDStatus2020-05-04T10_34_07.csv'\n",
        "  TIME_file = 'PointGreyTimestamps2020-05-04T10_34_07.csv'\n",
        "  VIDEO_file = 'PointGreyVideo2018-07-30T11_24_12downsampled.avi'\n",
        "  analysis_file = '/content/drive/My Drive/Stage/trail_1_whiteDLC_resnet50_downsampled_trackerSep16shuffle1_180000.csv'\n",
        "  \n",
        "  # load in data as dataframes\n",
        "  data = load(file_path+ADC_file)\n",
        "  print(data)\n",
        "  DF_ADC_samples, DF_ADC_timestamps = load_data_into_dataframe(data)\n",
        "  DF_p_led = pd.read_csv(file_path+LED_file, header=None)\n",
        "  DF_p_timestamps = pd.read_csv(file_path+TIME_file, header=None)\n",
        "  DF_analysis = pd.read_csv(analysis_file, skiprows=2)\n",
        "\n",
        "  # convert the pointgrey timestamps into seconds and uncylce them\n",
        "  #DF_p_timestamps.iloc[0:10000].plot.line(subplots=True)\n",
        "  DF_p_timestamps = convert_and_uncycle_timestamps(DF_p_timestamps)\n",
        "  print(DF_ADC_timestamps)\n",
        "  print(DF_p_timestamps)\n",
        "  print(DF_p_timestamps.iloc[180074])\n",
        "  DF_p_timestamps.iloc[81393:92455].diff().plot.line()\n",
        "  #DF_p_timestamps.plot.line(subplots=True)\n",
        "  # Divide all values by a thousand to make up for sampling rate of 30000\n",
        "  t = DF_p_timestamps.diff()\n",
        "  t_select = t.loc[t[0] > 0.033376]\n",
        "  df = DF_p_timestamps[DF_p_timestamps.index.isin(t_select.index)]\n",
        "  #t.plot.line()\n",
        "  df_t = pd.to_datetime(df[0], unit='s').dt.strftime(\"%H:%M:%S.%f\")\n",
        "  print(df_t)\n",
        "  df_t = df_t.to_frame()\n",
        "  #df1 = DF_p_led[DF_p_led.index.isin(t_select.index)]\n",
        "\n",
        "  df_tt = df_t[(df_t[0] < '01:37:37.927875')]\n",
        "  df_ttt = t[t.index.isin(df_tt.index)]\n",
        "\n",
        "  df_ttt = df_ttt.apply(lambda x: x - 0)\n",
        "  a = df_ttt.sum()[0]\n",
        " \n",
        "  # divide by 30k to convert to seconds\n",
        " # print(DF_ADC_timestamps)\n",
        "  DF_ADC_timestamps = DF_ADC_timestamps.apply(dividething)\n",
        "  print(DF_ADC_timestamps)\n",
        " # print(DF_ADC_timestamps)\n",
        " # print(DF_p_timestamps)\n",
        "  # add pointgrey delay\n",
        "  DF_p_ADC_timestamps1 = DF_ADC_timestamps.apply(lambda x: x - a) \n",
        "  DF_p_ADC_timestamps = DF_p_ADC_timestamps1.apply(lambda x: add_delay(x[0], DF_p_timestamps[0].iloc[0]),axis=1)\n",
        "  #print(DF_p_ADC_timestamps[0])\n",
        "\n",
        "  DF_time_1 = DF_p_ADC_timestamps.apply(lambda x: pd.to_datetime(x, unit='s').strftime('%H:%M:%S.%f'))\n",
        "\n",
        "  DF_p_ADC_timestamps = DF_p_ADC_timestamps.apply(lambda x: x - DF_p_ADC_timestamps[0] ) \n",
        "  # Convert seconds into HH MM SS ff timestamps\n",
        "  DF_time = DF_p_ADC_timestamps.apply(lambda x: pd.to_datetime(x, unit='s').strftime('%H:%M:%S.%f'))\n",
        "  print(DF_time_1[0])\n",
        "  print(DF_time_1.iloc[-1])\n",
        "\n",
        "  #cut(video_file_path=file_path+VIDEO_file, start_time= DF_time_1[0], end_time=DF_time_1.iloc[-1])\n",
        "  # convert the timestamps into frame numbers and put them in a list\n",
        "  trial_frame_numbers = []\n",
        "\n",
        "  for index, row in DF_time.items():\n",
        "    x = sce.FrameTimecode(timecode = row, fps = 30.0)\n",
        "    trial_frame_numbers.append(x.get_frames())\n",
        "\n",
        "  DF_trial_ADC_framenumbers = pd.DataFrame (trial_frame_numbers, columns = ['frames'])\n",
        "  #print(DF_trial_ADC_framenumbers)\n",
        "  DF_subset = DF_analysis.loc[DF_analysis.index & DF_trial_ADC_framenumbers.frames]\n",
        "  DF_head = DF_subset[['x','y','likelihood']]\n",
        "  return DF_head\n",
        "  \"\"\"\n",
        "  DF_ADC_timestamps = DF_ADC_timestamps.astype(int)\n",
        "\n",
        "\n",
        "  # Find the first and last timestamp in the ADC file to later determine \n",
        "  # start and end time of the trial\n",
        "  start = DF_ADC_timestamps[0].iloc[0]\n",
        "  end = DF_ADC_timestamps[0].iloc[-1]\n",
        "  start_time = int(DF_p_timestamps.loc[int(start),0])\n",
        "  end_time = int(DF_p_timestamps.loc[int(end),0])\n",
        "\n",
        "  timeframe = [str(start_time) + '-' + str(end_time)]\n",
        "  print(timeframe)\n",
        "  #cut_video(timeframe, file_path+VIDEO_file )\n",
        "  # setup deeplabcut\n",
        "  #os.environ[\"DLClight\"]=\"True\"\n",
        "  # analyse new video\n",
        "  #deeplabcut.analyze_videos(path_config_file, , save_as_csv=True)\n",
        "  print(DF_p_timestamps)\n",
        "  # Find the timestamps of the ADC file in the index of the pointgrey timestamps\n",
        "  DF_p_ADC_timestamps = DF_p_timestamps.loc[DF_p_timestamps.index & DF_ADC_timestamps[0]]\n",
        "\n",
        "  # Substract the starting time of the trial from the total seconds\n",
        "  # so that you have the seconds of the actual trial video these times corrolate to\n",
        "  DF_p_ADC_timestamps = DF_p_ADC_timestamps.apply(lambda x: transfer_frames(x[0], start_time),axis=1)\n",
        "\n",
        "  # Convert seconds into HH MM SS ff timestamps\n",
        "  DF_time = DF_p_ADC_timestamps.apply(lambda x: pd.to_datetime(x, unit='s').strftime('%H:%M:%S.%f'))\n",
        "  # convert the timestamps into frame numbers and put them in a list\n",
        "  trial_frame_numbers = []\n",
        "\n",
        "  for index, row in DF_time.items():\n",
        "    x = sce.FrameTimecode(timecode = row, fps = 30.0)\n",
        "    trial_frame_numbers.append(x.get_frames())\n",
        "\n",
        "  DF_trial_ADC_framenumbers = pd.DataFrame (trial_frame_numbers, columns = ['frames'])\n",
        "  DF_subset = DF_analysis.loc[DF_analysis.index & DF_trial_ADC_framenumbers.frames]\n",
        "  DF_head = DF_subset[['x','y','likelihood']]\n",
        "\n",
        "  return DF_head\n",
        "  \"\"\"\n",
        "\n",
        "head = main()\n",
        "head.plot.line(subplots=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading continuous data...\n",
            "{'header': {'format': \"'Open Ephys Data Format'\", ' version': '0.4', ' header_bytes': '1024', 'description': \"'each record contains one 64-bit timestamp, one 16-bit sample count (N), 1 uint16 recordingNumber, N 16-bit samples, and one 10-byte record marker (0 1 2 3 4 5 6 7 8 255)'\", ' date_created': \"'4-May-2020 112118'\", 'channel': \"'ADC1'\", 'channelType': \"'Continuous'\", 'sampleRate': '30000', 'blockLength': '1024', 'bufferSize': '1024', 'bitVolts': '0.000152588'}, 'timestamps': array([2.048000e+03, 3.072000e+03, 4.096000e+03, ..., 9.678848e+06,\n",
            "       9.679872e+06, 9.680896e+06]), 'data': array([-0.01113892, -0.00839234, -0.00869752, ...,  0.        ,\n",
            "        0.        ,  0.        ]), 'recordingNumber': array([0., 0., 0., ..., 0., 0., 0.])}\n",
            "              0\n",
            "0        2048.0\n",
            "1        3072.0\n",
            "2        4096.0\n",
            "3        5120.0\n",
            "4        6144.0\n",
            "...         ...\n",
            "9448  9676800.0\n",
            "9449  9677824.0\n",
            "9450  9678848.0\n",
            "9451  9679872.0\n",
            "9452  9680896.0\n",
            "\n",
            "[9453 rows x 1 columns]\n",
            "                   0\n",
            "0          34.931000\n",
            "1          34.964250\n",
            "2          34.997625\n",
            "3          35.030875\n",
            "4          35.064250\n",
            "...              ...\n",
            "800792  26721.376125\n",
            "800793  26721.409375\n",
            "800794  26721.442750\n",
            "800795  26721.476000\n",
            "800796  26721.509375\n",
            "\n",
            "[800797 rows x 1 columns]\n",
            "0    6036.022625\n",
            "Name: 180074, dtype: float64\n",
            "4688      00:03:11.219875\n",
            "83071     00:46:43.276500\n",
            "83072     00:46:43.343125\n",
            "111794    01:02:40.504000\n",
            "145212    01:21:14.153500\n",
            "164124    01:31:44.440625\n",
            "174568    01:37:32.541500\n",
            "207576    01:55:52.595000\n",
            "260051    02:25:01.297125\n",
            "269512    02:30:16.674000\n",
            "275446    02:33:34.451000\n",
            "326707    03:02:02.731375\n",
            "351486    03:15:48.496625\n",
            "415878    03:51:34.320250\n",
            "510918    04:44:21.453750\n",
            "514559    04:46:22.819250\n",
            "674845    06:15:24.201750\n",
            "675745    06:15:54.226625\n",
            "717190    06:38:55.367625\n",
            "737912    06:50:25.938125\n",
            "785579    07:16:54.420125\n",
            "Name: 0, dtype: object\n",
            "               0\n",
            "0       0.068267\n",
            "1       0.102400\n",
            "2       0.136533\n",
            "3       0.170667\n",
            "4       0.204800\n",
            "...          ...\n",
            "9448  322.560000\n",
            "9449  322.594133\n",
            "9450  322.628267\n",
            "9451  322.662400\n",
            "9452  322.696533\n",
            "\n",
            "[9453 rows x 1 columns]\n",
            "00:00:34.433141\n",
            "00:05:57.061408\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([<matplotlib.axes._subplots.AxesSubplot object at 0x7f1a5f2428d0>,\n",
              "       <matplotlib.axes._subplots.AxesSubplot object at 0x7f1a5d7d2f50>,\n",
              "       <matplotlib.axes._subplots.AxesSubplot object at 0x7f1a5f242c50>],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 3 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YARdO4RzGNJr"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nj_xqjYfsBL9",
        "outputId": "c95a0c3d-557a-4d50-e2bf-d3725d662d5b"
      },
      "source": [
        "cut(video_file_path='/content/drive/My Drive/Stage/white-TRIM.avi', start_time= '00:00:00.000000', end_time='00:05:43.722666')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input #0, avi, from '/content/drive/My Drive/Stage/white-TRIM.avi':\n",
            "  Metadata:\n",
            "    encoder         : Lavf58.45.100\n",
            "  Duration: 00:05:43.73, start: 0.000000, bitrate: 205 kb/s\n",
            "    Stream #0:0: Video: mpeg4 (Simple Profile) (FMP4 / 0x34504D46), yuv420p, 325x256 [SAR 1:1 DAR 325:256], 199 kb/s, SAR 41984:41925 DAR 164:129, 30 fps, 30 tbr, 30 tbn, 30 tbc\n",
            "Output #0, avi, to '/content/drive/My Drive/Stage//white-TRIM-TRIM.avi':\n",
            "  Metadata:\n",
            "    ISFT            : Lavf58.45.100\n",
            "    Stream #0:0: Video: mpeg4 (Simple Profile) (FMP4 / 0x34504D46), yuv420p, 325x256 [SAR 41984:41925 DAR 164:129], q=2-31, 199 kb/s, 30 fps, 30 tbr, 30 tbn, 30 tbc\n",
            "Stream mapping:\n",
            "  Stream #0:0 -> #0:0 (copy)\n",
            "Press [q] to stop, [?] for help\n",
            "frame=10301 fps=0.0 q=-1.0 Lsize=    8620kB time=00:05:43.73 bitrate= 205.4kbits/s speed=3.37e+03x    \n",
            "video:8367kB audio:0kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 3.015925%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6mpqKkKlPlc"
      },
      "source": [
        "### Test branch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "id": "sT5sbsurlSnQ",
        "outputId": "13c28ef9-5d2d-4dd7-a705-b92cbf993dd9"
      },
      "source": [
        "import pandas as pd\n",
        "import time\n",
        "from moviepy.video.io.ffmpeg_tools import ffmpeg_extract_subclip\n",
        "import scenedetect.frame_timecode as sce\n",
        "import deeplabcut\n",
        "import os\n",
        "def load_all():\n",
        "  # DLC variables\n",
        "  ProjectFolderName = 'Stage/sk_test-sanne-2021-09-08'\n",
        "  VideoType = 'avi' \n",
        "  # video path \n",
        "  videofile_path = '/content/drive/My Drive/'+ProjectFolderName+'/videos/'\n",
        "  videofile_path\n",
        "  # config file\n",
        "  path_config_file = '/content/drive/My Drive/'+ProjectFolderName+'/config.yaml'\n",
        "  path_config_file\n",
        "\n",
        "  # make file variables\n",
        "  file_path = '/content/drive/My Drive/Stage/synch_test/'\n",
        "  ADC_file = 'Trial1/100_ADC2_0.continuous'\n",
        "  LED_file = 'PointGreyLEDStatus2018-07-30T11_24_12.csv'\n",
        "  TIME_file = 'PointGreyTimestamps2018-07-30T11_24_12.csv'\n",
        "  VIDEO_file = 'PointGreyVideo2018-07-30T11_24_12downsampled.avi'\n",
        "  analysis_file = '/content/drive/My Drive/Stage/trail_1_whiteDLC_resnet50_downsampled_trackerSep16shuffle1_180000.csv'\n",
        "  \n",
        "  # load in data as dataframes\n",
        "  data = load(file_path+ADC_file)\n",
        "  DF_ADC_samples, DF_ADC_timestamps = load_data_into_dataframe(data)\n",
        "  #DF_p_led = pd.read_csv(file_path+LED_file, header=None)\n",
        "  DF_p_timestamps = pd.read_csv(file_path+TIME_file, header=None)\n",
        "  #DF_analysis = pd.read_csv(analysis_file, skiprows=2)\n",
        "\n",
        "  return DF_ADC_samples, DF_ADC_timestamps, DF_p_timestamps\n",
        "\n",
        "def main5():\n",
        "  # DLC variables\n",
        "  %matplotlib inline\n",
        "  DF_ADC_samples, DF_ADC_timestamps, DF_p_timestamps= load_all()\n",
        "  df= DF_ADC_samples.diff()\n",
        "  #df.iloc[0:5000].plot.line()\n",
        "  #print(df.iloc[3580:35])\n",
        "  df_s = df.loc[df[0] > 0.9]\n",
        "  df_s['index1'] = df_s.index\n",
        "  a = df_s['index1'].diff()\n",
        "  print(a.unique())\n",
        "  b = a.loc[a >= 999]\n",
        "  \n",
        "  b = b.reset_index()\n",
        "  b['state'] = 0\n",
        "  b.loc[b.index1 == 999, 'state'] = 0\n",
        "  b.loc[b.index1 == 1000, 'state'] = 1\n",
        "  b['state'].plot.line(subplots=True)\n",
        "\n",
        "  #a.plot.line()\n",
        "\n",
        "main5()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading continuous data...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:43: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[  nan 1000.  999.    1.]\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0KZlt4UEvDt3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pC-VwuDYj3Nq"
      },
      "source": [
        "## DeepLabCut"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pHcmhJflJCTP"
      },
      "source": [
        "import os\n",
        "\n",
        "all = []\n",
        "for root, dirs, files in os.walk(\"/content/drive/My Drive/Stage/\"):\n",
        "    for file in files:\n",
        "        if file.endswith(\".avi\"):\n",
        "          if 'downsampled' in file:\n",
        "            all.append(os.path.join(root, file))\n",
        "          \n",
        "for root, dirs, files in os.walk(\"/content/drive/My Drive/Stage/\"):\n",
        "    for file in files:\n",
        "        if file.endswith(\".avi\"):\n",
        "          old_file = os.path.join(root, file)\n",
        "          already_in = False      \n",
        "          if 'downsampled' not in file:\n",
        "             file_z = file.replace('.', 'downsampled.')\n",
        "             if os.path.join(root, file_z) in all:\n",
        "                already_in = True\n",
        "                #print(os.path.join(root, file))\n",
        "             if already_in is False:\n",
        "                old_file = os.path.join(root, file)\n",
        "                file_l = file.split('.')\n",
        "                file_l[1] = 'downsampled'\n",
        "                file_l.append('.avi')\n",
        "                file = ''.join(file_l)\n",
        "                new_file =  os.path.join(root, file)\n",
        "                !ffmpeg -i \"$old_file\" -filter:v scale=-1:256 -c:a copy \"$new_file\"\n",
        "                os.remove(old_file)\n",
        "             if already_in is True:\n",
        "                os.remove(old_file)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d68wDMoGj51e"
      },
      "source": [
        "### Variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PDHeL1XScAE4",
        "outputId": "71fd2081-a7ba-4cef-ebe3-59b7e5b87389"
      },
      "source": [
        "import deeplabcut\n",
        "#Setup your project variables:\n",
        "# PLEASE EDIT THESE:\n",
        "  \n",
        "ProjectFolderName = 'Stage/final_tracker-Sanne-2021-10-20'\n",
        "VideoType = 'avi' \n",
        "\n",
        "#don't edit these:\n",
        "videofile_path = '/content/drive/My Drive/'+ProjectFolderName+'/videos/'\n",
        "videofile_path\n",
        "#Enter the list of videos or folder to analyze.\n",
        "\n",
        "#This creates a path variable that links to your google drive copy\n",
        "#No need to edit this, as you set it up before: \n",
        "path_config_file = '/content/drive/My Drive/'+ProjectFolderName+'/config.yaml'\n",
        "path_config_file\n",
        "\n",
        "\n",
        "#GUIs don't work on the cloud, so label your data locally on your computer! This will suppress the GUI support\n",
        "import os\n",
        "os.environ[\"DLClight\"]=\"True\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DLC loaded in light mode; you cannot use any GUI (labeling, relabeling and standalone GUI)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s4JcQQOUj8vE"
      },
      "source": [
        "### Version"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQokhHVVccO1"
      },
      "source": [
        "deeplabcut.__version__"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19J_42S4j_Ci"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8tvti87Pce7h",
        "outputId": "e9165551-a3ee-44e0-8522-c52a27418ea4"
      },
      "source": [
        "deeplabcut.create_training_dataset(path_config_file, net_type='resnet_50', augmenter_type='imgaug')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Stage/final_tracker-Sanne-2021-10-20/training-datasets/iteration-0/UnaugmentedDataSet_final_trackerOct20  already exists!\n",
            "/content/drive/My Drive/Stage/final_tracker-Sanne-2021-10-20/labeled-data/trail_4_bluedownsampled/CollectedData_Sanne.h5  not found (perhaps not annotated).\n",
            "/content/drive/My Drive/Stage/final_tracker-Sanne-2021-10-20/labeled-data/trail_4_greendownsampled/CollectedData_Sanne.h5  not found (perhaps not annotated).\n",
            "/content/drive/My Drive/Stage/final_tracker-Sanne-2021-10-20/labeled-data/trail_4_white(2)downsampled/CollectedData_Sanne.h5  not found (perhaps not annotated).\n",
            "/content/drive/My Drive/Stage/final_tracker-Sanne-2021-10-20/labeled-data/trail_4_whitedownsampled/CollectedData_Sanne.h5  not found (perhaps not annotated).\n",
            "/content/drive/My Drive/Stage/final_tracker-Sanne-2021-10-20/labeled-data/trail_5_greendownsampled/CollectedData_Sanne.h5  not found (perhaps not annotated).\n",
            "/content/drive/My Drive/Stage/final_tracker-Sanne-2021-10-20/labeled-data/trail_5_white(2)downsampled/CollectedData_Sanne.h5  not found (perhaps not annotated).\n",
            "/content/drive/My Drive/Stage/final_tracker-Sanne-2021-10-20/labeled-data/trail_5_whitedownsampled/CollectedData_Sanne.h5  not found (perhaps not annotated).\n",
            "It appears that the images were labeled on a Windows system, but you are currently trying to create a training set on a Unix system. \n",
            " In this case the paths should be converted. Do you want to proceed with the conversion?\n",
            "yes/noy\n",
            "Annotation data converted to unix format...\n",
            "Downloading a ImageNet-pretrained model from http://download.tensorflow.org/models/resnet_v1_50_2016_08_28.tar.gz....\n",
            "/content/drive/My Drive/Stage/final_tracker-Sanne-2021-10-20/dlc-models/iteration-0/final_trackerOct20-trainset95shuffle1  already exists!\n",
            "/content/drive/My Drive/Stage/final_tracker-Sanne-2021-10-20/dlc-models/iteration-0/final_trackerOct20-trainset95shuffle1/train  already exists!\n",
            "/content/drive/My Drive/Stage/final_tracker-Sanne-2021-10-20/dlc-models/iteration-0/final_trackerOct20-trainset95shuffle1/test  already exists!\n",
            "The training dataset is successfully created. Use the function 'train_network' to start training. Happy training!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[(0.95,\n",
              "  1,\n",
              "  (array([153, 244,  13, 195,  32,  49,  74, 316, 189, 154, 114, 305, 107,\n",
              "          267, 318, 295, 255,  50, 221,  83, 214, 339, 242, 171, 245, 187,\n",
              "           34, 115, 302,  11, 313, 119, 231, 164, 218, 151, 317, 323, 156,\n",
              "          262,  15, 182, 257, 307, 232,  40, 249,  17,  93, 178, 265,   1,\n",
              "          141, 191, 236, 117,  86, 286, 104, 225, 294, 234,  92, 161, 241,\n",
              "          300, 209, 174, 150, 113,  37, 293,  80, 126, 198, 299, 143, 148,\n",
              "          135, 311, 190,  91, 110, 291, 181, 296, 258, 263, 341, 272, 152,\n",
              "          288, 136,  47, 109,  20, 325, 227,  29, 335, 212,  65, 213, 192,\n",
              "           39,  41, 310, 273,  70,  68,  95,  35, 336, 337, 252, 327,  98,\n",
              "          256,  59, 144, 199, 229, 274, 146,  36, 196, 306, 169,  30, 179,\n",
              "          173, 138, 132, 284,  94, 319,  48, 333,  16,  88, 125,  26, 103,\n",
              "           72,  87,  25, 285, 277, 170, 131, 134, 158,  21, 120, 207, 157,\n",
              "          282, 180,  46, 253, 290, 175, 276, 235,  19, 102, 108,  78, 210,\n",
              "           63, 283, 112, 116,  76,  54, 162, 183, 228, 315, 230, 219, 200,\n",
              "           51, 137, 309, 145, 271, 217,  12, 303, 279, 204,  43, 248, 224,\n",
              "          259, 159, 269,  31,  53,  44, 324, 176, 206, 338, 266, 216, 133,\n",
              "          343, 322, 100,  52, 268, 320, 184, 281, 344, 111, 129, 238, 254,\n",
              "          122,  75,  24, 166, 334, 297, 314,  66, 233, 251, 105,  96,  79,\n",
              "           10,  18, 142, 185, 260, 163,  69,  55,  81,   4,  23,  60, 222,\n",
              "           45,  82, 118, 261, 237, 123, 312, 220, 340,  85,  22,  57,  84,\n",
              "           73, 326,  97,   9, 186, 332, 330, 177, 280,   0, 239, 331,  89,\n",
              "          270,  27,  77, 165, 202, 321,  62, 193, 215, 289, 246, 149, 172,\n",
              "          203, 139, 201, 160, 342,   6, 240,  56, 275, 287, 292,  42, 127,\n",
              "          278,  28, 328, 130, 308, 194, 205, 188, 155,   5,  90,  67, 101,\n",
              "          264,  38,  99, 168, 226,  14, 301,   2,  61, 197, 298, 106, 304,\n",
              "           58,  33]),\n",
              "   array([ 64, 147, 167, 250, 329, 211, 247,   7, 140,   3, 243, 128, 121,\n",
              "          223,   8, 208,  71, 124])))]"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fkBfQrtkchQC",
        "outputId": "4ce85bf1-1a8c-4e88-ad59-b464695e6480"
      },
      "source": [
        "deeplabcut.train_network(path_config_file, displayiters=10,saveiters=15000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Config:\n",
            "{'all_joints': [[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10]],\n",
            " 'all_joints_names': ['Head',\n",
            "                      'Sideleft',\n",
            "                      'Sideright',\n",
            "                      'Spine1',\n",
            "                      'Tailbase',\n",
            "                      'Tailmiddle',\n",
            "                      'Tailend',\n",
            "                      'CornerTopLeft',\n",
            "                      'CornerBottomLeft',\n",
            "                      'CornerTopRight',\n",
            "                      'CornerBottomRight'],\n",
            " 'alpha_r': 0.02,\n",
            " 'apply_prob': 0.5,\n",
            " 'batch_size': 1,\n",
            " 'clahe': True,\n",
            " 'claheratio': 0.1,\n",
            " 'crop_pad': 0,\n",
            " 'crop_sampling': 'hybrid',\n",
            " 'crop_size': [400, 400],\n",
            " 'cropratio': 0.4,\n",
            " 'dataset': 'training-datasets/iteration-0/UnaugmentedDataSet_final_trackerOct20/final_tracker_Sanne95shuffle1.mat',\n",
            " 'dataset_type': 'imgaug',\n",
            " 'decay_steps': 30000,\n",
            " 'deterministic': False,\n",
            " 'display_iters': 1000,\n",
            " 'edge': False,\n",
            " 'emboss': {'alpha': [0.0, 1.0], 'embossratio': 0.1, 'strength': [0.5, 1.5]},\n",
            " 'fg_fraction': 0.25,\n",
            " 'global_scale': 0.8,\n",
            " 'histeq': True,\n",
            " 'histeqratio': 0.1,\n",
            " 'init_weights': '/usr/local/lib/python3.7/dist-packages/deeplabcut/pose_estimation_tensorflow/models/pretrained/resnet_v1_50.ckpt',\n",
            " 'intermediate_supervision': False,\n",
            " 'intermediate_supervision_layer': 12,\n",
            " 'location_refinement': True,\n",
            " 'locref_huber_loss': True,\n",
            " 'locref_loss_weight': 0.05,\n",
            " 'locref_stdev': 7.2801,\n",
            " 'log_dir': 'log',\n",
            " 'lr_init': 0.0005,\n",
            " 'max_input_size': 1500,\n",
            " 'max_shift': 0.4,\n",
            " 'mean_pixel': [123.68, 116.779, 103.939],\n",
            " 'metadataset': 'training-datasets/iteration-0/UnaugmentedDataSet_final_trackerOct20/Documentation_data-final_tracker_95shuffle1.pickle',\n",
            " 'min_input_size': 64,\n",
            " 'mirror': False,\n",
            " 'multi_stage': False,\n",
            " 'multi_step': [[0.005, 10000],\n",
            "                [0.02, 430000],\n",
            "                [0.002, 730000],\n",
            "                [0.001, 1030000]],\n",
            " 'net_type': 'resnet_50',\n",
            " 'num_joints': 11,\n",
            " 'optimizer': 'sgd',\n",
            " 'pairwise_huber_loss': False,\n",
            " 'pairwise_predict': False,\n",
            " 'partaffinityfield_predict': False,\n",
            " 'pos_dist_thresh': 17,\n",
            " 'pre_resize': [],\n",
            " 'project_path': '/content/drive/My Drive/Stage/final_tracker-Sanne-2021-10-20',\n",
            " 'regularize': False,\n",
            " 'rotation': 25,\n",
            " 'rotratio': 0.4,\n",
            " 'save_iters': 50000,\n",
            " 'scale_jitter_lo': 0.5,\n",
            " 'scale_jitter_up': 1.25,\n",
            " 'scoremap_dir': 'test',\n",
            " 'sharpen': False,\n",
            " 'sharpenratio': 0.3,\n",
            " 'shuffle': True,\n",
            " 'snapshot_prefix': '/content/drive/My '\n",
            "                    'Drive/Stage/final_tracker-Sanne-2021-10-20/dlc-models/iteration-0/final_trackerOct20-trainset95shuffle1/train/snapshot',\n",
            " 'stride': 8.0,\n",
            " 'weigh_negatives': False,\n",
            " 'weigh_only_present_joints': False,\n",
            " 'weigh_part_predictions': False,\n",
            " 'weight_decay': 0.0001}\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Selecting single-animal trainer\n",
            "Batch Size is 1\n",
            "Loading ImageNet-pretrained resnet_50\n",
            "Display_iters overwritten as 10\n",
            "Save_iters overwritten as 1500\n",
            "Training parameter:\n",
            "{'stride': 8.0, 'weigh_part_predictions': False, 'weigh_negatives': False, 'fg_fraction': 0.25, 'mean_pixel': [123.68, 116.779, 103.939], 'shuffle': True, 'snapshot_prefix': '/content/drive/My Drive/Stage/final_tracker-Sanne-2021-10-20/dlc-models/iteration-0/final_trackerOct20-trainset95shuffle1/train/snapshot', 'log_dir': 'log', 'global_scale': 0.8, 'location_refinement': True, 'locref_stdev': 7.2801, 'locref_loss_weight': 0.05, 'locref_huber_loss': True, 'optimizer': 'sgd', 'intermediate_supervision': False, 'intermediate_supervision_layer': 12, 'regularize': False, 'weight_decay': 0.0001, 'crop_pad': 0, 'scoremap_dir': 'test', 'batch_size': 1, 'dataset_type': 'imgaug', 'deterministic': False, 'mirror': False, 'pairwise_huber_loss': False, 'weigh_only_present_joints': False, 'partaffinityfield_predict': False, 'pairwise_predict': False, 'all_joints': [[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10]], 'all_joints_names': ['Head', 'Sideleft', 'Sideright', 'Spine1', 'Tailbase', 'Tailmiddle', 'Tailend', 'CornerTopLeft', 'CornerBottomLeft', 'CornerTopRight', 'CornerBottomRight'], 'alpha_r': 0.02, 'apply_prob': 0.5, 'clahe': True, 'claheratio': 0.1, 'crop_sampling': 'hybrid', 'crop_size': [400, 400], 'cropratio': 0.4, 'dataset': 'training-datasets/iteration-0/UnaugmentedDataSet_final_trackerOct20/final_tracker_Sanne95shuffle1.mat', 'decay_steps': 30000, 'display_iters': 1000, 'edge': False, 'emboss': {'alpha': [0.0, 1.0], 'embossratio': 0.1, 'strength': [0.5, 1.5]}, 'histeq': True, 'histeqratio': 0.1, 'init_weights': '/usr/local/lib/python3.7/dist-packages/deeplabcut/pose_estimation_tensorflow/models/pretrained/resnet_v1_50.ckpt', 'lr_init': 0.0005, 'max_input_size': 1500, 'max_shift': 0.4, 'metadataset': 'training-datasets/iteration-0/UnaugmentedDataSet_final_trackerOct20/Documentation_data-final_tracker_95shuffle1.pickle', 'min_input_size': 64, 'multi_stage': False, 'multi_step': [[0.005, 10000], [0.02, 430000], [0.002, 730000], [0.001, 1030000]], 'net_type': 'resnet_50', 'num_joints': 11, 'pos_dist_thresh': 17, 'pre_resize': [], 'project_path': '/content/drive/My Drive/Stage/final_tracker-Sanne-2021-10-20', 'rotation': 25, 'rotratio': 0.4, 'save_iters': 50000, 'scale_jitter_lo': 0.5, 'scale_jitter_up': 1.25, 'sharpen': False, 'sharpenratio': 0.3, 'covering': True, 'elastic_transform': True, 'motion_blur': True, 'motion_blur_params': {'k': 7, 'angle': (-90, 90)}}\n",
            "Starting training....\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaminguitvoer ingekort tot de laatste 5000 regels.\u001b[0m\n",
            "iteration: 261820 loss: 0.0031 lr: 0.02\n",
            "iteration: 261830 loss: 0.0037 lr: 0.02\n",
            "iteration: 261840 loss: 0.0042 lr: 0.02\n",
            "iteration: 261850 loss: 0.0043 lr: 0.02\n",
            "iteration: 261860 loss: 0.0037 lr: 0.02\n",
            "iteration: 261870 loss: 0.0043 lr: 0.02\n",
            "iteration: 261880 loss: 0.0040 lr: 0.02\n",
            "iteration: 261890 loss: 0.0040 lr: 0.02\n",
            "iteration: 261900 loss: 0.0033 lr: 0.02\n",
            "iteration: 261910 loss: 0.0043 lr: 0.02\n",
            "iteration: 261920 loss: 0.0050 lr: 0.02\n",
            "iteration: 261930 loss: 0.0044 lr: 0.02\n",
            "iteration: 261940 loss: 0.0049 lr: 0.02\n",
            "iteration: 261950 loss: 0.0035 lr: 0.02\n",
            "iteration: 261960 loss: 0.0035 lr: 0.02\n",
            "iteration: 261970 loss: 0.0036 lr: 0.02\n",
            "iteration: 261980 loss: 0.0031 lr: 0.02\n",
            "iteration: 261990 loss: 0.0035 lr: 0.02\n",
            "iteration: 262000 loss: 0.0028 lr: 0.02\n",
            "iteration: 262010 loss: 0.0030 lr: 0.02\n",
            "iteration: 262020 loss: 0.0033 lr: 0.02\n",
            "iteration: 262030 loss: 0.0030 lr: 0.02\n",
            "iteration: 262040 loss: 0.0033 lr: 0.02\n",
            "iteration: 262050 loss: 0.0028 lr: 0.02\n",
            "iteration: 262060 loss: 0.0037 lr: 0.02\n",
            "iteration: 262070 loss: 0.0027 lr: 0.02\n",
            "iteration: 262080 loss: 0.0052 lr: 0.02\n",
            "iteration: 262090 loss: 0.0035 lr: 0.02\n",
            "iteration: 262100 loss: 0.0041 lr: 0.02\n",
            "iteration: 262110 loss: 0.0043 lr: 0.02\n",
            "iteration: 262120 loss: 0.0030 lr: 0.02\n",
            "iteration: 262130 loss: 0.0038 lr: 0.02\n",
            "iteration: 262140 loss: 0.0028 lr: 0.02\n",
            "iteration: 262150 loss: 0.0041 lr: 0.02\n",
            "iteration: 262160 loss: 0.0026 lr: 0.02\n",
            "iteration: 262170 loss: 0.0039 lr: 0.02\n",
            "iteration: 262180 loss: 0.0043 lr: 0.02\n",
            "iteration: 262190 loss: 0.0047 lr: 0.02\n",
            "iteration: 262200 loss: 0.0034 lr: 0.02\n",
            "iteration: 262210 loss: 0.0040 lr: 0.02\n",
            "iteration: 262220 loss: 0.0042 lr: 0.02\n",
            "iteration: 262230 loss: 0.0030 lr: 0.02\n",
            "iteration: 262240 loss: 0.0031 lr: 0.02\n",
            "iteration: 262250 loss: 0.0032 lr: 0.02\n",
            "iteration: 262260 loss: 0.0032 lr: 0.02\n",
            "iteration: 262270 loss: 0.0036 lr: 0.02\n",
            "iteration: 262280 loss: 0.0040 lr: 0.02\n",
            "iteration: 262290 loss: 0.0030 lr: 0.02\n",
            "iteration: 262300 loss: 0.0032 lr: 0.02\n",
            "iteration: 262310 loss: 0.0048 lr: 0.02\n",
            "iteration: 262320 loss: 0.0040 lr: 0.02\n",
            "iteration: 262330 loss: 0.0040 lr: 0.02\n",
            "iteration: 262340 loss: 0.0034 lr: 0.02\n",
            "iteration: 262350 loss: 0.0038 lr: 0.02\n",
            "iteration: 262360 loss: 0.0037 lr: 0.02\n",
            "iteration: 262370 loss: 0.0065 lr: 0.02\n",
            "iteration: 262380 loss: 0.0036 lr: 0.02\n",
            "iteration: 262390 loss: 0.0040 lr: 0.02\n",
            "iteration: 262400 loss: 0.0036 lr: 0.02\n",
            "iteration: 262410 loss: 0.0038 lr: 0.02\n",
            "iteration: 262420 loss: 0.0037 lr: 0.02\n",
            "iteration: 262430 loss: 0.0037 lr: 0.02\n",
            "iteration: 262440 loss: 0.0041 lr: 0.02\n",
            "iteration: 262450 loss: 0.0027 lr: 0.02\n",
            "iteration: 262460 loss: 0.0031 lr: 0.02\n",
            "iteration: 262470 loss: 0.0032 lr: 0.02\n",
            "iteration: 262480 loss: 0.0034 lr: 0.02\n",
            "iteration: 262490 loss: 0.0035 lr: 0.02\n",
            "iteration: 262500 loss: 0.0030 lr: 0.02\n",
            "iteration: 262510 loss: 0.0039 lr: 0.02\n",
            "iteration: 262520 loss: 0.0027 lr: 0.02\n",
            "iteration: 262530 loss: 0.0034 lr: 0.02\n",
            "iteration: 262540 loss: 0.0043 lr: 0.02\n",
            "iteration: 262550 loss: 0.0035 lr: 0.02\n",
            "iteration: 262560 loss: 0.0041 lr: 0.02\n",
            "iteration: 262570 loss: 0.0042 lr: 0.02\n",
            "iteration: 262580 loss: 0.0032 lr: 0.02\n",
            "iteration: 262590 loss: 0.0038 lr: 0.02\n",
            "iteration: 262600 loss: 0.0035 lr: 0.02\n",
            "iteration: 262610 loss: 0.0043 lr: 0.02\n",
            "iteration: 262620 loss: 0.0039 lr: 0.02\n",
            "iteration: 262630 loss: 0.0033 lr: 0.02\n",
            "iteration: 262640 loss: 0.0028 lr: 0.02\n",
            "iteration: 262650 loss: 0.0049 lr: 0.02\n",
            "iteration: 262660 loss: 0.0036 lr: 0.02\n",
            "iteration: 262670 loss: 0.0044 lr: 0.02\n",
            "iteration: 262680 loss: 0.0030 lr: 0.02\n",
            "iteration: 262690 loss: 0.0052 lr: 0.02\n",
            "iteration: 262700 loss: 0.0031 lr: 0.02\n",
            "iteration: 262710 loss: 0.0030 lr: 0.02\n",
            "iteration: 262720 loss: 0.0037 lr: 0.02\n",
            "iteration: 262730 loss: 0.0041 lr: 0.02\n",
            "iteration: 262740 loss: 0.0036 lr: 0.02\n",
            "iteration: 262750 loss: 0.0036 lr: 0.02\n",
            "iteration: 262760 loss: 0.0035 lr: 0.02\n",
            "iteration: 262770 loss: 0.0028 lr: 0.02\n",
            "iteration: 262780 loss: 0.0042 lr: 0.02\n",
            "iteration: 262790 loss: 0.0036 lr: 0.02\n",
            "iteration: 262800 loss: 0.0039 lr: 0.02\n",
            "iteration: 262810 loss: 0.0033 lr: 0.02\n",
            "iteration: 262820 loss: 0.0030 lr: 0.02\n",
            "iteration: 262830 loss: 0.0031 lr: 0.02\n",
            "iteration: 262840 loss: 0.0030 lr: 0.02\n",
            "iteration: 262850 loss: 0.0027 lr: 0.02\n",
            "iteration: 262860 loss: 0.0046 lr: 0.02\n",
            "iteration: 262870 loss: 0.0038 lr: 0.02\n",
            "iteration: 262880 loss: 0.0043 lr: 0.02\n",
            "iteration: 262890 loss: 0.0030 lr: 0.02\n",
            "iteration: 262900 loss: 0.0039 lr: 0.02\n",
            "iteration: 262910 loss: 0.0028 lr: 0.02\n",
            "iteration: 262920 loss: 0.0033 lr: 0.02\n",
            "iteration: 262930 loss: 0.0039 lr: 0.02\n",
            "iteration: 262940 loss: 0.0027 lr: 0.02\n",
            "iteration: 262950 loss: 0.0030 lr: 0.02\n",
            "iteration: 262960 loss: 0.0029 lr: 0.02\n",
            "iteration: 262970 loss: 0.0034 lr: 0.02\n",
            "iteration: 262980 loss: 0.0030 lr: 0.02\n",
            "iteration: 262990 loss: 0.0035 lr: 0.02\n",
            "iteration: 263000 loss: 0.0033 lr: 0.02\n",
            "iteration: 263010 loss: 0.0035 lr: 0.02\n",
            "iteration: 263020 loss: 0.0050 lr: 0.02\n",
            "iteration: 263030 loss: 0.0037 lr: 0.02\n",
            "iteration: 263040 loss: 0.0036 lr: 0.02\n",
            "iteration: 263050 loss: 0.0027 lr: 0.02\n",
            "iteration: 263060 loss: 0.0033 lr: 0.02\n",
            "iteration: 263070 loss: 0.0028 lr: 0.02\n",
            "iteration: 263080 loss: 0.0030 lr: 0.02\n",
            "iteration: 263090 loss: 0.0041 lr: 0.02\n",
            "iteration: 263100 loss: 0.0031 lr: 0.02\n",
            "iteration: 263110 loss: 0.0041 lr: 0.02\n",
            "iteration: 263120 loss: 0.0034 lr: 0.02\n",
            "iteration: 263130 loss: 0.0033 lr: 0.02\n",
            "iteration: 263140 loss: 0.0029 lr: 0.02\n",
            "iteration: 263150 loss: 0.0040 lr: 0.02\n",
            "iteration: 263160 loss: 0.0031 lr: 0.02\n",
            "iteration: 263170 loss: 0.0044 lr: 0.02\n",
            "iteration: 263180 loss: 0.0034 lr: 0.02\n",
            "iteration: 263190 loss: 0.0034 lr: 0.02\n",
            "iteration: 263200 loss: 0.0029 lr: 0.02\n",
            "iteration: 263210 loss: 0.0043 lr: 0.02\n",
            "iteration: 263220 loss: 0.0035 lr: 0.02\n",
            "iteration: 263230 loss: 0.0027 lr: 0.02\n",
            "iteration: 263240 loss: 0.0043 lr: 0.02\n",
            "iteration: 263250 loss: 0.0038 lr: 0.02\n",
            "iteration: 263260 loss: 0.0035 lr: 0.02\n",
            "iteration: 263270 loss: 0.0042 lr: 0.02\n",
            "iteration: 263280 loss: 0.0030 lr: 0.02\n",
            "iteration: 263290 loss: 0.0036 lr: 0.02\n",
            "iteration: 263300 loss: 0.0041 lr: 0.02\n",
            "iteration: 263310 loss: 0.0037 lr: 0.02\n",
            "iteration: 263320 loss: 0.0043 lr: 0.02\n",
            "iteration: 263330 loss: 0.0035 lr: 0.02\n",
            "iteration: 263340 loss: 0.0035 lr: 0.02\n",
            "iteration: 263350 loss: 0.0041 lr: 0.02\n",
            "iteration: 263360 loss: 0.0038 lr: 0.02\n",
            "iteration: 263370 loss: 0.0034 lr: 0.02\n",
            "iteration: 263380 loss: 0.0030 lr: 0.02\n",
            "iteration: 263390 loss: 0.0030 lr: 0.02\n",
            "iteration: 263400 loss: 0.0030 lr: 0.02\n",
            "iteration: 263410 loss: 0.0031 lr: 0.02\n",
            "iteration: 263420 loss: 0.0033 lr: 0.02\n",
            "iteration: 263430 loss: 0.0036 lr: 0.02\n",
            "iteration: 263440 loss: 0.0036 lr: 0.02\n",
            "iteration: 263450 loss: 0.0029 lr: 0.02\n",
            "iteration: 263460 loss: 0.0034 lr: 0.02\n",
            "iteration: 263470 loss: 0.0031 lr: 0.02\n",
            "iteration: 263480 loss: 0.0038 lr: 0.02\n",
            "iteration: 263490 loss: 0.0039 lr: 0.02\n",
            "iteration: 263500 loss: 0.0039 lr: 0.02\n",
            "iteration: 263510 loss: 0.0035 lr: 0.02\n",
            "iteration: 263520 loss: 0.0032 lr: 0.02\n",
            "iteration: 263530 loss: 0.0043 lr: 0.02\n",
            "iteration: 263540 loss: 0.0044 lr: 0.02\n",
            "iteration: 263550 loss: 0.0042 lr: 0.02\n",
            "iteration: 263560 loss: 0.0048 lr: 0.02\n",
            "iteration: 263570 loss: 0.0032 lr: 0.02\n",
            "iteration: 263580 loss: 0.0034 lr: 0.02\n",
            "iteration: 263590 loss: 0.0031 lr: 0.02\n",
            "iteration: 263600 loss: 0.0033 lr: 0.02\n",
            "iteration: 263610 loss: 0.0031 lr: 0.02\n",
            "iteration: 263620 loss: 0.0047 lr: 0.02\n",
            "iteration: 263630 loss: 0.0044 lr: 0.02\n",
            "iteration: 263640 loss: 0.0040 lr: 0.02\n",
            "iteration: 263650 loss: 0.0037 lr: 0.02\n",
            "iteration: 263660 loss: 0.0044 lr: 0.02\n",
            "iteration: 263670 loss: 0.0049 lr: 0.02\n",
            "iteration: 263680 loss: 0.0039 lr: 0.02\n",
            "iteration: 263690 loss: 0.0033 lr: 0.02\n",
            "iteration: 263700 loss: 0.0037 lr: 0.02\n",
            "iteration: 263710 loss: 0.0045 lr: 0.02\n",
            "iteration: 263720 loss: 0.0035 lr: 0.02\n",
            "iteration: 263730 loss: 0.0047 lr: 0.02\n",
            "iteration: 263740 loss: 0.0034 lr: 0.02\n",
            "iteration: 263750 loss: 0.0034 lr: 0.02\n",
            "iteration: 263760 loss: 0.0033 lr: 0.02\n",
            "iteration: 263770 loss: 0.0037 lr: 0.02\n",
            "iteration: 263780 loss: 0.0033 lr: 0.02\n",
            "iteration: 263790 loss: 0.0032 lr: 0.02\n",
            "iteration: 263800 loss: 0.0029 lr: 0.02\n",
            "iteration: 263810 loss: 0.0026 lr: 0.02\n",
            "iteration: 263820 loss: 0.0032 lr: 0.02\n",
            "iteration: 263830 loss: 0.0029 lr: 0.02\n",
            "iteration: 263840 loss: 0.0033 lr: 0.02\n",
            "iteration: 263850 loss: 0.0034 lr: 0.02\n",
            "iteration: 263860 loss: 0.0043 lr: 0.02\n",
            "iteration: 263870 loss: 0.0035 lr: 0.02\n",
            "iteration: 263880 loss: 0.0031 lr: 0.02\n",
            "iteration: 263890 loss: 0.0035 lr: 0.02\n",
            "iteration: 263900 loss: 0.0027 lr: 0.02\n",
            "iteration: 263910 loss: 0.0024 lr: 0.02\n",
            "iteration: 263920 loss: 0.0037 lr: 0.02\n",
            "iteration: 263930 loss: 0.0033 lr: 0.02\n",
            "iteration: 263940 loss: 0.0051 lr: 0.02\n",
            "iteration: 263950 loss: 0.0034 lr: 0.02\n",
            "iteration: 263960 loss: 0.0035 lr: 0.02\n",
            "iteration: 263970 loss: 0.0049 lr: 0.02\n",
            "iteration: 263980 loss: 0.0038 lr: 0.02\n",
            "iteration: 263990 loss: 0.0032 lr: 0.02\n",
            "iteration: 264000 loss: 0.0053 lr: 0.02\n",
            "iteration: 264010 loss: 0.0040 lr: 0.02\n",
            "iteration: 264020 loss: 0.0031 lr: 0.02\n",
            "iteration: 264030 loss: 0.0028 lr: 0.02\n",
            "iteration: 264040 loss: 0.0041 lr: 0.02\n",
            "iteration: 264050 loss: 0.0050 lr: 0.02\n",
            "iteration: 264060 loss: 0.0043 lr: 0.02\n",
            "iteration: 264070 loss: 0.0042 lr: 0.02\n",
            "iteration: 264080 loss: 0.0031 lr: 0.02\n",
            "iteration: 264090 loss: 0.0037 lr: 0.02\n",
            "iteration: 264100 loss: 0.0036 lr: 0.02\n",
            "iteration: 264110 loss: 0.0036 lr: 0.02\n",
            "iteration: 264120 loss: 0.0035 lr: 0.02\n",
            "iteration: 264130 loss: 0.0033 lr: 0.02\n",
            "iteration: 264140 loss: 0.0034 lr: 0.02\n",
            "iteration: 264150 loss: 0.0035 lr: 0.02\n",
            "iteration: 264160 loss: 0.0030 lr: 0.02\n",
            "iteration: 264170 loss: 0.0051 lr: 0.02\n",
            "iteration: 264180 loss: 0.0041 lr: 0.02\n",
            "iteration: 264190 loss: 0.0033 lr: 0.02\n",
            "iteration: 264200 loss: 0.0039 lr: 0.02\n",
            "iteration: 264210 loss: 0.0046 lr: 0.02\n",
            "iteration: 264220 loss: 0.0040 lr: 0.02\n",
            "iteration: 264230 loss: 0.0048 lr: 0.02\n",
            "iteration: 264240 loss: 0.0034 lr: 0.02\n",
            "iteration: 264250 loss: 0.0033 lr: 0.02\n",
            "iteration: 264260 loss: 0.0038 lr: 0.02\n",
            "iteration: 264270 loss: 0.0036 lr: 0.02\n",
            "iteration: 264280 loss: 0.0058 lr: 0.02\n",
            "iteration: 264290 loss: 0.0034 lr: 0.02\n",
            "iteration: 264300 loss: 0.0034 lr: 0.02\n",
            "iteration: 264310 loss: 0.0034 lr: 0.02\n",
            "iteration: 264320 loss: 0.0036 lr: 0.02\n",
            "iteration: 264330 loss: 0.0033 lr: 0.02\n",
            "iteration: 264340 loss: 0.0033 lr: 0.02\n",
            "iteration: 264350 loss: 0.0037 lr: 0.02\n",
            "iteration: 264360 loss: 0.0030 lr: 0.02\n",
            "iteration: 264370 loss: 0.0041 lr: 0.02\n",
            "iteration: 264380 loss: 0.0037 lr: 0.02\n",
            "iteration: 264390 loss: 0.0035 lr: 0.02\n",
            "iteration: 264400 loss: 0.0041 lr: 0.02\n",
            "iteration: 264410 loss: 0.0037 lr: 0.02\n",
            "iteration: 264420 loss: 0.0046 lr: 0.02\n",
            "iteration: 264430 loss: 0.0044 lr: 0.02\n",
            "iteration: 264440 loss: 0.0033 lr: 0.02\n",
            "iteration: 264450 loss: 0.0040 lr: 0.02\n",
            "iteration: 264460 loss: 0.0030 lr: 0.02\n",
            "iteration: 264470 loss: 0.0028 lr: 0.02\n",
            "iteration: 264480 loss: 0.0038 lr: 0.02\n",
            "iteration: 264490 loss: 0.0032 lr: 0.02\n",
            "iteration: 264500 loss: 0.0033 lr: 0.02\n",
            "iteration: 264510 loss: 0.0036 lr: 0.02\n",
            "iteration: 264520 loss: 0.0040 lr: 0.02\n",
            "iteration: 264530 loss: 0.0028 lr: 0.02\n",
            "iteration: 264540 loss: 0.0046 lr: 0.02\n",
            "iteration: 264550 loss: 0.0024 lr: 0.02\n",
            "iteration: 264560 loss: 0.0034 lr: 0.02\n",
            "iteration: 264570 loss: 0.0024 lr: 0.02\n",
            "iteration: 264580 loss: 0.0038 lr: 0.02\n",
            "iteration: 264590 loss: 0.0042 lr: 0.02\n",
            "iteration: 264600 loss: 0.0034 lr: 0.02\n",
            "iteration: 264610 loss: 0.0035 lr: 0.02\n",
            "iteration: 264620 loss: 0.0030 lr: 0.02\n",
            "iteration: 264630 loss: 0.0034 lr: 0.02\n",
            "iteration: 264640 loss: 0.0037 lr: 0.02\n",
            "iteration: 264650 loss: 0.0035 lr: 0.02\n",
            "iteration: 264660 loss: 0.0035 lr: 0.02\n",
            "iteration: 264670 loss: 0.0042 lr: 0.02\n",
            "iteration: 264680 loss: 0.0037 lr: 0.02\n",
            "iteration: 264690 loss: 0.0041 lr: 0.02\n",
            "iteration: 264700 loss: 0.0033 lr: 0.02\n",
            "iteration: 264710 loss: 0.0029 lr: 0.02\n",
            "iteration: 264720 loss: 0.0033 lr: 0.02\n",
            "iteration: 264730 loss: 0.0039 lr: 0.02\n",
            "iteration: 264740 loss: 0.0032 lr: 0.02\n",
            "iteration: 264750 loss: 0.0023 lr: 0.02\n",
            "iteration: 264760 loss: 0.0040 lr: 0.02\n",
            "iteration: 264770 loss: 0.0069 lr: 0.02\n",
            "iteration: 264780 loss: 0.0036 lr: 0.02\n",
            "iteration: 264790 loss: 0.0032 lr: 0.02\n",
            "iteration: 264800 loss: 0.0043 lr: 0.02\n",
            "iteration: 264810 loss: 0.0044 lr: 0.02\n",
            "iteration: 264820 loss: 0.0036 lr: 0.02\n",
            "iteration: 264830 loss: 0.0040 lr: 0.02\n",
            "iteration: 264840 loss: 0.0036 lr: 0.02\n",
            "iteration: 264850 loss: 0.0032 lr: 0.02\n",
            "iteration: 264860 loss: 0.0036 lr: 0.02\n",
            "iteration: 264870 loss: 0.0040 lr: 0.02\n",
            "iteration: 264880 loss: 0.0038 lr: 0.02\n",
            "iteration: 264890 loss: 0.0037 lr: 0.02\n",
            "iteration: 264900 loss: 0.0045 lr: 0.02\n",
            "iteration: 264910 loss: 0.0051 lr: 0.02\n",
            "iteration: 264920 loss: 0.0036 lr: 0.02\n",
            "iteration: 264930 loss: 0.0022 lr: 0.02\n",
            "iteration: 264940 loss: 0.0028 lr: 0.02\n",
            "iteration: 264950 loss: 0.0031 lr: 0.02\n",
            "iteration: 264960 loss: 0.0040 lr: 0.02\n",
            "iteration: 264970 loss: 0.0038 lr: 0.02\n",
            "iteration: 264980 loss: 0.0029 lr: 0.02\n",
            "iteration: 264990 loss: 0.0034 lr: 0.02\n",
            "iteration: 265000 loss: 0.0039 lr: 0.02\n",
            "iteration: 265010 loss: 0.0038 lr: 0.02\n",
            "iteration: 265020 loss: 0.0035 lr: 0.02\n",
            "iteration: 265030 loss: 0.0041 lr: 0.02\n",
            "iteration: 265040 loss: 0.0028 lr: 0.02\n",
            "iteration: 265050 loss: 0.0033 lr: 0.02\n",
            "iteration: 265060 loss: 0.0044 lr: 0.02\n",
            "iteration: 265070 loss: 0.0046 lr: 0.02\n",
            "iteration: 265080 loss: 0.0035 lr: 0.02\n",
            "iteration: 265090 loss: 0.0033 lr: 0.02\n",
            "iteration: 265100 loss: 0.0029 lr: 0.02\n",
            "iteration: 265110 loss: 0.0033 lr: 0.02\n",
            "iteration: 265120 loss: 0.0036 lr: 0.02\n",
            "iteration: 265130 loss: 0.0027 lr: 0.02\n",
            "iteration: 265140 loss: 0.0043 lr: 0.02\n",
            "iteration: 265150 loss: 0.0033 lr: 0.02\n",
            "iteration: 265160 loss: 0.0031 lr: 0.02\n",
            "iteration: 265170 loss: 0.0041 lr: 0.02\n",
            "iteration: 265180 loss: 0.0032 lr: 0.02\n",
            "iteration: 265190 loss: 0.0038 lr: 0.02\n",
            "iteration: 265200 loss: 0.0036 lr: 0.02\n",
            "iteration: 265210 loss: 0.0039 lr: 0.02\n",
            "iteration: 265220 loss: 0.0035 lr: 0.02\n",
            "iteration: 265230 loss: 0.0038 lr: 0.02\n",
            "iteration: 265240 loss: 0.0035 lr: 0.02\n",
            "iteration: 265250 loss: 0.0030 lr: 0.02\n",
            "iteration: 265260 loss: 0.0036 lr: 0.02\n",
            "iteration: 265270 loss: 0.0034 lr: 0.02\n",
            "iteration: 265280 loss: 0.0035 lr: 0.02\n",
            "iteration: 265290 loss: 0.0047 lr: 0.02\n",
            "iteration: 265300 loss: 0.0033 lr: 0.02\n",
            "iteration: 265310 loss: 0.0034 lr: 0.02\n",
            "iteration: 265320 loss: 0.0032 lr: 0.02\n",
            "iteration: 265330 loss: 0.0034 lr: 0.02\n",
            "iteration: 265340 loss: 0.0031 lr: 0.02\n",
            "iteration: 265350 loss: 0.0040 lr: 0.02\n",
            "iteration: 265360 loss: 0.0045 lr: 0.02\n",
            "iteration: 265370 loss: 0.0037 lr: 0.02\n",
            "iteration: 265380 loss: 0.0040 lr: 0.02\n",
            "iteration: 265390 loss: 0.0032 lr: 0.02\n",
            "iteration: 265400 loss: 0.0038 lr: 0.02\n",
            "iteration: 265410 loss: 0.0037 lr: 0.02\n",
            "iteration: 265420 loss: 0.0031 lr: 0.02\n",
            "iteration: 265430 loss: 0.0029 lr: 0.02\n",
            "iteration: 265440 loss: 0.0043 lr: 0.02\n",
            "iteration: 265450 loss: 0.0038 lr: 0.02\n",
            "iteration: 265460 loss: 0.0051 lr: 0.02\n",
            "iteration: 265470 loss: 0.0031 lr: 0.02\n",
            "iteration: 265480 loss: 0.0035 lr: 0.02\n",
            "iteration: 265490 loss: 0.0030 lr: 0.02\n",
            "iteration: 265500 loss: 0.0043 lr: 0.02\n",
            "iteration: 265510 loss: 0.0037 lr: 0.02\n",
            "iteration: 265520 loss: 0.0031 lr: 0.02\n",
            "iteration: 265530 loss: 0.0038 lr: 0.02\n",
            "iteration: 265540 loss: 0.0043 lr: 0.02\n",
            "iteration: 265550 loss: 0.0039 lr: 0.02\n",
            "iteration: 265560 loss: 0.0031 lr: 0.02\n",
            "iteration: 265570 loss: 0.0034 lr: 0.02\n",
            "iteration: 265580 loss: 0.0031 lr: 0.02\n",
            "iteration: 265590 loss: 0.0031 lr: 0.02\n",
            "iteration: 265600 loss: 0.0034 lr: 0.02\n",
            "iteration: 265610 loss: 0.0031 lr: 0.02\n",
            "iteration: 265620 loss: 0.0050 lr: 0.02\n",
            "iteration: 265630 loss: 0.0034 lr: 0.02\n",
            "iteration: 265640 loss: 0.0036 lr: 0.02\n",
            "iteration: 265650 loss: 0.0041 lr: 0.02\n",
            "iteration: 265660 loss: 0.0040 lr: 0.02\n",
            "iteration: 265670 loss: 0.0045 lr: 0.02\n",
            "iteration: 265680 loss: 0.0032 lr: 0.02\n",
            "iteration: 265690 loss: 0.0038 lr: 0.02\n",
            "iteration: 265700 loss: 0.0033 lr: 0.02\n",
            "iteration: 265710 loss: 0.0030 lr: 0.02\n",
            "iteration: 265720 loss: 0.0046 lr: 0.02\n",
            "iteration: 265730 loss: 0.0036 lr: 0.02\n",
            "iteration: 265740 loss: 0.0031 lr: 0.02\n",
            "iteration: 265750 loss: 0.0034 lr: 0.02\n",
            "iteration: 265760 loss: 0.0033 lr: 0.02\n",
            "iteration: 265770 loss: 0.0029 lr: 0.02\n",
            "iteration: 265780 loss: 0.0037 lr: 0.02\n",
            "iteration: 265790 loss: 0.0029 lr: 0.02\n",
            "iteration: 265800 loss: 0.0031 lr: 0.02\n",
            "iteration: 265810 loss: 0.0042 lr: 0.02\n",
            "iteration: 265820 loss: 0.0042 lr: 0.02\n",
            "iteration: 265830 loss: 0.0036 lr: 0.02\n",
            "iteration: 265840 loss: 0.0042 lr: 0.02\n",
            "iteration: 265850 loss: 0.0042 lr: 0.02\n",
            "iteration: 265860 loss: 0.0041 lr: 0.02\n",
            "iteration: 265870 loss: 0.0038 lr: 0.02\n",
            "iteration: 265880 loss: 0.0036 lr: 0.02\n",
            "iteration: 265890 loss: 0.0029 lr: 0.02\n",
            "iteration: 265900 loss: 0.0033 lr: 0.02\n",
            "iteration: 265910 loss: 0.0039 lr: 0.02\n",
            "iteration: 265920 loss: 0.0048 lr: 0.02\n",
            "iteration: 265930 loss: 0.0029 lr: 0.02\n",
            "iteration: 265940 loss: 0.0038 lr: 0.02\n",
            "iteration: 265950 loss: 0.0033 lr: 0.02\n",
            "iteration: 265960 loss: 0.0030 lr: 0.02\n",
            "iteration: 265970 loss: 0.0046 lr: 0.02\n",
            "iteration: 265980 loss: 0.0034 lr: 0.02\n",
            "iteration: 265990 loss: 0.0039 lr: 0.02\n",
            "iteration: 266000 loss: 0.0031 lr: 0.02\n",
            "iteration: 266010 loss: 0.0031 lr: 0.02\n",
            "iteration: 266020 loss: 0.0039 lr: 0.02\n",
            "iteration: 266030 loss: 0.0039 lr: 0.02\n",
            "iteration: 266040 loss: 0.0039 lr: 0.02\n",
            "iteration: 266050 loss: 0.0047 lr: 0.02\n",
            "iteration: 266060 loss: 0.0032 lr: 0.02\n",
            "iteration: 266070 loss: 0.0043 lr: 0.02\n",
            "iteration: 266080 loss: 0.0041 lr: 0.02\n",
            "iteration: 266090 loss: 0.0043 lr: 0.02\n",
            "iteration: 266100 loss: 0.0043 lr: 0.02\n",
            "iteration: 266110 loss: 0.0032 lr: 0.02\n",
            "iteration: 266120 loss: 0.0045 lr: 0.02\n",
            "iteration: 266130 loss: 0.0040 lr: 0.02\n",
            "iteration: 266140 loss: 0.0041 lr: 0.02\n",
            "iteration: 266150 loss: 0.0031 lr: 0.02\n",
            "iteration: 266160 loss: 0.0035 lr: 0.02\n",
            "iteration: 266170 loss: 0.0040 lr: 0.02\n",
            "iteration: 266180 loss: 0.0040 lr: 0.02\n",
            "iteration: 266190 loss: 0.0036 lr: 0.02\n",
            "iteration: 266200 loss: 0.0042 lr: 0.02\n",
            "iteration: 266210 loss: 0.0050 lr: 0.02\n",
            "iteration: 266220 loss: 0.0038 lr: 0.02\n",
            "iteration: 266230 loss: 0.0042 lr: 0.02\n",
            "iteration: 266240 loss: 0.0041 lr: 0.02\n",
            "iteration: 266250 loss: 0.0039 lr: 0.02\n",
            "iteration: 266260 loss: 0.0034 lr: 0.02\n",
            "iteration: 266270 loss: 0.0039 lr: 0.02\n",
            "iteration: 266280 loss: 0.0031 lr: 0.02\n",
            "iteration: 266290 loss: 0.0037 lr: 0.02\n",
            "iteration: 266300 loss: 0.0037 lr: 0.02\n",
            "iteration: 266310 loss: 0.0047 lr: 0.02\n",
            "iteration: 266320 loss: 0.0042 lr: 0.02\n",
            "iteration: 266330 loss: 0.0033 lr: 0.02\n",
            "iteration: 266340 loss: 0.0038 lr: 0.02\n",
            "iteration: 266350 loss: 0.0034 lr: 0.02\n",
            "iteration: 266360 loss: 0.0040 lr: 0.02\n",
            "iteration: 266370 loss: 0.0034 lr: 0.02\n",
            "iteration: 266380 loss: 0.0041 lr: 0.02\n",
            "iteration: 266390 loss: 0.0039 lr: 0.02\n",
            "iteration: 266400 loss: 0.0037 lr: 0.02\n",
            "iteration: 266410 loss: 0.0038 lr: 0.02\n",
            "iteration: 266420 loss: 0.0038 lr: 0.02\n",
            "iteration: 266430 loss: 0.0042 lr: 0.02\n",
            "iteration: 266440 loss: 0.0040 lr: 0.02\n",
            "iteration: 266450 loss: 0.0043 lr: 0.02\n",
            "iteration: 266460 loss: 0.0040 lr: 0.02\n",
            "iteration: 266470 loss: 0.0045 lr: 0.02\n",
            "iteration: 266480 loss: 0.0030 lr: 0.02\n",
            "iteration: 266490 loss: 0.0040 lr: 0.02\n",
            "iteration: 266500 loss: 0.0041 lr: 0.02\n",
            "iteration: 266510 loss: 0.0040 lr: 0.02\n",
            "iteration: 266520 loss: 0.0034 lr: 0.02\n",
            "iteration: 266530 loss: 0.0034 lr: 0.02\n",
            "iteration: 266540 loss: 0.0032 lr: 0.02\n",
            "iteration: 266550 loss: 0.0035 lr: 0.02\n",
            "iteration: 266560 loss: 0.0032 lr: 0.02\n",
            "iteration: 266570 loss: 0.0038 lr: 0.02\n",
            "iteration: 266580 loss: 0.0031 lr: 0.02\n",
            "iteration: 266590 loss: 0.0032 lr: 0.02\n",
            "iteration: 266600 loss: 0.0031 lr: 0.02\n",
            "iteration: 266610 loss: 0.0036 lr: 0.02\n",
            "iteration: 266620 loss: 0.0034 lr: 0.02\n",
            "iteration: 266630 loss: 0.0034 lr: 0.02\n",
            "iteration: 266640 loss: 0.0034 lr: 0.02\n",
            "iteration: 266650 loss: 0.0030 lr: 0.02\n",
            "iteration: 266660 loss: 0.0040 lr: 0.02\n",
            "iteration: 266670 loss: 0.0034 lr: 0.02\n",
            "iteration: 266680 loss: 0.0034 lr: 0.02\n",
            "iteration: 266690 loss: 0.0059 lr: 0.02\n",
            "iteration: 266700 loss: 0.0038 lr: 0.02\n",
            "iteration: 266710 loss: 0.0031 lr: 0.02\n",
            "iteration: 266720 loss: 0.0030 lr: 0.02\n",
            "iteration: 266730 loss: 0.0034 lr: 0.02\n",
            "iteration: 266740 loss: 0.0042 lr: 0.02\n",
            "iteration: 266750 loss: 0.0038 lr: 0.02\n",
            "iteration: 266760 loss: 0.0039 lr: 0.02\n",
            "iteration: 266770 loss: 0.0029 lr: 0.02\n",
            "iteration: 266780 loss: 0.0034 lr: 0.02\n",
            "iteration: 266790 loss: 0.0036 lr: 0.02\n",
            "iteration: 266800 loss: 0.0055 lr: 0.02\n",
            "iteration: 266810 loss: 0.0052 lr: 0.02\n",
            "iteration: 266820 loss: 0.0038 lr: 0.02\n",
            "iteration: 266830 loss: 0.0036 lr: 0.02\n",
            "iteration: 266840 loss: 0.0043 lr: 0.02\n",
            "iteration: 266850 loss: 0.0037 lr: 0.02\n",
            "iteration: 266860 loss: 0.0040 lr: 0.02\n",
            "iteration: 266870 loss: 0.0039 lr: 0.02\n",
            "iteration: 266880 loss: 0.0033 lr: 0.02\n",
            "iteration: 266890 loss: 0.0050 lr: 0.02\n",
            "iteration: 266900 loss: 0.0035 lr: 0.02\n",
            "iteration: 266910 loss: 0.0037 lr: 0.02\n",
            "iteration: 266920 loss: 0.0040 lr: 0.02\n",
            "iteration: 266930 loss: 0.0029 lr: 0.02\n",
            "iteration: 266940 loss: 0.0036 lr: 0.02\n",
            "iteration: 266950 loss: 0.0038 lr: 0.02\n",
            "iteration: 266960 loss: 0.0029 lr: 0.02\n",
            "iteration: 266970 loss: 0.0037 lr: 0.02\n",
            "iteration: 266980 loss: 0.0033 lr: 0.02\n",
            "iteration: 266990 loss: 0.0033 lr: 0.02\n",
            "iteration: 267000 loss: 0.0043 lr: 0.02\n",
            "iteration: 267010 loss: 0.0036 lr: 0.02\n",
            "iteration: 267020 loss: 0.0043 lr: 0.02\n",
            "iteration: 267030 loss: 0.0030 lr: 0.02\n",
            "iteration: 267040 loss: 0.0043 lr: 0.02\n",
            "iteration: 267050 loss: 0.0030 lr: 0.02\n",
            "iteration: 267060 loss: 0.0045 lr: 0.02\n",
            "iteration: 267070 loss: 0.0046 lr: 0.02\n",
            "iteration: 267080 loss: 0.0039 lr: 0.02\n",
            "iteration: 267090 loss: 0.0034 lr: 0.02\n",
            "iteration: 267100 loss: 0.0051 lr: 0.02\n",
            "iteration: 267110 loss: 0.0040 lr: 0.02\n",
            "iteration: 267120 loss: 0.0043 lr: 0.02\n",
            "iteration: 267130 loss: 0.0040 lr: 0.02\n",
            "iteration: 267140 loss: 0.0039 lr: 0.02\n",
            "iteration: 267150 loss: 0.0027 lr: 0.02\n",
            "iteration: 267160 loss: 0.0029 lr: 0.02\n",
            "iteration: 267170 loss: 0.0040 lr: 0.02\n",
            "iteration: 267180 loss: 0.0050 lr: 0.02\n",
            "iteration: 267190 loss: 0.0038 lr: 0.02\n",
            "iteration: 267200 loss: 0.0040 lr: 0.02\n",
            "iteration: 267210 loss: 0.0037 lr: 0.02\n",
            "iteration: 267220 loss: 0.0048 lr: 0.02\n",
            "iteration: 267230 loss: 0.0043 lr: 0.02\n",
            "iteration: 267240 loss: 0.0035 lr: 0.02\n",
            "iteration: 267250 loss: 0.0041 lr: 0.02\n",
            "iteration: 267260 loss: 0.0040 lr: 0.02\n",
            "iteration: 267270 loss: 0.0032 lr: 0.02\n",
            "iteration: 267280 loss: 0.0029 lr: 0.02\n",
            "iteration: 267290 loss: 0.0039 lr: 0.02\n",
            "iteration: 267300 loss: 0.0040 lr: 0.02\n",
            "iteration: 267310 loss: 0.0044 lr: 0.02\n",
            "iteration: 267320 loss: 0.0030 lr: 0.02\n",
            "iteration: 267330 loss: 0.0043 lr: 0.02\n",
            "iteration: 267340 loss: 0.0032 lr: 0.02\n",
            "iteration: 267350 loss: 0.0037 lr: 0.02\n",
            "iteration: 267360 loss: 0.0038 lr: 0.02\n",
            "iteration: 267370 loss: 0.0040 lr: 0.02\n",
            "iteration: 267380 loss: 0.0030 lr: 0.02\n",
            "iteration: 267390 loss: 0.0040 lr: 0.02\n",
            "iteration: 267400 loss: 0.0051 lr: 0.02\n",
            "iteration: 267410 loss: 0.0032 lr: 0.02\n",
            "iteration: 267420 loss: 0.0028 lr: 0.02\n",
            "iteration: 267430 loss: 0.0041 lr: 0.02\n",
            "iteration: 267440 loss: 0.0049 lr: 0.02\n",
            "iteration: 267450 loss: 0.0042 lr: 0.02\n",
            "iteration: 267460 loss: 0.0043 lr: 0.02\n",
            "iteration: 267470 loss: 0.0036 lr: 0.02\n",
            "iteration: 267480 loss: 0.0045 lr: 0.02\n",
            "iteration: 267490 loss: 0.0033 lr: 0.02\n",
            "iteration: 267500 loss: 0.0027 lr: 0.02\n",
            "iteration: 267510 loss: 0.0028 lr: 0.02\n",
            "iteration: 267520 loss: 0.0031 lr: 0.02\n",
            "iteration: 267530 loss: 0.0036 lr: 0.02\n",
            "iteration: 267540 loss: 0.0052 lr: 0.02\n",
            "iteration: 267550 loss: 0.0031 lr: 0.02\n",
            "iteration: 267560 loss: 0.0034 lr: 0.02\n",
            "iteration: 267570 loss: 0.0037 lr: 0.02\n",
            "iteration: 267580 loss: 0.0035 lr: 0.02\n",
            "iteration: 267590 loss: 0.0037 lr: 0.02\n",
            "iteration: 267600 loss: 0.0040 lr: 0.02\n",
            "iteration: 267610 loss: 0.0029 lr: 0.02\n",
            "iteration: 267620 loss: 0.0037 lr: 0.02\n",
            "iteration: 267630 loss: 0.0037 lr: 0.02\n",
            "iteration: 267640 loss: 0.0034 lr: 0.02\n",
            "iteration: 267650 loss: 0.0053 lr: 0.02\n",
            "iteration: 267660 loss: 0.0042 lr: 0.02\n",
            "iteration: 267670 loss: 0.0046 lr: 0.02\n",
            "iteration: 267680 loss: 0.0034 lr: 0.02\n",
            "iteration: 267690 loss: 0.0038 lr: 0.02\n",
            "iteration: 267700 loss: 0.0029 lr: 0.02\n",
            "iteration: 267710 loss: 0.0044 lr: 0.02\n",
            "iteration: 267720 loss: 0.0040 lr: 0.02\n",
            "iteration: 267730 loss: 0.0027 lr: 0.02\n",
            "iteration: 267740 loss: 0.0035 lr: 0.02\n",
            "iteration: 267750 loss: 0.0031 lr: 0.02\n",
            "iteration: 267760 loss: 0.0039 lr: 0.02\n",
            "iteration: 267770 loss: 0.0038 lr: 0.02\n",
            "iteration: 267780 loss: 0.0034 lr: 0.02\n",
            "iteration: 267790 loss: 0.0030 lr: 0.02\n",
            "iteration: 267800 loss: 0.0041 lr: 0.02\n",
            "iteration: 267810 loss: 0.0040 lr: 0.02\n",
            "iteration: 267820 loss: 0.0038 lr: 0.02\n",
            "iteration: 267830 loss: 0.0036 lr: 0.02\n",
            "iteration: 267840 loss: 0.0038 lr: 0.02\n",
            "iteration: 267850 loss: 0.0045 lr: 0.02\n",
            "iteration: 267860 loss: 0.0034 lr: 0.02\n",
            "iteration: 267870 loss: 0.0035 lr: 0.02\n",
            "iteration: 267880 loss: 0.0044 lr: 0.02\n",
            "iteration: 267890 loss: 0.0033 lr: 0.02\n",
            "iteration: 267900 loss: 0.0038 lr: 0.02\n",
            "iteration: 267910 loss: 0.0042 lr: 0.02\n",
            "iteration: 267920 loss: 0.0037 lr: 0.02\n",
            "iteration: 267930 loss: 0.0027 lr: 0.02\n",
            "iteration: 267940 loss: 0.0031 lr: 0.02\n",
            "iteration: 267950 loss: 0.0042 lr: 0.02\n",
            "iteration: 267960 loss: 0.0024 lr: 0.02\n",
            "iteration: 267970 loss: 0.0035 lr: 0.02\n",
            "iteration: 267980 loss: 0.0031 lr: 0.02\n",
            "iteration: 267990 loss: 0.0037 lr: 0.02\n",
            "iteration: 268000 loss: 0.0043 lr: 0.02\n",
            "iteration: 268010 loss: 0.0045 lr: 0.02\n",
            "iteration: 268020 loss: 0.0041 lr: 0.02\n",
            "iteration: 268030 loss: 0.0030 lr: 0.02\n",
            "iteration: 268040 loss: 0.0037 lr: 0.02\n",
            "iteration: 268050 loss: 0.0030 lr: 0.02\n",
            "iteration: 268060 loss: 0.0036 lr: 0.02\n",
            "iteration: 268070 loss: 0.0040 lr: 0.02\n",
            "iteration: 268080 loss: 0.0033 lr: 0.02\n",
            "iteration: 268090 loss: 0.0050 lr: 0.02\n",
            "iteration: 268100 loss: 0.0049 lr: 0.02\n",
            "iteration: 268110 loss: 0.0027 lr: 0.02\n",
            "iteration: 268120 loss: 0.0039 lr: 0.02\n",
            "iteration: 268130 loss: 0.0028 lr: 0.02\n",
            "iteration: 268140 loss: 0.0035 lr: 0.02\n",
            "iteration: 268150 loss: 0.0031 lr: 0.02\n",
            "iteration: 268160 loss: 0.0051 lr: 0.02\n",
            "iteration: 268170 loss: 0.0035 lr: 0.02\n",
            "iteration: 268180 loss: 0.0039 lr: 0.02\n",
            "iteration: 268190 loss: 0.0036 lr: 0.02\n",
            "iteration: 268200 loss: 0.0037 lr: 0.02\n",
            "iteration: 268210 loss: 0.0040 lr: 0.02\n",
            "iteration: 268220 loss: 0.0033 lr: 0.02\n",
            "iteration: 268230 loss: 0.0040 lr: 0.02\n",
            "iteration: 268240 loss: 0.0033 lr: 0.02\n",
            "iteration: 268250 loss: 0.0039 lr: 0.02\n",
            "iteration: 268260 loss: 0.0040 lr: 0.02\n",
            "iteration: 268270 loss: 0.0028 lr: 0.02\n",
            "iteration: 268280 loss: 0.0041 lr: 0.02\n",
            "iteration: 268290 loss: 0.0034 lr: 0.02\n",
            "iteration: 268300 loss: 0.0034 lr: 0.02\n",
            "iteration: 268310 loss: 0.0039 lr: 0.02\n",
            "iteration: 268320 loss: 0.0033 lr: 0.02\n",
            "iteration: 268330 loss: 0.0044 lr: 0.02\n",
            "iteration: 268340 loss: 0.0034 lr: 0.02\n",
            "iteration: 268350 loss: 0.0033 lr: 0.02\n",
            "iteration: 268360 loss: 0.0029 lr: 0.02\n",
            "iteration: 268370 loss: 0.0031 lr: 0.02\n",
            "iteration: 268380 loss: 0.0047 lr: 0.02\n",
            "iteration: 268390 loss: 0.0036 lr: 0.02\n",
            "iteration: 268400 loss: 0.0037 lr: 0.02\n",
            "iteration: 268410 loss: 0.0027 lr: 0.02\n",
            "iteration: 268420 loss: 0.0030 lr: 0.02\n",
            "iteration: 268430 loss: 0.0028 lr: 0.02\n",
            "iteration: 268440 loss: 0.0038 lr: 0.02\n",
            "iteration: 268450 loss: 0.0046 lr: 0.02\n",
            "iteration: 268460 loss: 0.0034 lr: 0.02\n",
            "iteration: 268470 loss: 0.0042 lr: 0.02\n",
            "iteration: 268480 loss: 0.0033 lr: 0.02\n",
            "iteration: 268490 loss: 0.0039 lr: 0.02\n",
            "iteration: 268500 loss: 0.0042 lr: 0.02\n",
            "iteration: 268510 loss: 0.0034 lr: 0.02\n",
            "iteration: 268520 loss: 0.0026 lr: 0.02\n",
            "iteration: 268530 loss: 0.0034 lr: 0.02\n",
            "iteration: 268540 loss: 0.0038 lr: 0.02\n",
            "iteration: 268550 loss: 0.0045 lr: 0.02\n",
            "iteration: 268560 loss: 0.0044 lr: 0.02\n",
            "iteration: 268570 loss: 0.0039 lr: 0.02\n",
            "iteration: 268580 loss: 0.0039 lr: 0.02\n",
            "iteration: 268590 loss: 0.0037 lr: 0.02\n",
            "iteration: 268600 loss: 0.0029 lr: 0.02\n",
            "iteration: 268610 loss: 0.0048 lr: 0.02\n",
            "iteration: 268620 loss: 0.0033 lr: 0.02\n",
            "iteration: 268630 loss: 0.0033 lr: 0.02\n",
            "iteration: 268640 loss: 0.0042 lr: 0.02\n",
            "iteration: 268650 loss: 0.0028 lr: 0.02\n",
            "iteration: 268660 loss: 0.0043 lr: 0.02\n",
            "iteration: 268670 loss: 0.0030 lr: 0.02\n",
            "iteration: 268680 loss: 0.0036 lr: 0.02\n",
            "iteration: 268690 loss: 0.0042 lr: 0.02\n",
            "iteration: 268700 loss: 0.0033 lr: 0.02\n",
            "iteration: 268710 loss: 0.0037 lr: 0.02\n",
            "iteration: 268720 loss: 0.0043 lr: 0.02\n",
            "iteration: 268730 loss: 0.0035 lr: 0.02\n",
            "iteration: 268740 loss: 0.0041 lr: 0.02\n",
            "iteration: 268750 loss: 0.0040 lr: 0.02\n",
            "iteration: 268760 loss: 0.0034 lr: 0.02\n",
            "iteration: 268770 loss: 0.0029 lr: 0.02\n",
            "iteration: 268780 loss: 0.0027 lr: 0.02\n",
            "iteration: 268790 loss: 0.0032 lr: 0.02\n",
            "iteration: 268800 loss: 0.0040 lr: 0.02\n",
            "iteration: 268810 loss: 0.0034 lr: 0.02\n",
            "iteration: 268820 loss: 0.0029 lr: 0.02\n",
            "iteration: 268830 loss: 0.0033 lr: 0.02\n",
            "iteration: 268840 loss: 0.0033 lr: 0.02\n",
            "iteration: 268850 loss: 0.0034 lr: 0.02\n",
            "iteration: 268860 loss: 0.0042 lr: 0.02\n",
            "iteration: 268870 loss: 0.0028 lr: 0.02\n",
            "iteration: 268880 loss: 0.0044 lr: 0.02\n",
            "iteration: 268890 loss: 0.0036 lr: 0.02\n",
            "iteration: 268900 loss: 0.0035 lr: 0.02\n",
            "iteration: 268910 loss: 0.0026 lr: 0.02\n",
            "iteration: 268920 loss: 0.0026 lr: 0.02\n",
            "iteration: 268930 loss: 0.0042 lr: 0.02\n",
            "iteration: 268940 loss: 0.0034 lr: 0.02\n",
            "iteration: 268950 loss: 0.0040 lr: 0.02\n",
            "iteration: 268960 loss: 0.0025 lr: 0.02\n",
            "iteration: 268970 loss: 0.0042 lr: 0.02\n",
            "iteration: 268980 loss: 0.0029 lr: 0.02\n",
            "iteration: 268990 loss: 0.0050 lr: 0.02\n",
            "iteration: 269000 loss: 0.0035 lr: 0.02\n",
            "iteration: 269010 loss: 0.0035 lr: 0.02\n",
            "iteration: 269020 loss: 0.0029 lr: 0.02\n",
            "iteration: 269030 loss: 0.0043 lr: 0.02\n",
            "iteration: 269040 loss: 0.0022 lr: 0.02\n",
            "iteration: 269050 loss: 0.0042 lr: 0.02\n",
            "iteration: 269060 loss: 0.0043 lr: 0.02\n",
            "iteration: 269070 loss: 0.0040 lr: 0.02\n",
            "iteration: 269080 loss: 0.0031 lr: 0.02\n",
            "iteration: 269090 loss: 0.0028 lr: 0.02\n",
            "iteration: 269100 loss: 0.0036 lr: 0.02\n",
            "iteration: 269110 loss: 0.0034 lr: 0.02\n",
            "iteration: 269120 loss: 0.0027 lr: 0.02\n",
            "iteration: 269130 loss: 0.0038 lr: 0.02\n",
            "iteration: 269140 loss: 0.0036 lr: 0.02\n",
            "iteration: 269150 loss: 0.0031 lr: 0.02\n",
            "iteration: 269160 loss: 0.0035 lr: 0.02\n",
            "iteration: 269170 loss: 0.0029 lr: 0.02\n",
            "iteration: 269180 loss: 0.0036 lr: 0.02\n",
            "iteration: 269190 loss: 0.0038 lr: 0.02\n",
            "iteration: 269200 loss: 0.0028 lr: 0.02\n",
            "iteration: 269210 loss: 0.0026 lr: 0.02\n",
            "iteration: 269220 loss: 0.0033 lr: 0.02\n",
            "iteration: 269230 loss: 0.0028 lr: 0.02\n",
            "iteration: 269240 loss: 0.0037 lr: 0.02\n",
            "iteration: 269250 loss: 0.0039 lr: 0.02\n",
            "iteration: 269260 loss: 0.0034 lr: 0.02\n",
            "iteration: 269270 loss: 0.0030 lr: 0.02\n",
            "iteration: 269280 loss: 0.0038 lr: 0.02\n",
            "iteration: 269290 loss: 0.0043 lr: 0.02\n",
            "iteration: 269300 loss: 0.0033 lr: 0.02\n",
            "iteration: 269310 loss: 0.0055 lr: 0.02\n",
            "iteration: 269320 loss: 0.0040 lr: 0.02\n",
            "iteration: 269330 loss: 0.0049 lr: 0.02\n",
            "iteration: 269340 loss: 0.0043 lr: 0.02\n",
            "iteration: 269350 loss: 0.0037 lr: 0.02\n",
            "iteration: 269360 loss: 0.0039 lr: 0.02\n",
            "iteration: 269370 loss: 0.0028 lr: 0.02\n",
            "iteration: 269380 loss: 0.0033 lr: 0.02\n",
            "iteration: 269390 loss: 0.0034 lr: 0.02\n",
            "iteration: 269400 loss: 0.0036 lr: 0.02\n",
            "iteration: 269410 loss: 0.0049 lr: 0.02\n",
            "iteration: 269420 loss: 0.0048 lr: 0.02\n",
            "iteration: 269430 loss: 0.0032 lr: 0.02\n",
            "iteration: 269440 loss: 0.0034 lr: 0.02\n",
            "iteration: 269450 loss: 0.0042 lr: 0.02\n",
            "iteration: 269460 loss: 0.0024 lr: 0.02\n",
            "iteration: 269470 loss: 0.0039 lr: 0.02\n",
            "iteration: 269480 loss: 0.0043 lr: 0.02\n",
            "iteration: 269490 loss: 0.0030 lr: 0.02\n",
            "iteration: 269500 loss: 0.0035 lr: 0.02\n",
            "iteration: 269510 loss: 0.0032 lr: 0.02\n",
            "iteration: 269520 loss: 0.0036 lr: 0.02\n",
            "iteration: 269530 loss: 0.0039 lr: 0.02\n",
            "iteration: 269540 loss: 0.0025 lr: 0.02\n",
            "iteration: 269550 loss: 0.0027 lr: 0.02\n",
            "iteration: 269560 loss: 0.0028 lr: 0.02\n",
            "iteration: 269570 loss: 0.0039 lr: 0.02\n",
            "iteration: 269580 loss: 0.0032 lr: 0.02\n",
            "iteration: 269590 loss: 0.0048 lr: 0.02\n",
            "iteration: 269600 loss: 0.0038 lr: 0.02\n",
            "iteration: 269610 loss: 0.0046 lr: 0.02\n",
            "iteration: 269620 loss: 0.0034 lr: 0.02\n",
            "iteration: 269630 loss: 0.0038 lr: 0.02\n",
            "iteration: 269640 loss: 0.0038 lr: 0.02\n",
            "iteration: 269650 loss: 0.0037 lr: 0.02\n",
            "iteration: 269660 loss: 0.0036 lr: 0.02\n",
            "iteration: 269670 loss: 0.0030 lr: 0.02\n",
            "iteration: 269680 loss: 0.0034 lr: 0.02\n",
            "iteration: 269690 loss: 0.0053 lr: 0.02\n",
            "iteration: 269700 loss: 0.0044 lr: 0.02\n",
            "iteration: 269710 loss: 0.0037 lr: 0.02\n",
            "iteration: 269720 loss: 0.0043 lr: 0.02\n",
            "iteration: 269730 loss: 0.0050 lr: 0.02\n",
            "iteration: 269740 loss: 0.0038 lr: 0.02\n",
            "iteration: 269750 loss: 0.0044 lr: 0.02\n",
            "iteration: 269760 loss: 0.0037 lr: 0.02\n",
            "iteration: 269770 loss: 0.0031 lr: 0.02\n",
            "iteration: 269780 loss: 0.0049 lr: 0.02\n",
            "iteration: 269790 loss: 0.0045 lr: 0.02\n",
            "iteration: 269800 loss: 0.0037 lr: 0.02\n",
            "iteration: 269810 loss: 0.0034 lr: 0.02\n",
            "iteration: 269820 loss: 0.0047 lr: 0.02\n",
            "iteration: 269830 loss: 0.0034 lr: 0.02\n",
            "iteration: 269840 loss: 0.0045 lr: 0.02\n",
            "iteration: 269850 loss: 0.0041 lr: 0.02\n",
            "iteration: 269860 loss: 0.0026 lr: 0.02\n",
            "iteration: 269870 loss: 0.0038 lr: 0.02\n",
            "iteration: 269880 loss: 0.0028 lr: 0.02\n",
            "iteration: 269890 loss: 0.0032 lr: 0.02\n",
            "iteration: 269900 loss: 0.0028 lr: 0.02\n",
            "iteration: 269910 loss: 0.0044 lr: 0.02\n",
            "iteration: 269920 loss: 0.0051 lr: 0.02\n",
            "iteration: 269930 loss: 0.0034 lr: 0.02\n",
            "iteration: 269940 loss: 0.0039 lr: 0.02\n",
            "iteration: 269950 loss: 0.0034 lr: 0.02\n",
            "iteration: 269960 loss: 0.0035 lr: 0.02\n",
            "iteration: 269970 loss: 0.0033 lr: 0.02\n",
            "iteration: 269980 loss: 0.0043 lr: 0.02\n",
            "iteration: 269990 loss: 0.0045 lr: 0.02\n",
            "iteration: 270000 loss: 0.0028 lr: 0.02\n",
            "iteration: 270010 loss: 0.0044 lr: 0.02\n",
            "iteration: 270020 loss: 0.0027 lr: 0.02\n",
            "iteration: 270030 loss: 0.0043 lr: 0.02\n",
            "iteration: 270040 loss: 0.0029 lr: 0.02\n",
            "iteration: 270050 loss: 0.0041 lr: 0.02\n",
            "iteration: 270060 loss: 0.0033 lr: 0.02\n",
            "iteration: 270070 loss: 0.0038 lr: 0.02\n",
            "iteration: 270080 loss: 0.0035 lr: 0.02\n",
            "iteration: 270090 loss: 0.0039 lr: 0.02\n",
            "iteration: 270100 loss: 0.0033 lr: 0.02\n",
            "iteration: 270110 loss: 0.0034 lr: 0.02\n",
            "iteration: 270120 loss: 0.0031 lr: 0.02\n",
            "iteration: 270130 loss: 0.0049 lr: 0.02\n",
            "iteration: 270140 loss: 0.0040 lr: 0.02\n",
            "iteration: 270150 loss: 0.0035 lr: 0.02\n",
            "iteration: 270160 loss: 0.0030 lr: 0.02\n",
            "iteration: 270170 loss: 0.0029 lr: 0.02\n",
            "iteration: 270180 loss: 0.0036 lr: 0.02\n",
            "iteration: 270190 loss: 0.0032 lr: 0.02\n",
            "iteration: 270200 loss: 0.0040 lr: 0.02\n",
            "iteration: 270210 loss: 0.0036 lr: 0.02\n",
            "iteration: 270220 loss: 0.0036 lr: 0.02\n",
            "iteration: 270230 loss: 0.0041 lr: 0.02\n",
            "iteration: 270240 loss: 0.0034 lr: 0.02\n",
            "iteration: 270250 loss: 0.0034 lr: 0.02\n",
            "iteration: 270260 loss: 0.0033 lr: 0.02\n",
            "iteration: 270270 loss: 0.0041 lr: 0.02\n",
            "iteration: 270280 loss: 0.0032 lr: 0.02\n",
            "iteration: 270290 loss: 0.0045 lr: 0.02\n",
            "iteration: 270300 loss: 0.0043 lr: 0.02\n",
            "iteration: 270310 loss: 0.0030 lr: 0.02\n",
            "iteration: 270320 loss: 0.0036 lr: 0.02\n",
            "iteration: 270330 loss: 0.0031 lr: 0.02\n",
            "iteration: 270340 loss: 0.0037 lr: 0.02\n",
            "iteration: 270350 loss: 0.0045 lr: 0.02\n",
            "iteration: 270360 loss: 0.0037 lr: 0.02\n",
            "iteration: 270370 loss: 0.0033 lr: 0.02\n",
            "iteration: 270380 loss: 0.0032 lr: 0.02\n",
            "iteration: 270390 loss: 0.0029 lr: 0.02\n",
            "iteration: 270400 loss: 0.0033 lr: 0.02\n",
            "iteration: 270410 loss: 0.0041 lr: 0.02\n",
            "iteration: 270420 loss: 0.0039 lr: 0.02\n",
            "iteration: 270430 loss: 0.0028 lr: 0.02\n",
            "iteration: 270440 loss: 0.0036 lr: 0.02\n",
            "iteration: 270450 loss: 0.0028 lr: 0.02\n",
            "iteration: 270460 loss: 0.0045 lr: 0.02\n",
            "iteration: 270470 loss: 0.0034 lr: 0.02\n",
            "iteration: 270480 loss: 0.0029 lr: 0.02\n",
            "iteration: 270490 loss: 0.0033 lr: 0.02\n",
            "iteration: 270500 loss: 0.0035 lr: 0.02\n",
            "iteration: 270510 loss: 0.0037 lr: 0.02\n",
            "iteration: 270520 loss: 0.0031 lr: 0.02\n",
            "iteration: 270530 loss: 0.0034 lr: 0.02\n",
            "iteration: 270540 loss: 0.0045 lr: 0.02\n",
            "iteration: 270550 loss: 0.0044 lr: 0.02\n",
            "iteration: 270560 loss: 0.0035 lr: 0.02\n",
            "iteration: 270570 loss: 0.0036 lr: 0.02\n",
            "iteration: 270580 loss: 0.0034 lr: 0.02\n",
            "iteration: 270590 loss: 0.0037 lr: 0.02\n",
            "iteration: 270600 loss: 0.0037 lr: 0.02\n",
            "iteration: 270610 loss: 0.0036 lr: 0.02\n",
            "iteration: 270620 loss: 0.0034 lr: 0.02\n",
            "iteration: 270630 loss: 0.0037 lr: 0.02\n",
            "iteration: 270640 loss: 0.0039 lr: 0.02\n",
            "iteration: 270650 loss: 0.0034 lr: 0.02\n",
            "iteration: 270660 loss: 0.0048 lr: 0.02\n",
            "iteration: 270670 loss: 0.0033 lr: 0.02\n",
            "iteration: 270680 loss: 0.0040 lr: 0.02\n",
            "iteration: 270690 loss: 0.0029 lr: 0.02\n",
            "iteration: 270700 loss: 0.0035 lr: 0.02\n",
            "iteration: 270710 loss: 0.0028 lr: 0.02\n",
            "iteration: 270720 loss: 0.0048 lr: 0.02\n",
            "iteration: 270730 loss: 0.0033 lr: 0.02\n",
            "iteration: 270740 loss: 0.0032 lr: 0.02\n",
            "iteration: 270750 loss: 0.0030 lr: 0.02\n",
            "iteration: 270760 loss: 0.0042 lr: 0.02\n",
            "iteration: 270770 loss: 0.0037 lr: 0.02\n",
            "iteration: 270780 loss: 0.0040 lr: 0.02\n",
            "iteration: 270790 loss: 0.0031 lr: 0.02\n",
            "iteration: 270800 loss: 0.0040 lr: 0.02\n",
            "iteration: 270810 loss: 0.0037 lr: 0.02\n",
            "iteration: 270820 loss: 0.0028 lr: 0.02\n",
            "iteration: 270830 loss: 0.0024 lr: 0.02\n",
            "iteration: 270840 loss: 0.0036 lr: 0.02\n",
            "iteration: 270850 loss: 0.0026 lr: 0.02\n",
            "iteration: 270860 loss: 0.0029 lr: 0.02\n",
            "iteration: 270870 loss: 0.0034 lr: 0.02\n",
            "iteration: 270880 loss: 0.0048 lr: 0.02\n",
            "iteration: 270890 loss: 0.0036 lr: 0.02\n",
            "iteration: 270900 loss: 0.0036 lr: 0.02\n",
            "iteration: 270910 loss: 0.0034 lr: 0.02\n",
            "iteration: 270920 loss: 0.0036 lr: 0.02\n",
            "iteration: 270930 loss: 0.0029 lr: 0.02\n",
            "iteration: 270940 loss: 0.0035 lr: 0.02\n",
            "iteration: 270950 loss: 0.0037 lr: 0.02\n",
            "iteration: 270960 loss: 0.0030 lr: 0.02\n",
            "iteration: 270970 loss: 0.0033 lr: 0.02\n",
            "iteration: 270980 loss: 0.0040 lr: 0.02\n",
            "iteration: 270990 loss: 0.0027 lr: 0.02\n",
            "iteration: 271000 loss: 0.0039 lr: 0.02\n",
            "iteration: 271010 loss: 0.0039 lr: 0.02\n",
            "iteration: 271020 loss: 0.0035 lr: 0.02\n",
            "iteration: 271030 loss: 0.0048 lr: 0.02\n",
            "iteration: 271040 loss: 0.0032 lr: 0.02\n",
            "iteration: 271050 loss: 0.0040 lr: 0.02\n",
            "iteration: 271060 loss: 0.0040 lr: 0.02\n",
            "iteration: 271070 loss: 0.0030 lr: 0.02\n",
            "iteration: 271080 loss: 0.0037 lr: 0.02\n",
            "iteration: 271090 loss: 0.0046 lr: 0.02\n",
            "iteration: 271100 loss: 0.0040 lr: 0.02\n",
            "iteration: 271110 loss: 0.0033 lr: 0.02\n",
            "iteration: 271120 loss: 0.0043 lr: 0.02\n",
            "iteration: 271130 loss: 0.0032 lr: 0.02\n",
            "iteration: 271140 loss: 0.0036 lr: 0.02\n",
            "iteration: 271150 loss: 0.0041 lr: 0.02\n",
            "iteration: 271160 loss: 0.0054 lr: 0.02\n",
            "iteration: 271170 loss: 0.0043 lr: 0.02\n",
            "iteration: 271180 loss: 0.0036 lr: 0.02\n",
            "iteration: 271190 loss: 0.0031 lr: 0.02\n",
            "iteration: 271200 loss: 0.0059 lr: 0.02\n",
            "iteration: 271210 loss: 0.0033 lr: 0.02\n",
            "iteration: 271220 loss: 0.0038 lr: 0.02\n",
            "iteration: 271230 loss: 0.0047 lr: 0.02\n",
            "iteration: 271240 loss: 0.0031 lr: 0.02\n",
            "iteration: 271250 loss: 0.0040 lr: 0.02\n",
            "iteration: 271260 loss: 0.0038 lr: 0.02\n",
            "iteration: 271270 loss: 0.0036 lr: 0.02\n",
            "iteration: 271280 loss: 0.0032 lr: 0.02\n",
            "iteration: 271290 loss: 0.0057 lr: 0.02\n",
            "iteration: 271300 loss: 0.0036 lr: 0.02\n",
            "iteration: 271310 loss: 0.0043 lr: 0.02\n",
            "iteration: 271320 loss: 0.0029 lr: 0.02\n",
            "iteration: 271330 loss: 0.0032 lr: 0.02\n",
            "iteration: 271340 loss: 0.0045 lr: 0.02\n",
            "iteration: 271350 loss: 0.0049 lr: 0.02\n",
            "iteration: 271360 loss: 0.0027 lr: 0.02\n",
            "iteration: 271370 loss: 0.0041 lr: 0.02\n",
            "iteration: 271380 loss: 0.0042 lr: 0.02\n",
            "iteration: 271390 loss: 0.0030 lr: 0.02\n",
            "iteration: 271400 loss: 0.0036 lr: 0.02\n",
            "iteration: 271410 loss: 0.0041 lr: 0.02\n",
            "iteration: 271420 loss: 0.0046 lr: 0.02\n",
            "iteration: 271430 loss: 0.0037 lr: 0.02\n",
            "iteration: 271440 loss: 0.0045 lr: 0.02\n",
            "iteration: 271450 loss: 0.0030 lr: 0.02\n",
            "iteration: 271460 loss: 0.0043 lr: 0.02\n",
            "iteration: 271470 loss: 0.0029 lr: 0.02\n",
            "iteration: 271480 loss: 0.0034 lr: 0.02\n",
            "iteration: 271490 loss: 0.0037 lr: 0.02\n",
            "iteration: 271500 loss: 0.0040 lr: 0.02\n",
            "iteration: 271510 loss: 0.0031 lr: 0.02\n",
            "iteration: 271520 loss: 0.0040 lr: 0.02\n",
            "iteration: 271530 loss: 0.0036 lr: 0.02\n",
            "iteration: 271540 loss: 0.0037 lr: 0.02\n",
            "iteration: 271550 loss: 0.0029 lr: 0.02\n",
            "iteration: 271560 loss: 0.0031 lr: 0.02\n",
            "iteration: 271570 loss: 0.0051 lr: 0.02\n",
            "iteration: 271580 loss: 0.0025 lr: 0.02\n",
            "iteration: 271590 loss: 0.0029 lr: 0.02\n",
            "iteration: 271600 loss: 0.0046 lr: 0.02\n",
            "iteration: 271610 loss: 0.0027 lr: 0.02\n",
            "iteration: 271620 loss: 0.0038 lr: 0.02\n",
            "iteration: 271630 loss: 0.0037 lr: 0.02\n",
            "iteration: 271640 loss: 0.0024 lr: 0.02\n",
            "iteration: 271650 loss: 0.0038 lr: 0.02\n",
            "iteration: 271660 loss: 0.0040 lr: 0.02\n",
            "iteration: 271670 loss: 0.0047 lr: 0.02\n",
            "iteration: 271680 loss: 0.0040 lr: 0.02\n",
            "iteration: 271690 loss: 0.0036 lr: 0.02\n",
            "iteration: 271700 loss: 0.0028 lr: 0.02\n",
            "iteration: 271710 loss: 0.0042 lr: 0.02\n",
            "iteration: 271720 loss: 0.0039 lr: 0.02\n",
            "iteration: 271730 loss: 0.0039 lr: 0.02\n",
            "iteration: 271740 loss: 0.0036 lr: 0.02\n",
            "iteration: 271750 loss: 0.0035 lr: 0.02\n",
            "iteration: 271760 loss: 0.0049 lr: 0.02\n",
            "iteration: 271770 loss: 0.0024 lr: 0.02\n",
            "iteration: 271780 loss: 0.0033 lr: 0.02\n",
            "iteration: 271790 loss: 0.0029 lr: 0.02\n",
            "iteration: 271800 loss: 0.0042 lr: 0.02\n",
            "iteration: 271810 loss: 0.0044 lr: 0.02\n",
            "iteration: 271820 loss: 0.0038 lr: 0.02\n",
            "iteration: 271830 loss: 0.0043 lr: 0.02\n",
            "iteration: 271840 loss: 0.0044 lr: 0.02\n",
            "iteration: 271850 loss: 0.0038 lr: 0.02\n",
            "iteration: 271860 loss: 0.0039 lr: 0.02\n",
            "iteration: 271870 loss: 0.0036 lr: 0.02\n",
            "iteration: 271880 loss: 0.0038 lr: 0.02\n",
            "iteration: 271890 loss: 0.0036 lr: 0.02\n",
            "iteration: 271900 loss: 0.0036 lr: 0.02\n",
            "iteration: 271910 loss: 0.0026 lr: 0.02\n",
            "iteration: 271920 loss: 0.0039 lr: 0.02\n",
            "iteration: 271930 loss: 0.0032 lr: 0.02\n",
            "iteration: 271940 loss: 0.0037 lr: 0.02\n",
            "iteration: 271950 loss: 0.0034 lr: 0.02\n",
            "iteration: 271960 loss: 0.0048 lr: 0.02\n",
            "iteration: 271970 loss: 0.0038 lr: 0.02\n",
            "iteration: 271980 loss: 0.0050 lr: 0.02\n",
            "iteration: 271990 loss: 0.0035 lr: 0.02\n",
            "iteration: 272000 loss: 0.0036 lr: 0.02\n",
            "iteration: 272010 loss: 0.0039 lr: 0.02\n",
            "iteration: 272020 loss: 0.0035 lr: 0.02\n",
            "iteration: 272030 loss: 0.0037 lr: 0.02\n",
            "iteration: 272040 loss: 0.0042 lr: 0.02\n",
            "iteration: 272050 loss: 0.0029 lr: 0.02\n",
            "iteration: 272060 loss: 0.0043 lr: 0.02\n",
            "iteration: 272070 loss: 0.0038 lr: 0.02\n",
            "iteration: 272080 loss: 0.0037 lr: 0.02\n",
            "iteration: 272090 loss: 0.0036 lr: 0.02\n",
            "iteration: 272100 loss: 0.0042 lr: 0.02\n",
            "iteration: 272110 loss: 0.0040 lr: 0.02\n",
            "iteration: 272120 loss: 0.0033 lr: 0.02\n",
            "iteration: 272130 loss: 0.0033 lr: 0.02\n",
            "iteration: 272140 loss: 0.0036 lr: 0.02\n",
            "iteration: 272150 loss: 0.0038 lr: 0.02\n",
            "iteration: 272160 loss: 0.0026 lr: 0.02\n",
            "iteration: 272170 loss: 0.0041 lr: 0.02\n",
            "iteration: 272180 loss: 0.0038 lr: 0.02\n",
            "iteration: 272190 loss: 0.0049 lr: 0.02\n",
            "iteration: 272200 loss: 0.0044 lr: 0.02\n",
            "iteration: 272210 loss: 0.0032 lr: 0.02\n",
            "iteration: 272220 loss: 0.0045 lr: 0.02\n",
            "iteration: 272230 loss: 0.0029 lr: 0.02\n",
            "iteration: 272240 loss: 0.0042 lr: 0.02\n",
            "iteration: 272250 loss: 0.0034 lr: 0.02\n",
            "iteration: 272260 loss: 0.0030 lr: 0.02\n",
            "iteration: 272270 loss: 0.0029 lr: 0.02\n",
            "iteration: 272280 loss: 0.0036 lr: 0.02\n",
            "iteration: 272290 loss: 0.0041 lr: 0.02\n",
            "iteration: 272300 loss: 0.0045 lr: 0.02\n",
            "iteration: 272310 loss: 0.0040 lr: 0.02\n",
            "iteration: 272320 loss: 0.0033 lr: 0.02\n",
            "iteration: 272330 loss: 0.0051 lr: 0.02\n",
            "iteration: 272340 loss: 0.0033 lr: 0.02\n",
            "iteration: 272350 loss: 0.0037 lr: 0.02\n",
            "iteration: 272360 loss: 0.0035 lr: 0.02\n",
            "iteration: 272370 loss: 0.0033 lr: 0.02\n",
            "iteration: 272380 loss: 0.0036 lr: 0.02\n",
            "iteration: 272390 loss: 0.0038 lr: 0.02\n",
            "iteration: 272400 loss: 0.0033 lr: 0.02\n",
            "iteration: 272410 loss: 0.0049 lr: 0.02\n",
            "iteration: 272420 loss: 0.0033 lr: 0.02\n",
            "iteration: 272430 loss: 0.0044 lr: 0.02\n",
            "iteration: 272440 loss: 0.0047 lr: 0.02\n",
            "iteration: 272450 loss: 0.0032 lr: 0.02\n",
            "iteration: 272460 loss: 0.0039 lr: 0.02\n",
            "iteration: 272470 loss: 0.0033 lr: 0.02\n",
            "iteration: 272480 loss: 0.0028 lr: 0.02\n",
            "iteration: 272490 loss: 0.0039 lr: 0.02\n",
            "iteration: 272500 loss: 0.0040 lr: 0.02\n",
            "iteration: 272510 loss: 0.0035 lr: 0.02\n",
            "iteration: 272520 loss: 0.0047 lr: 0.02\n",
            "iteration: 272530 loss: 0.0037 lr: 0.02\n",
            "iteration: 272540 loss: 0.0031 lr: 0.02\n",
            "iteration: 272550 loss: 0.0038 lr: 0.02\n",
            "iteration: 272560 loss: 0.0043 lr: 0.02\n",
            "iteration: 272570 loss: 0.0048 lr: 0.02\n",
            "iteration: 272580 loss: 0.0059 lr: 0.02\n",
            "iteration: 272590 loss: 0.0035 lr: 0.02\n",
            "iteration: 272600 loss: 0.0027 lr: 0.02\n",
            "iteration: 272610 loss: 0.0033 lr: 0.02\n",
            "iteration: 272620 loss: 0.0033 lr: 0.02\n",
            "iteration: 272630 loss: 0.0029 lr: 0.02\n",
            "iteration: 272640 loss: 0.0046 lr: 0.02\n",
            "iteration: 272650 loss: 0.0038 lr: 0.02\n",
            "iteration: 272660 loss: 0.0033 lr: 0.02\n",
            "iteration: 272670 loss: 0.0038 lr: 0.02\n",
            "iteration: 272680 loss: 0.0030 lr: 0.02\n",
            "iteration: 272690 loss: 0.0034 lr: 0.02\n",
            "iteration: 272700 loss: 0.0031 lr: 0.02\n",
            "iteration: 272710 loss: 0.0031 lr: 0.02\n",
            "iteration: 272720 loss: 0.0043 lr: 0.02\n",
            "iteration: 272730 loss: 0.0029 lr: 0.02\n",
            "iteration: 272740 loss: 0.0034 lr: 0.02\n",
            "iteration: 272750 loss: 0.0024 lr: 0.02\n",
            "iteration: 272760 loss: 0.0036 lr: 0.02\n",
            "iteration: 272770 loss: 0.0033 lr: 0.02\n",
            "iteration: 272780 loss: 0.0028 lr: 0.02\n",
            "iteration: 272790 loss: 0.0030 lr: 0.02\n",
            "iteration: 272800 loss: 0.0029 lr: 0.02\n",
            "iteration: 272810 loss: 0.0039 lr: 0.02\n",
            "iteration: 272820 loss: 0.0036 lr: 0.02\n",
            "iteration: 272830 loss: 0.0031 lr: 0.02\n",
            "iteration: 272840 loss: 0.0033 lr: 0.02\n",
            "iteration: 272850 loss: 0.0031 lr: 0.02\n",
            "iteration: 272860 loss: 0.0037 lr: 0.02\n",
            "iteration: 272870 loss: 0.0041 lr: 0.02\n",
            "iteration: 272880 loss: 0.0039 lr: 0.02\n",
            "iteration: 272890 loss: 0.0031 lr: 0.02\n",
            "iteration: 272900 loss: 0.0043 lr: 0.02\n",
            "iteration: 272910 loss: 0.0040 lr: 0.02\n",
            "iteration: 272920 loss: 0.0039 lr: 0.02\n",
            "iteration: 272930 loss: 0.0034 lr: 0.02\n",
            "iteration: 272940 loss: 0.0041 lr: 0.02\n",
            "iteration: 272950 loss: 0.0037 lr: 0.02\n",
            "iteration: 272960 loss: 0.0029 lr: 0.02\n",
            "iteration: 272970 loss: 0.0032 lr: 0.02\n",
            "iteration: 272980 loss: 0.0032 lr: 0.02\n",
            "iteration: 272990 loss: 0.0032 lr: 0.02\n",
            "iteration: 273000 loss: 0.0039 lr: 0.02\n",
            "iteration: 273010 loss: 0.0030 lr: 0.02\n",
            "iteration: 273020 loss: 0.0041 lr: 0.02\n",
            "iteration: 273030 loss: 0.0035 lr: 0.02\n",
            "iteration: 273040 loss: 0.0035 lr: 0.02\n",
            "iteration: 273050 loss: 0.0032 lr: 0.02\n",
            "iteration: 273060 loss: 0.0033 lr: 0.02\n",
            "iteration: 273070 loss: 0.0031 lr: 0.02\n",
            "iteration: 273080 loss: 0.0038 lr: 0.02\n",
            "iteration: 273090 loss: 0.0035 lr: 0.02\n",
            "iteration: 273100 loss: 0.0035 lr: 0.02\n",
            "iteration: 273110 loss: 0.0023 lr: 0.02\n",
            "iteration: 273120 loss: 0.0034 lr: 0.02\n",
            "iteration: 273130 loss: 0.0049 lr: 0.02\n",
            "iteration: 273140 loss: 0.0053 lr: 0.02\n",
            "iteration: 273150 loss: 0.0035 lr: 0.02\n",
            "iteration: 273160 loss: 0.0041 lr: 0.02\n",
            "iteration: 273170 loss: 0.0026 lr: 0.02\n",
            "iteration: 273180 loss: 0.0042 lr: 0.02\n",
            "iteration: 273190 loss: 0.0032 lr: 0.02\n",
            "iteration: 273200 loss: 0.0037 lr: 0.02\n",
            "iteration: 273210 loss: 0.0044 lr: 0.02\n",
            "iteration: 273220 loss: 0.0044 lr: 0.02\n",
            "iteration: 273230 loss: 0.0037 lr: 0.02\n",
            "iteration: 273240 loss: 0.0040 lr: 0.02\n",
            "iteration: 273250 loss: 0.0033 lr: 0.02\n",
            "iteration: 273260 loss: 0.0029 lr: 0.02\n",
            "iteration: 273270 loss: 0.0036 lr: 0.02\n",
            "iteration: 273280 loss: 0.0030 lr: 0.02\n",
            "iteration: 273290 loss: 0.0036 lr: 0.02\n",
            "iteration: 273300 loss: 0.0039 lr: 0.02\n",
            "iteration: 273310 loss: 0.0037 lr: 0.02\n",
            "iteration: 273320 loss: 0.0031 lr: 0.02\n",
            "iteration: 273330 loss: 0.0040 lr: 0.02\n",
            "iteration: 273340 loss: 0.0037 lr: 0.02\n",
            "iteration: 273350 loss: 0.0036 lr: 0.02\n",
            "iteration: 273360 loss: 0.0044 lr: 0.02\n",
            "iteration: 273370 loss: 0.0031 lr: 0.02\n",
            "iteration: 273380 loss: 0.0029 lr: 0.02\n",
            "iteration: 273390 loss: 0.0045 lr: 0.02\n",
            "iteration: 273400 loss: 0.0034 lr: 0.02\n",
            "iteration: 273410 loss: 0.0036 lr: 0.02\n",
            "iteration: 273420 loss: 0.0035 lr: 0.02\n",
            "iteration: 273430 loss: 0.0044 lr: 0.02\n",
            "iteration: 273440 loss: 0.0037 lr: 0.02\n",
            "iteration: 273450 loss: 0.0037 lr: 0.02\n",
            "iteration: 273460 loss: 0.0043 lr: 0.02\n",
            "iteration: 273470 loss: 0.0033 lr: 0.02\n",
            "iteration: 273480 loss: 0.0043 lr: 0.02\n",
            "iteration: 273490 loss: 0.0035 lr: 0.02\n",
            "iteration: 273500 loss: 0.0046 lr: 0.02\n",
            "iteration: 273510 loss: 0.0036 lr: 0.02\n",
            "iteration: 273520 loss: 0.0034 lr: 0.02\n",
            "iteration: 273530 loss: 0.0049 lr: 0.02\n",
            "iteration: 273540 loss: 0.0035 lr: 0.02\n",
            "iteration: 273550 loss: 0.0036 lr: 0.02\n",
            "iteration: 273560 loss: 0.0039 lr: 0.02\n",
            "iteration: 273570 loss: 0.0044 lr: 0.02\n",
            "iteration: 273580 loss: 0.0034 lr: 0.02\n",
            "iteration: 273590 loss: 0.0041 lr: 0.02\n",
            "iteration: 273600 loss: 0.0042 lr: 0.02\n",
            "iteration: 273610 loss: 0.0047 lr: 0.02\n",
            "iteration: 273620 loss: 0.0030 lr: 0.02\n",
            "iteration: 273630 loss: 0.0035 lr: 0.02\n",
            "iteration: 273640 loss: 0.0037 lr: 0.02\n",
            "iteration: 273650 loss: 0.0036 lr: 0.02\n",
            "iteration: 273660 loss: 0.0034 lr: 0.02\n",
            "iteration: 273670 loss: 0.0044 lr: 0.02\n",
            "iteration: 273680 loss: 0.0036 lr: 0.02\n",
            "iteration: 273690 loss: 0.0032 lr: 0.02\n",
            "iteration: 273700 loss: 0.0039 lr: 0.02\n",
            "iteration: 273710 loss: 0.0039 lr: 0.02\n",
            "iteration: 273720 loss: 0.0040 lr: 0.02\n",
            "iteration: 273730 loss: 0.0033 lr: 0.02\n",
            "iteration: 273740 loss: 0.0042 lr: 0.02\n",
            "iteration: 273750 loss: 0.0021 lr: 0.02\n",
            "iteration: 273760 loss: 0.0034 lr: 0.02\n",
            "iteration: 273770 loss: 0.0038 lr: 0.02\n",
            "iteration: 273780 loss: 0.0030 lr: 0.02\n",
            "iteration: 273790 loss: 0.0036 lr: 0.02\n",
            "iteration: 273800 loss: 0.0037 lr: 0.02\n",
            "iteration: 273810 loss: 0.0035 lr: 0.02\n",
            "iteration: 273820 loss: 0.0035 lr: 0.02\n",
            "iteration: 273830 loss: 0.0035 lr: 0.02\n",
            "iteration: 273840 loss: 0.0031 lr: 0.02\n",
            "iteration: 273850 loss: 0.0036 lr: 0.02\n",
            "iteration: 273860 loss: 0.0031 lr: 0.02\n",
            "iteration: 273870 loss: 0.0040 lr: 0.02\n",
            "iteration: 273880 loss: 0.0035 lr: 0.02\n",
            "iteration: 273890 loss: 0.0041 lr: 0.02\n",
            "iteration: 273900 loss: 0.0032 lr: 0.02\n",
            "iteration: 273910 loss: 0.0044 lr: 0.02\n",
            "iteration: 273920 loss: 0.0037 lr: 0.02\n",
            "iteration: 273930 loss: 0.0048 lr: 0.02\n",
            "iteration: 273940 loss: 0.0030 lr: 0.02\n",
            "iteration: 273950 loss: 0.0038 lr: 0.02\n",
            "iteration: 273960 loss: 0.0028 lr: 0.02\n",
            "iteration: 273970 loss: 0.0039 lr: 0.02\n",
            "iteration: 273980 loss: 0.0048 lr: 0.02\n",
            "iteration: 273990 loss: 0.0045 lr: 0.02\n",
            "iteration: 274000 loss: 0.0034 lr: 0.02\n",
            "iteration: 274010 loss: 0.0037 lr: 0.02\n",
            "iteration: 274020 loss: 0.0041 lr: 0.02\n",
            "iteration: 274030 loss: 0.0032 lr: 0.02\n",
            "iteration: 274040 loss: 0.0039 lr: 0.02\n",
            "iteration: 274050 loss: 0.0049 lr: 0.02\n",
            "iteration: 274060 loss: 0.0040 lr: 0.02\n",
            "iteration: 274070 loss: 0.0042 lr: 0.02\n",
            "iteration: 274080 loss: 0.0039 lr: 0.02\n",
            "iteration: 274090 loss: 0.0035 lr: 0.02\n",
            "iteration: 274100 loss: 0.0038 lr: 0.02\n",
            "iteration: 274110 loss: 0.0040 lr: 0.02\n",
            "iteration: 274120 loss: 0.0029 lr: 0.02\n",
            "iteration: 274130 loss: 0.0048 lr: 0.02\n",
            "iteration: 274140 loss: 0.0055 lr: 0.02\n",
            "iteration: 274150 loss: 0.0034 lr: 0.02\n",
            "iteration: 274160 loss: 0.0039 lr: 0.02\n",
            "iteration: 274170 loss: 0.0033 lr: 0.02\n",
            "iteration: 274180 loss: 0.0041 lr: 0.02\n",
            "iteration: 274190 loss: 0.0042 lr: 0.02\n",
            "iteration: 274200 loss: 0.0035 lr: 0.02\n",
            "iteration: 274210 loss: 0.0036 lr: 0.02\n",
            "iteration: 274220 loss: 0.0046 lr: 0.02\n",
            "iteration: 274230 loss: 0.0037 lr: 0.02\n",
            "iteration: 274240 loss: 0.0033 lr: 0.02\n",
            "iteration: 274250 loss: 0.0036 lr: 0.02\n",
            "iteration: 274260 loss: 0.0035 lr: 0.02\n",
            "iteration: 274270 loss: 0.0038 lr: 0.02\n",
            "iteration: 274280 loss: 0.0040 lr: 0.02\n",
            "iteration: 274290 loss: 0.0045 lr: 0.02\n",
            "iteration: 274300 loss: 0.0028 lr: 0.02\n",
            "iteration: 274310 loss: 0.0033 lr: 0.02\n",
            "iteration: 274320 loss: 0.0032 lr: 0.02\n",
            "iteration: 274330 loss: 0.0038 lr: 0.02\n",
            "iteration: 274340 loss: 0.0057 lr: 0.02\n",
            "iteration: 274350 loss: 0.0035 lr: 0.02\n",
            "iteration: 274360 loss: 0.0031 lr: 0.02\n",
            "iteration: 274370 loss: 0.0039 lr: 0.02\n",
            "iteration: 274380 loss: 0.0034 lr: 0.02\n",
            "iteration: 274390 loss: 0.0031 lr: 0.02\n",
            "iteration: 274400 loss: 0.0037 lr: 0.02\n",
            "iteration: 274410 loss: 0.0035 lr: 0.02\n",
            "iteration: 274420 loss: 0.0041 lr: 0.02\n",
            "iteration: 274430 loss: 0.0042 lr: 0.02\n",
            "iteration: 274440 loss: 0.0052 lr: 0.02\n",
            "iteration: 274450 loss: 0.0036 lr: 0.02\n",
            "iteration: 274460 loss: 0.0034 lr: 0.02\n",
            "iteration: 274470 loss: 0.0035 lr: 0.02\n",
            "iteration: 274480 loss: 0.0039 lr: 0.02\n",
            "iteration: 274490 loss: 0.0035 lr: 0.02\n",
            "iteration: 274500 loss: 0.0036 lr: 0.02\n",
            "iteration: 274510 loss: 0.0038 lr: 0.02\n",
            "iteration: 274520 loss: 0.0053 lr: 0.02\n",
            "iteration: 274530 loss: 0.0033 lr: 0.02\n",
            "iteration: 274540 loss: 0.0040 lr: 0.02\n",
            "iteration: 274550 loss: 0.0036 lr: 0.02\n",
            "iteration: 274560 loss: 0.0039 lr: 0.02\n",
            "iteration: 274570 loss: 0.0042 lr: 0.02\n",
            "iteration: 274580 loss: 0.0032 lr: 0.02\n",
            "iteration: 274590 loss: 0.0034 lr: 0.02\n",
            "iteration: 274600 loss: 0.0034 lr: 0.02\n",
            "iteration: 274610 loss: 0.0040 lr: 0.02\n",
            "iteration: 274620 loss: 0.0034 lr: 0.02\n",
            "iteration: 274630 loss: 0.0035 lr: 0.02\n",
            "iteration: 274640 loss: 0.0025 lr: 0.02\n",
            "iteration: 274650 loss: 0.0052 lr: 0.02\n",
            "iteration: 274660 loss: 0.0023 lr: 0.02\n",
            "iteration: 274670 loss: 0.0031 lr: 0.02\n",
            "iteration: 274680 loss: 0.0037 lr: 0.02\n",
            "iteration: 274690 loss: 0.0026 lr: 0.02\n",
            "iteration: 274700 loss: 0.0036 lr: 0.02\n",
            "iteration: 274710 loss: 0.0041 lr: 0.02\n",
            "iteration: 274720 loss: 0.0032 lr: 0.02\n",
            "iteration: 274730 loss: 0.0036 lr: 0.02\n",
            "iteration: 274740 loss: 0.0031 lr: 0.02\n",
            "iteration: 274750 loss: 0.0033 lr: 0.02\n",
            "iteration: 274760 loss: 0.0034 lr: 0.02\n",
            "iteration: 274770 loss: 0.0046 lr: 0.02\n",
            "iteration: 274780 loss: 0.0037 lr: 0.02\n",
            "iteration: 274790 loss: 0.0031 lr: 0.02\n",
            "iteration: 274800 loss: 0.0037 lr: 0.02\n",
            "iteration: 274810 loss: 0.0027 lr: 0.02\n",
            "iteration: 274820 loss: 0.0039 lr: 0.02\n",
            "iteration: 274830 loss: 0.0025 lr: 0.02\n",
            "iteration: 274840 loss: 0.0035 lr: 0.02\n",
            "iteration: 274850 loss: 0.0037 lr: 0.02\n",
            "iteration: 274860 loss: 0.0023 lr: 0.02\n",
            "iteration: 274870 loss: 0.0029 lr: 0.02\n",
            "iteration: 274880 loss: 0.0037 lr: 0.02\n",
            "iteration: 274890 loss: 0.0032 lr: 0.02\n",
            "iteration: 274900 loss: 0.0031 lr: 0.02\n",
            "iteration: 274910 loss: 0.0031 lr: 0.02\n",
            "iteration: 274920 loss: 0.0035 lr: 0.02\n",
            "iteration: 274930 loss: 0.0032 lr: 0.02\n",
            "iteration: 274940 loss: 0.0025 lr: 0.02\n",
            "iteration: 274950 loss: 0.0037 lr: 0.02\n",
            "iteration: 274960 loss: 0.0038 lr: 0.02\n",
            "iteration: 274970 loss: 0.0034 lr: 0.02\n",
            "iteration: 274980 loss: 0.0037 lr: 0.02\n",
            "iteration: 274990 loss: 0.0041 lr: 0.02\n",
            "iteration: 275000 loss: 0.0038 lr: 0.02\n",
            "iteration: 275010 loss: 0.0029 lr: 0.02\n",
            "iteration: 275020 loss: 0.0035 lr: 0.02\n",
            "iteration: 275030 loss: 0.0035 lr: 0.02\n",
            "iteration: 275040 loss: 0.0037 lr: 0.02\n",
            "iteration: 275050 loss: 0.0034 lr: 0.02\n",
            "iteration: 275060 loss: 0.0033 lr: 0.02\n",
            "iteration: 275070 loss: 0.0044 lr: 0.02\n",
            "iteration: 275080 loss: 0.0031 lr: 0.02\n",
            "iteration: 275090 loss: 0.0034 lr: 0.02\n",
            "iteration: 275100 loss: 0.0043 lr: 0.02\n",
            "iteration: 275110 loss: 0.0048 lr: 0.02\n",
            "iteration: 275120 loss: 0.0032 lr: 0.02\n",
            "iteration: 275130 loss: 0.0039 lr: 0.02\n",
            "iteration: 275140 loss: 0.0029 lr: 0.02\n",
            "iteration: 275150 loss: 0.0036 lr: 0.02\n",
            "iteration: 275160 loss: 0.0034 lr: 0.02\n",
            "iteration: 275170 loss: 0.0040 lr: 0.02\n",
            "iteration: 275180 loss: 0.0036 lr: 0.02\n",
            "iteration: 275190 loss: 0.0045 lr: 0.02\n",
            "iteration: 275200 loss: 0.0034 lr: 0.02\n",
            "iteration: 275210 loss: 0.0029 lr: 0.02\n",
            "iteration: 275220 loss: 0.0028 lr: 0.02\n",
            "iteration: 275230 loss: 0.0041 lr: 0.02\n",
            "iteration: 275240 loss: 0.0030 lr: 0.02\n",
            "iteration: 275250 loss: 0.0028 lr: 0.02\n",
            "iteration: 275260 loss: 0.0036 lr: 0.02\n",
            "iteration: 275270 loss: 0.0042 lr: 0.02\n",
            "iteration: 275280 loss: 0.0031 lr: 0.02\n",
            "iteration: 275290 loss: 0.0035 lr: 0.02\n",
            "iteration: 275300 loss: 0.0035 lr: 0.02\n",
            "iteration: 275310 loss: 0.0035 lr: 0.02\n",
            "iteration: 275320 loss: 0.0034 lr: 0.02\n",
            "iteration: 275330 loss: 0.0042 lr: 0.02\n",
            "iteration: 275340 loss: 0.0028 lr: 0.02\n",
            "iteration: 275350 loss: 0.0045 lr: 0.02\n",
            "iteration: 275360 loss: 0.0030 lr: 0.02\n",
            "iteration: 275370 loss: 0.0032 lr: 0.02\n",
            "iteration: 275380 loss: 0.0052 lr: 0.02\n",
            "iteration: 275390 loss: 0.0034 lr: 0.02\n",
            "iteration: 275400 loss: 0.0034 lr: 0.02\n",
            "iteration: 275410 loss: 0.0035 lr: 0.02\n",
            "iteration: 275420 loss: 0.0031 lr: 0.02\n",
            "iteration: 275430 loss: 0.0033 lr: 0.02\n",
            "iteration: 275440 loss: 0.0029 lr: 0.02\n",
            "iteration: 275450 loss: 0.0039 lr: 0.02\n",
            "iteration: 275460 loss: 0.0036 lr: 0.02\n",
            "iteration: 275470 loss: 0.0044 lr: 0.02\n",
            "iteration: 275480 loss: 0.0035 lr: 0.02\n",
            "iteration: 275490 loss: 0.0043 lr: 0.02\n",
            "iteration: 275500 loss: 0.0040 lr: 0.02\n",
            "iteration: 275510 loss: 0.0035 lr: 0.02\n",
            "iteration: 275520 loss: 0.0033 lr: 0.02\n",
            "iteration: 275530 loss: 0.0032 lr: 0.02\n",
            "iteration: 275540 loss: 0.0033 lr: 0.02\n",
            "iteration: 275550 loss: 0.0046 lr: 0.02\n",
            "iteration: 275560 loss: 0.0046 lr: 0.02\n",
            "iteration: 275570 loss: 0.0037 lr: 0.02\n",
            "iteration: 275580 loss: 0.0023 lr: 0.02\n",
            "iteration: 275590 loss: 0.0036 lr: 0.02\n",
            "iteration: 275600 loss: 0.0028 lr: 0.02\n",
            "iteration: 275610 loss: 0.0024 lr: 0.02\n",
            "iteration: 275620 loss: 0.0039 lr: 0.02\n",
            "iteration: 275630 loss: 0.0043 lr: 0.02\n",
            "iteration: 275640 loss: 0.0038 lr: 0.02\n",
            "iteration: 275650 loss: 0.0039 lr: 0.02\n",
            "iteration: 275660 loss: 0.0028 lr: 0.02\n",
            "iteration: 275670 loss: 0.0033 lr: 0.02\n",
            "iteration: 275680 loss: 0.0037 lr: 0.02\n",
            "iteration: 275690 loss: 0.0037 lr: 0.02\n",
            "iteration: 275700 loss: 0.0036 lr: 0.02\n",
            "iteration: 275710 loss: 0.0033 lr: 0.02\n",
            "iteration: 275720 loss: 0.0032 lr: 0.02\n",
            "iteration: 275730 loss: 0.0035 lr: 0.02\n",
            "iteration: 275740 loss: 0.0033 lr: 0.02\n",
            "iteration: 275750 loss: 0.0054 lr: 0.02\n",
            "iteration: 275760 loss: 0.0044 lr: 0.02\n",
            "iteration: 275770 loss: 0.0033 lr: 0.02\n",
            "iteration: 275780 loss: 0.0035 lr: 0.02\n",
            "iteration: 275790 loss: 0.0037 lr: 0.02\n",
            "iteration: 275800 loss: 0.0034 lr: 0.02\n",
            "iteration: 275810 loss: 0.0042 lr: 0.02\n",
            "iteration: 275820 loss: 0.0028 lr: 0.02\n",
            "iteration: 275830 loss: 0.0030 lr: 0.02\n",
            "iteration: 275840 loss: 0.0039 lr: 0.02\n",
            "iteration: 275850 loss: 0.0029 lr: 0.02\n",
            "iteration: 275860 loss: 0.0033 lr: 0.02\n",
            "iteration: 275870 loss: 0.0033 lr: 0.02\n",
            "iteration: 275880 loss: 0.0037 lr: 0.02\n",
            "iteration: 275890 loss: 0.0031 lr: 0.02\n",
            "iteration: 275900 loss: 0.0036 lr: 0.02\n",
            "iteration: 275910 loss: 0.0043 lr: 0.02\n",
            "iteration: 275920 loss: 0.0031 lr: 0.02\n",
            "iteration: 275930 loss: 0.0054 lr: 0.02\n",
            "iteration: 275940 loss: 0.0042 lr: 0.02\n",
            "iteration: 275950 loss: 0.0045 lr: 0.02\n",
            "iteration: 275960 loss: 0.0035 lr: 0.02\n",
            "iteration: 275970 loss: 0.0035 lr: 0.02\n",
            "iteration: 275980 loss: 0.0034 lr: 0.02\n",
            "iteration: 275990 loss: 0.0045 lr: 0.02\n",
            "iteration: 276000 loss: 0.0047 lr: 0.02\n",
            "iteration: 276010 loss: 0.0031 lr: 0.02\n",
            "iteration: 276020 loss: 0.0033 lr: 0.02\n",
            "iteration: 276030 loss: 0.0035 lr: 0.02\n",
            "iteration: 276040 loss: 0.0039 lr: 0.02\n",
            "iteration: 276050 loss: 0.0043 lr: 0.02\n",
            "iteration: 276060 loss: 0.0036 lr: 0.02\n",
            "iteration: 276070 loss: 0.0034 lr: 0.02\n",
            "iteration: 276080 loss: 0.0048 lr: 0.02\n",
            "iteration: 276090 loss: 0.0050 lr: 0.02\n",
            "iteration: 276100 loss: 0.0029 lr: 0.02\n",
            "iteration: 276110 loss: 0.0033 lr: 0.02\n",
            "iteration: 276120 loss: 0.0037 lr: 0.02\n",
            "iteration: 276130 loss: 0.0040 lr: 0.02\n",
            "iteration: 276140 loss: 0.0041 lr: 0.02\n",
            "iteration: 276150 loss: 0.0035 lr: 0.02\n",
            "iteration: 276160 loss: 0.0036 lr: 0.02\n",
            "iteration: 276170 loss: 0.0035 lr: 0.02\n",
            "iteration: 276180 loss: 0.0041 lr: 0.02\n",
            "iteration: 276190 loss: 0.0032 lr: 0.02\n",
            "iteration: 276200 loss: 0.0032 lr: 0.02\n",
            "iteration: 276210 loss: 0.0035 lr: 0.02\n",
            "iteration: 276220 loss: 0.0039 lr: 0.02\n",
            "iteration: 276230 loss: 0.0042 lr: 0.02\n",
            "iteration: 276240 loss: 0.0039 lr: 0.02\n",
            "iteration: 276250 loss: 0.0039 lr: 0.02\n",
            "iteration: 276260 loss: 0.0033 lr: 0.02\n",
            "iteration: 276270 loss: 0.0034 lr: 0.02\n",
            "iteration: 276280 loss: 0.0031 lr: 0.02\n",
            "iteration: 276290 loss: 0.0036 lr: 0.02\n",
            "iteration: 276300 loss: 0.0032 lr: 0.02\n",
            "iteration: 276310 loss: 0.0034 lr: 0.02\n",
            "iteration: 276320 loss: 0.0033 lr: 0.02\n",
            "iteration: 276330 loss: 0.0036 lr: 0.02\n",
            "iteration: 276340 loss: 0.0048 lr: 0.02\n",
            "iteration: 276350 loss: 0.0033 lr: 0.02\n",
            "iteration: 276360 loss: 0.0041 lr: 0.02\n",
            "iteration: 276370 loss: 0.0031 lr: 0.02\n",
            "iteration: 276380 loss: 0.0042 lr: 0.02\n",
            "iteration: 276390 loss: 0.0034 lr: 0.02\n",
            "iteration: 276400 loss: 0.0043 lr: 0.02\n",
            "iteration: 276410 loss: 0.0034 lr: 0.02\n",
            "iteration: 276420 loss: 0.0039 lr: 0.02\n",
            "iteration: 276430 loss: 0.0035 lr: 0.02\n",
            "iteration: 276440 loss: 0.0033 lr: 0.02\n",
            "iteration: 276450 loss: 0.0035 lr: 0.02\n",
            "iteration: 276460 loss: 0.0030 lr: 0.02\n",
            "iteration: 276470 loss: 0.0039 lr: 0.02\n",
            "iteration: 276480 loss: 0.0040 lr: 0.02\n",
            "iteration: 276490 loss: 0.0021 lr: 0.02\n",
            "iteration: 276500 loss: 0.0043 lr: 0.02\n",
            "iteration: 276510 loss: 0.0029 lr: 0.02\n",
            "iteration: 276520 loss: 0.0032 lr: 0.02\n",
            "iteration: 276530 loss: 0.0034 lr: 0.02\n",
            "iteration: 276540 loss: 0.0033 lr: 0.02\n",
            "iteration: 276550 loss: 0.0038 lr: 0.02\n",
            "iteration: 276560 loss: 0.0038 lr: 0.02\n",
            "iteration: 276570 loss: 0.0030 lr: 0.02\n",
            "iteration: 276580 loss: 0.0037 lr: 0.02\n",
            "iteration: 276590 loss: 0.0033 lr: 0.02\n",
            "iteration: 276600 loss: 0.0041 lr: 0.02\n",
            "iteration: 276610 loss: 0.0030 lr: 0.02\n",
            "iteration: 276620 loss: 0.0037 lr: 0.02\n",
            "iteration: 276630 loss: 0.0035 lr: 0.02\n",
            "iteration: 276640 loss: 0.0036 lr: 0.02\n",
            "iteration: 276650 loss: 0.0040 lr: 0.02\n",
            "iteration: 276660 loss: 0.0041 lr: 0.02\n",
            "iteration: 276670 loss: 0.0034 lr: 0.02\n",
            "iteration: 276680 loss: 0.0034 lr: 0.02\n",
            "iteration: 276690 loss: 0.0026 lr: 0.02\n",
            "iteration: 276700 loss: 0.0037 lr: 0.02\n",
            "iteration: 276710 loss: 0.0033 lr: 0.02\n",
            "iteration: 276720 loss: 0.0036 lr: 0.02\n",
            "iteration: 276730 loss: 0.0037 lr: 0.02\n",
            "iteration: 276740 loss: 0.0031 lr: 0.02\n",
            "iteration: 276750 loss: 0.0044 lr: 0.02\n",
            "iteration: 276760 loss: 0.0039 lr: 0.02\n",
            "iteration: 276770 loss: 0.0042 lr: 0.02\n",
            "iteration: 276780 loss: 0.0031 lr: 0.02\n",
            "iteration: 276790 loss: 0.0044 lr: 0.02\n",
            "iteration: 276800 loss: 0.0035 lr: 0.02\n",
            "iteration: 276810 loss: 0.0034 lr: 0.02\n",
            "iteration: 276820 loss: 0.0030 lr: 0.02\n",
            "iteration: 276830 loss: 0.0037 lr: 0.02\n",
            "iteration: 276840 loss: 0.0034 lr: 0.02\n",
            "iteration: 276850 loss: 0.0035 lr: 0.02\n",
            "iteration: 276860 loss: 0.0029 lr: 0.02\n",
            "iteration: 276870 loss: 0.0036 lr: 0.02\n",
            "iteration: 276880 loss: 0.0040 lr: 0.02\n",
            "iteration: 276890 loss: 0.0034 lr: 0.02\n",
            "iteration: 276900 loss: 0.0035 lr: 0.02\n",
            "iteration: 276910 loss: 0.0030 lr: 0.02\n",
            "iteration: 276920 loss: 0.0039 lr: 0.02\n",
            "iteration: 276930 loss: 0.0040 lr: 0.02\n",
            "iteration: 276940 loss: 0.0039 lr: 0.02\n",
            "iteration: 276950 loss: 0.0034 lr: 0.02\n",
            "iteration: 276960 loss: 0.0037 lr: 0.02\n",
            "iteration: 276970 loss: 0.0036 lr: 0.02\n",
            "iteration: 276980 loss: 0.0045 lr: 0.02\n",
            "iteration: 276990 loss: 0.0046 lr: 0.02\n",
            "iteration: 277000 loss: 0.0032 lr: 0.02\n",
            "iteration: 277010 loss: 0.0030 lr: 0.02\n",
            "iteration: 277020 loss: 0.0032 lr: 0.02\n",
            "iteration: 277030 loss: 0.0036 lr: 0.02\n",
            "iteration: 277040 loss: 0.0036 lr: 0.02\n",
            "iteration: 277050 loss: 0.0041 lr: 0.02\n",
            "iteration: 277060 loss: 0.0039 lr: 0.02\n",
            "iteration: 277070 loss: 0.0040 lr: 0.02\n",
            "iteration: 277080 loss: 0.0039 lr: 0.02\n",
            "iteration: 277090 loss: 0.0034 lr: 0.02\n",
            "iteration: 277100 loss: 0.0039 lr: 0.02\n",
            "iteration: 277110 loss: 0.0029 lr: 0.02\n",
            "iteration: 277120 loss: 0.0044 lr: 0.02\n",
            "iteration: 277130 loss: 0.0029 lr: 0.02\n",
            "iteration: 277140 loss: 0.0042 lr: 0.02\n",
            "iteration: 277150 loss: 0.0036 lr: 0.02\n",
            "iteration: 277160 loss: 0.0050 lr: 0.02\n",
            "iteration: 277170 loss: 0.0041 lr: 0.02\n",
            "iteration: 277180 loss: 0.0041 lr: 0.02\n",
            "iteration: 277190 loss: 0.0030 lr: 0.02\n",
            "iteration: 277200 loss: 0.0032 lr: 0.02\n",
            "iteration: 277210 loss: 0.0035 lr: 0.02\n",
            "iteration: 277220 loss: 0.0038 lr: 0.02\n",
            "iteration: 277230 loss: 0.0037 lr: 0.02\n",
            "iteration: 277240 loss: 0.0038 lr: 0.02\n",
            "iteration: 277250 loss: 0.0035 lr: 0.02\n",
            "iteration: 277260 loss: 0.0038 lr: 0.02\n",
            "iteration: 277270 loss: 0.0043 lr: 0.02\n",
            "iteration: 277280 loss: 0.0039 lr: 0.02\n",
            "iteration: 277290 loss: 0.0044 lr: 0.02\n",
            "iteration: 277300 loss: 0.0053 lr: 0.02\n",
            "iteration: 277310 loss: 0.0038 lr: 0.02\n",
            "iteration: 277320 loss: 0.0024 lr: 0.02\n",
            "iteration: 277330 loss: 0.0037 lr: 0.02\n",
            "iteration: 277340 loss: 0.0031 lr: 0.02\n",
            "iteration: 277350 loss: 0.0030 lr: 0.02\n",
            "iteration: 277360 loss: 0.0046 lr: 0.02\n",
            "iteration: 277370 loss: 0.0034 lr: 0.02\n",
            "iteration: 277380 loss: 0.0037 lr: 0.02\n",
            "iteration: 277390 loss: 0.0029 lr: 0.02\n",
            "iteration: 277400 loss: 0.0035 lr: 0.02\n",
            "iteration: 277410 loss: 0.0045 lr: 0.02\n",
            "iteration: 277420 loss: 0.0040 lr: 0.02\n",
            "iteration: 277430 loss: 0.0029 lr: 0.02\n",
            "iteration: 277440 loss: 0.0034 lr: 0.02\n",
            "iteration: 277450 loss: 0.0043 lr: 0.02\n",
            "iteration: 277460 loss: 0.0035 lr: 0.02\n",
            "iteration: 277470 loss: 0.0028 lr: 0.02\n",
            "iteration: 277480 loss: 0.0035 lr: 0.02\n",
            "iteration: 277490 loss: 0.0036 lr: 0.02\n",
            "iteration: 277500 loss: 0.0024 lr: 0.02\n",
            "iteration: 277510 loss: 0.0031 lr: 0.02\n",
            "iteration: 277520 loss: 0.0032 lr: 0.02\n",
            "iteration: 277530 loss: 0.0038 lr: 0.02\n",
            "iteration: 277540 loss: 0.0043 lr: 0.02\n",
            "iteration: 277550 loss: 0.0040 lr: 0.02\n",
            "iteration: 277560 loss: 0.0033 lr: 0.02\n",
            "iteration: 277570 loss: 0.0040 lr: 0.02\n",
            "iteration: 277580 loss: 0.0040 lr: 0.02\n",
            "iteration: 277590 loss: 0.0032 lr: 0.02\n",
            "iteration: 277600 loss: 0.0039 lr: 0.02\n",
            "iteration: 277610 loss: 0.0046 lr: 0.02\n",
            "iteration: 277620 loss: 0.0028 lr: 0.02\n",
            "iteration: 277630 loss: 0.0035 lr: 0.02\n",
            "iteration: 277640 loss: 0.0029 lr: 0.02\n",
            "iteration: 277650 loss: 0.0036 lr: 0.02\n",
            "iteration: 277660 loss: 0.0034 lr: 0.02\n",
            "iteration: 277670 loss: 0.0036 lr: 0.02\n",
            "iteration: 277680 loss: 0.0044 lr: 0.02\n",
            "iteration: 277690 loss: 0.0038 lr: 0.02\n",
            "iteration: 277700 loss: 0.0034 lr: 0.02\n",
            "iteration: 277710 loss: 0.0035 lr: 0.02\n",
            "iteration: 277720 loss: 0.0033 lr: 0.02\n",
            "iteration: 277730 loss: 0.0045 lr: 0.02\n",
            "iteration: 277740 loss: 0.0044 lr: 0.02\n",
            "iteration: 277750 loss: 0.0034 lr: 0.02\n",
            "iteration: 277760 loss: 0.0027 lr: 0.02\n",
            "iteration: 277770 loss: 0.0030 lr: 0.02\n",
            "iteration: 277780 loss: 0.0030 lr: 0.02\n",
            "iteration: 277790 loss: 0.0034 lr: 0.02\n",
            "iteration: 277800 loss: 0.0032 lr: 0.02\n",
            "iteration: 277810 loss: 0.0041 lr: 0.02\n",
            "iteration: 277820 loss: 0.0037 lr: 0.02\n",
            "iteration: 277830 loss: 0.0035 lr: 0.02\n",
            "iteration: 277840 loss: 0.0036 lr: 0.02\n",
            "iteration: 277850 loss: 0.0038 lr: 0.02\n",
            "iteration: 277860 loss: 0.0038 lr: 0.02\n",
            "iteration: 277870 loss: 0.0029 lr: 0.02\n",
            "iteration: 277880 loss: 0.0035 lr: 0.02\n",
            "iteration: 277890 loss: 0.0039 lr: 0.02\n",
            "iteration: 277900 loss: 0.0028 lr: 0.02\n",
            "iteration: 277910 loss: 0.0040 lr: 0.02\n",
            "iteration: 277920 loss: 0.0040 lr: 0.02\n",
            "iteration: 277930 loss: 0.0026 lr: 0.02\n",
            "iteration: 277940 loss: 0.0043 lr: 0.02\n",
            "iteration: 277950 loss: 0.0039 lr: 0.02\n",
            "iteration: 277960 loss: 0.0041 lr: 0.02\n",
            "iteration: 277970 loss: 0.0030 lr: 0.02\n",
            "iteration: 277980 loss: 0.0046 lr: 0.02\n",
            "iteration: 277990 loss: 0.0047 lr: 0.02\n",
            "iteration: 278000 loss: 0.0039 lr: 0.02\n",
            "iteration: 278010 loss: 0.0025 lr: 0.02\n",
            "iteration: 278020 loss: 0.0034 lr: 0.02\n",
            "iteration: 278030 loss: 0.0041 lr: 0.02\n",
            "iteration: 278040 loss: 0.0040 lr: 0.02\n",
            "iteration: 278050 loss: 0.0044 lr: 0.02\n",
            "iteration: 278060 loss: 0.0038 lr: 0.02\n",
            "iteration: 278070 loss: 0.0047 lr: 0.02\n",
            "iteration: 278080 loss: 0.0043 lr: 0.02\n",
            "iteration: 278090 loss: 0.0038 lr: 0.02\n",
            "iteration: 278100 loss: 0.0036 lr: 0.02\n",
            "iteration: 278110 loss: 0.0040 lr: 0.02\n",
            "iteration: 278120 loss: 0.0035 lr: 0.02\n",
            "iteration: 278130 loss: 0.0037 lr: 0.02\n",
            "iteration: 278140 loss: 0.0033 lr: 0.02\n",
            "iteration: 278150 loss: 0.0029 lr: 0.02\n",
            "iteration: 278160 loss: 0.0039 lr: 0.02\n",
            "iteration: 278170 loss: 0.0028 lr: 0.02\n",
            "iteration: 278180 loss: 0.0038 lr: 0.02\n",
            "iteration: 278190 loss: 0.0039 lr: 0.02\n",
            "iteration: 278200 loss: 0.0035 lr: 0.02\n",
            "iteration: 278210 loss: 0.0030 lr: 0.02\n",
            "iteration: 278220 loss: 0.0037 lr: 0.02\n",
            "iteration: 278230 loss: 0.0032 lr: 0.02\n",
            "iteration: 278240 loss: 0.0038 lr: 0.02\n",
            "iteration: 278250 loss: 0.0036 lr: 0.02\n",
            "iteration: 278260 loss: 0.0036 lr: 0.02\n",
            "iteration: 278270 loss: 0.0038 lr: 0.02\n",
            "iteration: 278280 loss: 0.0029 lr: 0.02\n",
            "iteration: 278290 loss: 0.0034 lr: 0.02\n",
            "iteration: 278300 loss: 0.0030 lr: 0.02\n",
            "iteration: 278310 loss: 0.0033 lr: 0.02\n",
            "iteration: 278320 loss: 0.0032 lr: 0.02\n",
            "iteration: 278330 loss: 0.0033 lr: 0.02\n",
            "iteration: 278340 loss: 0.0044 lr: 0.02\n",
            "iteration: 278350 loss: 0.0034 lr: 0.02\n",
            "iteration: 278360 loss: 0.0031 lr: 0.02\n",
            "iteration: 278370 loss: 0.0046 lr: 0.02\n",
            "iteration: 278380 loss: 0.0040 lr: 0.02\n",
            "iteration: 278390 loss: 0.0035 lr: 0.02\n",
            "iteration: 278400 loss: 0.0044 lr: 0.02\n",
            "iteration: 278410 loss: 0.0047 lr: 0.02\n",
            "iteration: 278420 loss: 0.0038 lr: 0.02\n",
            "iteration: 278430 loss: 0.0036 lr: 0.02\n",
            "iteration: 278440 loss: 0.0034 lr: 0.02\n",
            "iteration: 278450 loss: 0.0034 lr: 0.02\n",
            "iteration: 278460 loss: 0.0046 lr: 0.02\n",
            "iteration: 278470 loss: 0.0039 lr: 0.02\n",
            "iteration: 278480 loss: 0.0033 lr: 0.02\n",
            "iteration: 278490 loss: 0.0035 lr: 0.02\n",
            "iteration: 278500 loss: 0.0033 lr: 0.02\n",
            "iteration: 278510 loss: 0.0031 lr: 0.02\n",
            "iteration: 278520 loss: 0.0038 lr: 0.02\n",
            "iteration: 278530 loss: 0.0037 lr: 0.02\n",
            "iteration: 278540 loss: 0.0032 lr: 0.02\n",
            "iteration: 278550 loss: 0.0038 lr: 0.02\n",
            "iteration: 278560 loss: 0.0038 lr: 0.02\n",
            "iteration: 278570 loss: 0.0034 lr: 0.02\n",
            "iteration: 278580 loss: 0.0034 lr: 0.02\n",
            "iteration: 278590 loss: 0.0029 lr: 0.02\n",
            "iteration: 278600 loss: 0.0038 lr: 0.02\n",
            "iteration: 278610 loss: 0.0030 lr: 0.02\n",
            "iteration: 278620 loss: 0.0039 lr: 0.02\n",
            "iteration: 278630 loss: 0.0025 lr: 0.02\n",
            "iteration: 278640 loss: 0.0035 lr: 0.02\n",
            "iteration: 278650 loss: 0.0043 lr: 0.02\n",
            "iteration: 278660 loss: 0.0031 lr: 0.02\n",
            "iteration: 278670 loss: 0.0030 lr: 0.02\n",
            "iteration: 278680 loss: 0.0041 lr: 0.02\n",
            "iteration: 278690 loss: 0.0029 lr: 0.02\n",
            "iteration: 278700 loss: 0.0041 lr: 0.02\n",
            "iteration: 278710 loss: 0.0026 lr: 0.02\n",
            "iteration: 278720 loss: 0.0029 lr: 0.02\n",
            "iteration: 278730 loss: 0.0024 lr: 0.02\n",
            "iteration: 278740 loss: 0.0029 lr: 0.02\n",
            "iteration: 278750 loss: 0.0033 lr: 0.02\n",
            "iteration: 278760 loss: 0.0032 lr: 0.02\n",
            "iteration: 278770 loss: 0.0029 lr: 0.02\n",
            "iteration: 278780 loss: 0.0036 lr: 0.02\n",
            "iteration: 278790 loss: 0.0031 lr: 0.02\n",
            "iteration: 278800 loss: 0.0039 lr: 0.02\n",
            "iteration: 278810 loss: 0.0032 lr: 0.02\n",
            "iteration: 278820 loss: 0.0044 lr: 0.02\n",
            "iteration: 278830 loss: 0.0040 lr: 0.02\n",
            "iteration: 278840 loss: 0.0036 lr: 0.02\n",
            "iteration: 278850 loss: 0.0033 lr: 0.02\n",
            "iteration: 278860 loss: 0.0048 lr: 0.02\n",
            "iteration: 278870 loss: 0.0027 lr: 0.02\n",
            "iteration: 278880 loss: 0.0032 lr: 0.02\n",
            "iteration: 278890 loss: 0.0039 lr: 0.02\n",
            "iteration: 278900 loss: 0.0034 lr: 0.02\n",
            "iteration: 278910 loss: 0.0039 lr: 0.02\n",
            "iteration: 278920 loss: 0.0032 lr: 0.02\n",
            "iteration: 278930 loss: 0.0034 lr: 0.02\n",
            "iteration: 278940 loss: 0.0030 lr: 0.02\n",
            "iteration: 278950 loss: 0.0040 lr: 0.02\n",
            "iteration: 278960 loss: 0.0036 lr: 0.02\n",
            "iteration: 278970 loss: 0.0035 lr: 0.02\n",
            "iteration: 278980 loss: 0.0032 lr: 0.02\n",
            "iteration: 278990 loss: 0.0027 lr: 0.02\n",
            "iteration: 279000 loss: 0.0036 lr: 0.02\n",
            "iteration: 279010 loss: 0.0037 lr: 0.02\n",
            "iteration: 279020 loss: 0.0025 lr: 0.02\n",
            "iteration: 279030 loss: 0.0053 lr: 0.02\n",
            "iteration: 279040 loss: 0.0031 lr: 0.02\n",
            "iteration: 279050 loss: 0.0046 lr: 0.02\n",
            "iteration: 279060 loss: 0.0031 lr: 0.02\n",
            "iteration: 279070 loss: 0.0038 lr: 0.02\n",
            "iteration: 279080 loss: 0.0028 lr: 0.02\n",
            "iteration: 279090 loss: 0.0034 lr: 0.02\n",
            "iteration: 279100 loss: 0.0035 lr: 0.02\n",
            "iteration: 279110 loss: 0.0042 lr: 0.02\n",
            "iteration: 279120 loss: 0.0034 lr: 0.02\n",
            "iteration: 279130 loss: 0.0029 lr: 0.02\n",
            "iteration: 279140 loss: 0.0039 lr: 0.02\n",
            "iteration: 279150 loss: 0.0041 lr: 0.02\n",
            "iteration: 279160 loss: 0.0035 lr: 0.02\n",
            "iteration: 279170 loss: 0.0032 lr: 0.02\n",
            "iteration: 279180 loss: 0.0029 lr: 0.02\n",
            "iteration: 279190 loss: 0.0037 lr: 0.02\n",
            "iteration: 279200 loss: 0.0039 lr: 0.02\n",
            "iteration: 279210 loss: 0.0040 lr: 0.02\n",
            "iteration: 279220 loss: 0.0037 lr: 0.02\n",
            "iteration: 279230 loss: 0.0044 lr: 0.02\n",
            "iteration: 279240 loss: 0.0040 lr: 0.02\n",
            "iteration: 279250 loss: 0.0031 lr: 0.02\n",
            "iteration: 279260 loss: 0.0043 lr: 0.02\n",
            "iteration: 279270 loss: 0.0048 lr: 0.02\n",
            "iteration: 279280 loss: 0.0030 lr: 0.02\n",
            "iteration: 279290 loss: 0.0027 lr: 0.02\n",
            "iteration: 279300 loss: 0.0033 lr: 0.02\n",
            "iteration: 279310 loss: 0.0036 lr: 0.02\n",
            "iteration: 279320 loss: 0.0037 lr: 0.02\n",
            "iteration: 279330 loss: 0.0039 lr: 0.02\n",
            "iteration: 279340 loss: 0.0039 lr: 0.02\n",
            "iteration: 279350 loss: 0.0035 lr: 0.02\n",
            "iteration: 279360 loss: 0.0033 lr: 0.02\n",
            "iteration: 279370 loss: 0.0040 lr: 0.02\n",
            "iteration: 279380 loss: 0.0040 lr: 0.02\n",
            "iteration: 279390 loss: 0.0037 lr: 0.02\n",
            "iteration: 279400 loss: 0.0031 lr: 0.02\n",
            "iteration: 279410 loss: 0.0034 lr: 0.02\n",
            "iteration: 279420 loss: 0.0038 lr: 0.02\n",
            "iteration: 279430 loss: 0.0042 lr: 0.02\n",
            "iteration: 279440 loss: 0.0033 lr: 0.02\n",
            "iteration: 279450 loss: 0.0035 lr: 0.02\n",
            "iteration: 279460 loss: 0.0057 lr: 0.02\n",
            "iteration: 279470 loss: 0.0034 lr: 0.02\n",
            "iteration: 279480 loss: 0.0041 lr: 0.02\n",
            "iteration: 279490 loss: 0.0039 lr: 0.02\n",
            "iteration: 279500 loss: 0.0033 lr: 0.02\n",
            "iteration: 279510 loss: 0.0031 lr: 0.02\n",
            "iteration: 279520 loss: 0.0029 lr: 0.02\n",
            "iteration: 279530 loss: 0.0034 lr: 0.02\n",
            "iteration: 279540 loss: 0.0024 lr: 0.02\n",
            "iteration: 279550 loss: 0.0042 lr: 0.02\n",
            "iteration: 279560 loss: 0.0033 lr: 0.02\n",
            "iteration: 279570 loss: 0.0041 lr: 0.02\n",
            "iteration: 279580 loss: 0.0031 lr: 0.02\n",
            "iteration: 279590 loss: 0.0035 lr: 0.02\n",
            "iteration: 279600 loss: 0.0033 lr: 0.02\n",
            "iteration: 279610 loss: 0.0049 lr: 0.02\n",
            "iteration: 279620 loss: 0.0028 lr: 0.02\n",
            "iteration: 279630 loss: 0.0032 lr: 0.02\n",
            "iteration: 279640 loss: 0.0026 lr: 0.02\n",
            "iteration: 279650 loss: 0.0036 lr: 0.02\n",
            "iteration: 279660 loss: 0.0037 lr: 0.02\n",
            "iteration: 279670 loss: 0.0052 lr: 0.02\n",
            "iteration: 279680 loss: 0.0037 lr: 0.02\n",
            "iteration: 279690 loss: 0.0037 lr: 0.02\n",
            "iteration: 279700 loss: 0.0038 lr: 0.02\n",
            "iteration: 279710 loss: 0.0031 lr: 0.02\n",
            "iteration: 279720 loss: 0.0038 lr: 0.02\n",
            "iteration: 279730 loss: 0.0043 lr: 0.02\n",
            "iteration: 279740 loss: 0.0035 lr: 0.02\n",
            "iteration: 279750 loss: 0.0049 lr: 0.02\n",
            "iteration: 279760 loss: 0.0031 lr: 0.02\n",
            "iteration: 279770 loss: 0.0036 lr: 0.02\n",
            "iteration: 279780 loss: 0.0042 lr: 0.02\n",
            "iteration: 279790 loss: 0.0031 lr: 0.02\n",
            "iteration: 279800 loss: 0.0035 lr: 0.02\n",
            "iteration: 279810 loss: 0.0031 lr: 0.02\n",
            "iteration: 279820 loss: 0.0040 lr: 0.02\n",
            "iteration: 279830 loss: 0.0035 lr: 0.02\n",
            "iteration: 279840 loss: 0.0031 lr: 0.02\n",
            "iteration: 279850 loss: 0.0028 lr: 0.02\n",
            "iteration: 279860 loss: 0.0032 lr: 0.02\n",
            "iteration: 279870 loss: 0.0030 lr: 0.02\n",
            "iteration: 279880 loss: 0.0031 lr: 0.02\n",
            "iteration: 279890 loss: 0.0036 lr: 0.02\n",
            "iteration: 279900 loss: 0.0033 lr: 0.02\n",
            "iteration: 279910 loss: 0.0040 lr: 0.02\n",
            "iteration: 279920 loss: 0.0028 lr: 0.02\n",
            "iteration: 279930 loss: 0.0028 lr: 0.02\n",
            "iteration: 279940 loss: 0.0030 lr: 0.02\n",
            "iteration: 279950 loss: 0.0037 lr: 0.02\n",
            "iteration: 279960 loss: 0.0036 lr: 0.02\n",
            "iteration: 279970 loss: 0.0024 lr: 0.02\n",
            "iteration: 279980 loss: 0.0036 lr: 0.02\n",
            "iteration: 279990 loss: 0.0030 lr: 0.02\n",
            "iteration: 280000 loss: 0.0039 lr: 0.02\n",
            "iteration: 280010 loss: 0.0035 lr: 0.02\n",
            "iteration: 280020 loss: 0.0037 lr: 0.02\n",
            "iteration: 280030 loss: 0.0038 lr: 0.02\n",
            "iteration: 280040 loss: 0.0031 lr: 0.02\n",
            "iteration: 280050 loss: 0.0030 lr: 0.02\n",
            "iteration: 280060 loss: 0.0038 lr: 0.02\n",
            "iteration: 280070 loss: 0.0048 lr: 0.02\n",
            "iteration: 280080 loss: 0.0036 lr: 0.02\n",
            "iteration: 280090 loss: 0.0030 lr: 0.02\n",
            "iteration: 280100 loss: 0.0038 lr: 0.02\n",
            "iteration: 280110 loss: 0.0042 lr: 0.02\n",
            "iteration: 280120 loss: 0.0059 lr: 0.02\n",
            "iteration: 280130 loss: 0.0034 lr: 0.02\n",
            "iteration: 280140 loss: 0.0040 lr: 0.02\n",
            "iteration: 280150 loss: 0.0035 lr: 0.02\n",
            "iteration: 280160 loss: 0.0033 lr: 0.02\n",
            "iteration: 280170 loss: 0.0048 lr: 0.02\n",
            "iteration: 280180 loss: 0.0039 lr: 0.02\n",
            "iteration: 280190 loss: 0.0036 lr: 0.02\n",
            "iteration: 280200 loss: 0.0046 lr: 0.02\n",
            "iteration: 280210 loss: 0.0032 lr: 0.02\n",
            "iteration: 280220 loss: 0.0037 lr: 0.02\n",
            "iteration: 280230 loss: 0.0033 lr: 0.02\n",
            "iteration: 280240 loss: 0.0032 lr: 0.02\n",
            "iteration: 280250 loss: 0.0040 lr: 0.02\n",
            "iteration: 280260 loss: 0.0038 lr: 0.02\n",
            "iteration: 280270 loss: 0.0035 lr: 0.02\n",
            "iteration: 280280 loss: 0.0039 lr: 0.02\n",
            "iteration: 280290 loss: 0.0029 lr: 0.02\n",
            "iteration: 280300 loss: 0.0025 lr: 0.02\n",
            "iteration: 280310 loss: 0.0044 lr: 0.02\n",
            "iteration: 280320 loss: 0.0023 lr: 0.02\n",
            "iteration: 280330 loss: 0.0041 lr: 0.02\n",
            "iteration: 280340 loss: 0.0038 lr: 0.02\n",
            "iteration: 280350 loss: 0.0035 lr: 0.02\n",
            "iteration: 280360 loss: 0.0037 lr: 0.02\n",
            "iteration: 280370 loss: 0.0029 lr: 0.02\n",
            "iteration: 280380 loss: 0.0035 lr: 0.02\n",
            "iteration: 280390 loss: 0.0040 lr: 0.02\n",
            "iteration: 280400 loss: 0.0032 lr: 0.02\n",
            "iteration: 280410 loss: 0.0033 lr: 0.02\n",
            "iteration: 280420 loss: 0.0036 lr: 0.02\n",
            "iteration: 280430 loss: 0.0040 lr: 0.02\n",
            "iteration: 280440 loss: 0.0034 lr: 0.02\n",
            "iteration: 280450 loss: 0.0037 lr: 0.02\n",
            "iteration: 280460 loss: 0.0025 lr: 0.02\n",
            "iteration: 280470 loss: 0.0029 lr: 0.02\n",
            "iteration: 280480 loss: 0.0039 lr: 0.02\n",
            "iteration: 280490 loss: 0.0039 lr: 0.02\n",
            "iteration: 280500 loss: 0.0032 lr: 0.02\n",
            "iteration: 280510 loss: 0.0029 lr: 0.02\n",
            "iteration: 280520 loss: 0.0033 lr: 0.02\n",
            "iteration: 280530 loss: 0.0036 lr: 0.02\n",
            "iteration: 280540 loss: 0.0035 lr: 0.02\n",
            "iteration: 280550 loss: 0.0025 lr: 0.02\n",
            "iteration: 280560 loss: 0.0038 lr: 0.02\n",
            "iteration: 280570 loss: 0.0032 lr: 0.02\n",
            "iteration: 280580 loss: 0.0038 lr: 0.02\n",
            "iteration: 280590 loss: 0.0043 lr: 0.02\n",
            "iteration: 280600 loss: 0.0040 lr: 0.02\n",
            "iteration: 280610 loss: 0.0043 lr: 0.02\n",
            "iteration: 280620 loss: 0.0043 lr: 0.02\n",
            "iteration: 280630 loss: 0.0031 lr: 0.02\n",
            "iteration: 280640 loss: 0.0036 lr: 0.02\n",
            "iteration: 280650 loss: 0.0034 lr: 0.02\n",
            "iteration: 280660 loss: 0.0025 lr: 0.02\n",
            "iteration: 280670 loss: 0.0039 lr: 0.02\n",
            "iteration: 280680 loss: 0.0036 lr: 0.02\n",
            "iteration: 280690 loss: 0.0029 lr: 0.02\n",
            "iteration: 280700 loss: 0.0033 lr: 0.02\n",
            "iteration: 280710 loss: 0.0034 lr: 0.02\n",
            "iteration: 280720 loss: 0.0036 lr: 0.02\n",
            "iteration: 280730 loss: 0.0036 lr: 0.02\n",
            "iteration: 280740 loss: 0.0046 lr: 0.02\n",
            "iteration: 280750 loss: 0.0035 lr: 0.02\n",
            "iteration: 280760 loss: 0.0039 lr: 0.02\n",
            "iteration: 280770 loss: 0.0033 lr: 0.02\n",
            "iteration: 280780 loss: 0.0037 lr: 0.02\n",
            "iteration: 280790 loss: 0.0027 lr: 0.02\n",
            "iteration: 280800 loss: 0.0036 lr: 0.02\n",
            "iteration: 280810 loss: 0.0040 lr: 0.02\n",
            "iteration: 280820 loss: 0.0033 lr: 0.02\n",
            "iteration: 280830 loss: 0.0039 lr: 0.02\n",
            "iteration: 280840 loss: 0.0045 lr: 0.02\n",
            "iteration: 280850 loss: 0.0029 lr: 0.02\n",
            "iteration: 280860 loss: 0.0038 lr: 0.02\n",
            "iteration: 280870 loss: 0.0033 lr: 0.02\n",
            "iteration: 280880 loss: 0.0038 lr: 0.02\n",
            "iteration: 280890 loss: 0.0042 lr: 0.02\n",
            "iteration: 280900 loss: 0.0035 lr: 0.02\n",
            "iteration: 280910 loss: 0.0030 lr: 0.02\n",
            "iteration: 280920 loss: 0.0033 lr: 0.02\n",
            "iteration: 280930 loss: 0.0045 lr: 0.02\n",
            "iteration: 280940 loss: 0.0040 lr: 0.02\n",
            "iteration: 280950 loss: 0.0034 lr: 0.02\n",
            "iteration: 280960 loss: 0.0039 lr: 0.02\n",
            "iteration: 280970 loss: 0.0053 lr: 0.02\n",
            "iteration: 280980 loss: 0.0041 lr: 0.02\n",
            "iteration: 280990 loss: 0.0038 lr: 0.02\n",
            "iteration: 281000 loss: 0.0031 lr: 0.02\n",
            "iteration: 281010 loss: 0.0029 lr: 0.02\n",
            "iteration: 281020 loss: 0.0039 lr: 0.02\n",
            "iteration: 281030 loss: 0.0038 lr: 0.02\n",
            "iteration: 281040 loss: 0.0039 lr: 0.02\n",
            "iteration: 281050 loss: 0.0044 lr: 0.02\n",
            "iteration: 281060 loss: 0.0034 lr: 0.02\n",
            "iteration: 281070 loss: 0.0036 lr: 0.02\n",
            "iteration: 281080 loss: 0.0035 lr: 0.02\n",
            "iteration: 281090 loss: 0.0040 lr: 0.02\n",
            "iteration: 281100 loss: 0.0032 lr: 0.02\n",
            "iteration: 281110 loss: 0.0039 lr: 0.02\n",
            "iteration: 281120 loss: 0.0038 lr: 0.02\n",
            "iteration: 281130 loss: 0.0035 lr: 0.02\n",
            "iteration: 281140 loss: 0.0028 lr: 0.02\n",
            "iteration: 281150 loss: 0.0034 lr: 0.02\n",
            "iteration: 281160 loss: 0.0038 lr: 0.02\n",
            "iteration: 281170 loss: 0.0033 lr: 0.02\n",
            "iteration: 281180 loss: 0.0031 lr: 0.02\n",
            "iteration: 281190 loss: 0.0032 lr: 0.02\n",
            "iteration: 281200 loss: 0.0032 lr: 0.02\n",
            "iteration: 281210 loss: 0.0028 lr: 0.02\n",
            "iteration: 281220 loss: 0.0033 lr: 0.02\n",
            "iteration: 281230 loss: 0.0031 lr: 0.02\n",
            "iteration: 281240 loss: 0.0039 lr: 0.02\n",
            "iteration: 281250 loss: 0.0033 lr: 0.02\n",
            "iteration: 281260 loss: 0.0030 lr: 0.02\n",
            "iteration: 281270 loss: 0.0036 lr: 0.02\n",
            "iteration: 281280 loss: 0.0036 lr: 0.02\n",
            "iteration: 281290 loss: 0.0045 lr: 0.02\n",
            "iteration: 281300 loss: 0.0028 lr: 0.02\n",
            "iteration: 281310 loss: 0.0039 lr: 0.02\n",
            "iteration: 281320 loss: 0.0034 lr: 0.02\n",
            "iteration: 281330 loss: 0.0031 lr: 0.02\n",
            "iteration: 281340 loss: 0.0046 lr: 0.02\n",
            "iteration: 281350 loss: 0.0038 lr: 0.02\n",
            "iteration: 281360 loss: 0.0042 lr: 0.02\n",
            "iteration: 281370 loss: 0.0033 lr: 0.02\n",
            "iteration: 281380 loss: 0.0039 lr: 0.02\n",
            "iteration: 281390 loss: 0.0034 lr: 0.02\n",
            "iteration: 281400 loss: 0.0034 lr: 0.02\n",
            "iteration: 281410 loss: 0.0036 lr: 0.02\n",
            "iteration: 281420 loss: 0.0028 lr: 0.02\n",
            "iteration: 281430 loss: 0.0037 lr: 0.02\n",
            "iteration: 281440 loss: 0.0039 lr: 0.02\n",
            "iteration: 281450 loss: 0.0051 lr: 0.02\n",
            "iteration: 281460 loss: 0.0045 lr: 0.02\n",
            "iteration: 281470 loss: 0.0035 lr: 0.02\n",
            "iteration: 281480 loss: 0.0041 lr: 0.02\n",
            "iteration: 281490 loss: 0.0048 lr: 0.02\n",
            "iteration: 281500 loss: 0.0047 lr: 0.02\n",
            "iteration: 281510 loss: 0.0037 lr: 0.02\n",
            "iteration: 281520 loss: 0.0033 lr: 0.02\n",
            "iteration: 281530 loss: 0.0038 lr: 0.02\n",
            "iteration: 281540 loss: 0.0036 lr: 0.02\n",
            "iteration: 281550 loss: 0.0042 lr: 0.02\n",
            "iteration: 281560 loss: 0.0035 lr: 0.02\n",
            "iteration: 281570 loss: 0.0031 lr: 0.02\n",
            "iteration: 281580 loss: 0.0046 lr: 0.02\n",
            "iteration: 281590 loss: 0.0043 lr: 0.02\n",
            "iteration: 281600 loss: 0.0028 lr: 0.02\n",
            "iteration: 281610 loss: 0.0037 lr: 0.02\n",
            "iteration: 281620 loss: 0.0035 lr: 0.02\n",
            "iteration: 281630 loss: 0.0038 lr: 0.02\n",
            "iteration: 281640 loss: 0.0029 lr: 0.02\n",
            "iteration: 281650 loss: 0.0033 lr: 0.02\n",
            "iteration: 281660 loss: 0.0033 lr: 0.02\n",
            "iteration: 281670 loss: 0.0043 lr: 0.02\n",
            "iteration: 281680 loss: 0.0034 lr: 0.02\n",
            "iteration: 281690 loss: 0.0033 lr: 0.02\n",
            "iteration: 281700 loss: 0.0032 lr: 0.02\n",
            "iteration: 281710 loss: 0.0036 lr: 0.02\n",
            "iteration: 281720 loss: 0.0041 lr: 0.02\n",
            "iteration: 281730 loss: 0.0041 lr: 0.02\n",
            "iteration: 281740 loss: 0.0025 lr: 0.02\n",
            "iteration: 281750 loss: 0.0036 lr: 0.02\n",
            "iteration: 281760 loss: 0.0031 lr: 0.02\n",
            "iteration: 281770 loss: 0.0053 lr: 0.02\n",
            "iteration: 281780 loss: 0.0033 lr: 0.02\n",
            "iteration: 281790 loss: 0.0048 lr: 0.02\n",
            "iteration: 281800 loss: 0.0048 lr: 0.02\n",
            "iteration: 281810 loss: 0.0038 lr: 0.02\n",
            "iteration: 281820 loss: 0.0037 lr: 0.02\n",
            "iteration: 281830 loss: 0.0031 lr: 0.02\n",
            "iteration: 281840 loss: 0.0030 lr: 0.02\n",
            "iteration: 281850 loss: 0.0039 lr: 0.02\n",
            "iteration: 281860 loss: 0.0030 lr: 0.02\n",
            "iteration: 281870 loss: 0.0029 lr: 0.02\n",
            "iteration: 281880 loss: 0.0044 lr: 0.02\n",
            "iteration: 281890 loss: 0.0032 lr: 0.02\n",
            "iteration: 281900 loss: 0.0036 lr: 0.02\n",
            "iteration: 281910 loss: 0.0033 lr: 0.02\n",
            "iteration: 281920 loss: 0.0044 lr: 0.02\n",
            "iteration: 281930 loss: 0.0039 lr: 0.02\n",
            "iteration: 281940 loss: 0.0050 lr: 0.02\n",
            "iteration: 281950 loss: 0.0037 lr: 0.02\n",
            "iteration: 281960 loss: 0.0026 lr: 0.02\n",
            "iteration: 281970 loss: 0.0035 lr: 0.02\n",
            "iteration: 281980 loss: 0.0037 lr: 0.02\n",
            "iteration: 281990 loss: 0.0034 lr: 0.02\n",
            "iteration: 282000 loss: 0.0033 lr: 0.02\n",
            "iteration: 282010 loss: 0.0041 lr: 0.02\n",
            "iteration: 282020 loss: 0.0038 lr: 0.02\n",
            "iteration: 282030 loss: 0.0038 lr: 0.02\n",
            "iteration: 282040 loss: 0.0023 lr: 0.02\n",
            "iteration: 282050 loss: 0.0034 lr: 0.02\n",
            "iteration: 282060 loss: 0.0038 lr: 0.02\n",
            "iteration: 282070 loss: 0.0029 lr: 0.02\n",
            "iteration: 282080 loss: 0.0032 lr: 0.02\n",
            "iteration: 282090 loss: 0.0035 lr: 0.02\n",
            "iteration: 282100 loss: 0.0038 lr: 0.02\n",
            "iteration: 282110 loss: 0.0026 lr: 0.02\n",
            "iteration: 282120 loss: 0.0040 lr: 0.02\n",
            "iteration: 282130 loss: 0.0035 lr: 0.02\n",
            "iteration: 282140 loss: 0.0033 lr: 0.02\n",
            "iteration: 282150 loss: 0.0032 lr: 0.02\n",
            "iteration: 282160 loss: 0.0036 lr: 0.02\n",
            "iteration: 282170 loss: 0.0039 lr: 0.02\n",
            "iteration: 282180 loss: 0.0042 lr: 0.02\n",
            "iteration: 282190 loss: 0.0039 lr: 0.02\n",
            "iteration: 282200 loss: 0.0037 lr: 0.02\n",
            "iteration: 282210 loss: 0.0036 lr: 0.02\n",
            "iteration: 282220 loss: 0.0051 lr: 0.02\n",
            "iteration: 282230 loss: 0.0035 lr: 0.02\n",
            "iteration: 282240 loss: 0.0039 lr: 0.02\n",
            "iteration: 282250 loss: 0.0055 lr: 0.02\n",
            "iteration: 282260 loss: 0.0048 lr: 0.02\n",
            "iteration: 282270 loss: 0.0037 lr: 0.02\n",
            "iteration: 282280 loss: 0.0038 lr: 0.02\n",
            "iteration: 282290 loss: 0.0035 lr: 0.02\n",
            "iteration: 282300 loss: 0.0031 lr: 0.02\n",
            "iteration: 282310 loss: 0.0032 lr: 0.02\n",
            "iteration: 282320 loss: 0.0040 lr: 0.02\n",
            "iteration: 282330 loss: 0.0036 lr: 0.02\n",
            "iteration: 282340 loss: 0.0026 lr: 0.02\n",
            "iteration: 282350 loss: 0.0045 lr: 0.02\n",
            "iteration: 282360 loss: 0.0034 lr: 0.02\n",
            "iteration: 282370 loss: 0.0038 lr: 0.02\n",
            "iteration: 282380 loss: 0.0053 lr: 0.02\n",
            "iteration: 282390 loss: 0.0043 lr: 0.02\n",
            "iteration: 282400 loss: 0.0038 lr: 0.02\n",
            "iteration: 282410 loss: 0.0032 lr: 0.02\n",
            "iteration: 282420 loss: 0.0028 lr: 0.02\n",
            "iteration: 282430 loss: 0.0041 lr: 0.02\n",
            "iteration: 282440 loss: 0.0033 lr: 0.02\n",
            "iteration: 282450 loss: 0.0037 lr: 0.02\n",
            "iteration: 282460 loss: 0.0034 lr: 0.02\n",
            "iteration: 282470 loss: 0.0044 lr: 0.02\n",
            "iteration: 282480 loss: 0.0030 lr: 0.02\n",
            "iteration: 282490 loss: 0.0033 lr: 0.02\n",
            "iteration: 282500 loss: 0.0032 lr: 0.02\n",
            "iteration: 282510 loss: 0.0042 lr: 0.02\n",
            "iteration: 282520 loss: 0.0040 lr: 0.02\n",
            "iteration: 282530 loss: 0.0032 lr: 0.02\n",
            "iteration: 282540 loss: 0.0041 lr: 0.02\n",
            "iteration: 282550 loss: 0.0038 lr: 0.02\n",
            "iteration: 282560 loss: 0.0033 lr: 0.02\n",
            "iteration: 282570 loss: 0.0034 lr: 0.02\n",
            "iteration: 282580 loss: 0.0042 lr: 0.02\n",
            "iteration: 282590 loss: 0.0033 lr: 0.02\n",
            "iteration: 282600 loss: 0.0036 lr: 0.02\n",
            "iteration: 282610 loss: 0.0037 lr: 0.02\n",
            "iteration: 282620 loss: 0.0033 lr: 0.02\n",
            "iteration: 282630 loss: 0.0044 lr: 0.02\n",
            "iteration: 282640 loss: 0.0038 lr: 0.02\n",
            "iteration: 282650 loss: 0.0031 lr: 0.02\n",
            "iteration: 282660 loss: 0.0028 lr: 0.02\n",
            "iteration: 282670 loss: 0.0037 lr: 0.02\n",
            "iteration: 282680 loss: 0.0032 lr: 0.02\n",
            "iteration: 282690 loss: 0.0029 lr: 0.02\n",
            "iteration: 282700 loss: 0.0030 lr: 0.02\n",
            "iteration: 282710 loss: 0.0039 lr: 0.02\n",
            "iteration: 282720 loss: 0.0034 lr: 0.02\n",
            "iteration: 282730 loss: 0.0036 lr: 0.02\n",
            "iteration: 282740 loss: 0.0039 lr: 0.02\n",
            "iteration: 282750 loss: 0.0035 lr: 0.02\n",
            "iteration: 282760 loss: 0.0026 lr: 0.02\n",
            "iteration: 282770 loss: 0.0040 lr: 0.02\n",
            "iteration: 282780 loss: 0.0039 lr: 0.02\n",
            "iteration: 282790 loss: 0.0038 lr: 0.02\n",
            "iteration: 282800 loss: 0.0028 lr: 0.02\n",
            "iteration: 282810 loss: 0.0038 lr: 0.02\n",
            "iteration: 282820 loss: 0.0032 lr: 0.02\n",
            "iteration: 282830 loss: 0.0034 lr: 0.02\n",
            "iteration: 282840 loss: 0.0025 lr: 0.02\n",
            "iteration: 282850 loss: 0.0024 lr: 0.02\n",
            "iteration: 282860 loss: 0.0033 lr: 0.02\n",
            "iteration: 282870 loss: 0.0041 lr: 0.02\n",
            "iteration: 282880 loss: 0.0045 lr: 0.02\n",
            "iteration: 282890 loss: 0.0031 lr: 0.02\n",
            "iteration: 282900 loss: 0.0034 lr: 0.02\n",
            "iteration: 282910 loss: 0.0027 lr: 0.02\n",
            "iteration: 282920 loss: 0.0029 lr: 0.02\n",
            "iteration: 282930 loss: 0.0034 lr: 0.02\n",
            "iteration: 282940 loss: 0.0043 lr: 0.02\n",
            "iteration: 282950 loss: 0.0040 lr: 0.02\n",
            "iteration: 282960 loss: 0.0034 lr: 0.02\n",
            "iteration: 282970 loss: 0.0044 lr: 0.02\n",
            "iteration: 282980 loss: 0.0027 lr: 0.02\n",
            "iteration: 282990 loss: 0.0034 lr: 0.02\n",
            "iteration: 283000 loss: 0.0036 lr: 0.02\n",
            "iteration: 283010 loss: 0.0033 lr: 0.02\n",
            "iteration: 283020 loss: 0.0042 lr: 0.02\n",
            "iteration: 283030 loss: 0.0031 lr: 0.02\n",
            "iteration: 283040 loss: 0.0036 lr: 0.02\n",
            "iteration: 283050 loss: 0.0049 lr: 0.02\n",
            "iteration: 283060 loss: 0.0050 lr: 0.02\n",
            "iteration: 283070 loss: 0.0046 lr: 0.02\n",
            "iteration: 283080 loss: 0.0029 lr: 0.02\n",
            "iteration: 283090 loss: 0.0043 lr: 0.02\n",
            "iteration: 283100 loss: 0.0041 lr: 0.02\n",
            "iteration: 283110 loss: 0.0030 lr: 0.02\n",
            "iteration: 283120 loss: 0.0031 lr: 0.02\n",
            "iteration: 283130 loss: 0.0023 lr: 0.02\n",
            "iteration: 283140 loss: 0.0030 lr: 0.02\n",
            "iteration: 283150 loss: 0.0035 lr: 0.02\n",
            "iteration: 283160 loss: 0.0029 lr: 0.02\n",
            "iteration: 283170 loss: 0.0030 lr: 0.02\n",
            "iteration: 283180 loss: 0.0030 lr: 0.02\n",
            "iteration: 283190 loss: 0.0036 lr: 0.02\n",
            "iteration: 283200 loss: 0.0041 lr: 0.02\n",
            "iteration: 283210 loss: 0.0039 lr: 0.02\n",
            "iteration: 283220 loss: 0.0030 lr: 0.02\n",
            "iteration: 283230 loss: 0.0038 lr: 0.02\n",
            "iteration: 283240 loss: 0.0027 lr: 0.02\n",
            "iteration: 283250 loss: 0.0028 lr: 0.02\n",
            "iteration: 283260 loss: 0.0026 lr: 0.02\n",
            "iteration: 283270 loss: 0.0046 lr: 0.02\n",
            "iteration: 283280 loss: 0.0038 lr: 0.02\n",
            "iteration: 283290 loss: 0.0035 lr: 0.02\n",
            "iteration: 283300 loss: 0.0039 lr: 0.02\n",
            "iteration: 283310 loss: 0.0030 lr: 0.02\n",
            "iteration: 283320 loss: 0.0043 lr: 0.02\n",
            "iteration: 283330 loss: 0.0031 lr: 0.02\n",
            "iteration: 283340 loss: 0.0033 lr: 0.02\n",
            "iteration: 283350 loss: 0.0042 lr: 0.02\n",
            "iteration: 283360 loss: 0.0040 lr: 0.02\n",
            "iteration: 283370 loss: 0.0042 lr: 0.02\n",
            "iteration: 283380 loss: 0.0040 lr: 0.02\n",
            "iteration: 283390 loss: 0.0040 lr: 0.02\n",
            "iteration: 283400 loss: 0.0030 lr: 0.02\n",
            "iteration: 283410 loss: 0.0032 lr: 0.02\n",
            "iteration: 283420 loss: 0.0028 lr: 0.02\n",
            "iteration: 283430 loss: 0.0030 lr: 0.02\n",
            "iteration: 283440 loss: 0.0054 lr: 0.02\n",
            "iteration: 283450 loss: 0.0036 lr: 0.02\n",
            "iteration: 283460 loss: 0.0036 lr: 0.02\n",
            "iteration: 283470 loss: 0.0028 lr: 0.02\n",
            "iteration: 283480 loss: 0.0031 lr: 0.02\n",
            "iteration: 283490 loss: 0.0038 lr: 0.02\n",
            "iteration: 283500 loss: 0.0025 lr: 0.02\n",
            "iteration: 283510 loss: 0.0042 lr: 0.02\n",
            "iteration: 283520 loss: 0.0033 lr: 0.02\n",
            "iteration: 283530 loss: 0.0038 lr: 0.02\n",
            "iteration: 283540 loss: 0.0026 lr: 0.02\n",
            "iteration: 283550 loss: 0.0033 lr: 0.02\n",
            "iteration: 283560 loss: 0.0040 lr: 0.02\n",
            "iteration: 283570 loss: 0.0036 lr: 0.02\n",
            "iteration: 283580 loss: 0.0042 lr: 0.02\n",
            "iteration: 283590 loss: 0.0036 lr: 0.02\n",
            "iteration: 283600 loss: 0.0027 lr: 0.02\n",
            "iteration: 283610 loss: 0.0029 lr: 0.02\n",
            "iteration: 283620 loss: 0.0034 lr: 0.02\n",
            "iteration: 283630 loss: 0.0040 lr: 0.02\n",
            "iteration: 283640 loss: 0.0045 lr: 0.02\n",
            "iteration: 283650 loss: 0.0045 lr: 0.02\n",
            "iteration: 283660 loss: 0.0030 lr: 0.02\n",
            "iteration: 283670 loss: 0.0042 lr: 0.02\n",
            "iteration: 283680 loss: 0.0033 lr: 0.02\n",
            "iteration: 283690 loss: 0.0024 lr: 0.02\n",
            "iteration: 283700 loss: 0.0041 lr: 0.02\n",
            "iteration: 283710 loss: 0.0037 lr: 0.02\n",
            "iteration: 283720 loss: 0.0029 lr: 0.02\n",
            "iteration: 283730 loss: 0.0040 lr: 0.02\n",
            "iteration: 283740 loss: 0.0029 lr: 0.02\n",
            "iteration: 283750 loss: 0.0025 lr: 0.02\n",
            "iteration: 283760 loss: 0.0041 lr: 0.02\n",
            "iteration: 283770 loss: 0.0042 lr: 0.02\n",
            "iteration: 283780 loss: 0.0035 lr: 0.02\n",
            "iteration: 283790 loss: 0.0036 lr: 0.02\n",
            "iteration: 283800 loss: 0.0035 lr: 0.02\n",
            "iteration: 283810 loss: 0.0033 lr: 0.02\n",
            "iteration: 283820 loss: 0.0040 lr: 0.02\n",
            "iteration: 283830 loss: 0.0030 lr: 0.02\n",
            "iteration: 283840 loss: 0.0036 lr: 0.02\n",
            "iteration: 283850 loss: 0.0040 lr: 0.02\n",
            "iteration: 283860 loss: 0.0036 lr: 0.02\n",
            "iteration: 283870 loss: 0.0030 lr: 0.02\n",
            "iteration: 283880 loss: 0.0048 lr: 0.02\n",
            "iteration: 283890 loss: 0.0041 lr: 0.02\n",
            "iteration: 283900 loss: 0.0035 lr: 0.02\n",
            "iteration: 283910 loss: 0.0039 lr: 0.02\n",
            "iteration: 283920 loss: 0.0028 lr: 0.02\n",
            "iteration: 283930 loss: 0.0033 lr: 0.02\n",
            "iteration: 283940 loss: 0.0053 lr: 0.02\n",
            "iteration: 283950 loss: 0.0029 lr: 0.02\n",
            "iteration: 283960 loss: 0.0030 lr: 0.02\n",
            "iteration: 283970 loss: 0.0033 lr: 0.02\n",
            "iteration: 283980 loss: 0.0041 lr: 0.02\n",
            "iteration: 283990 loss: 0.0033 lr: 0.02\n",
            "iteration: 284000 loss: 0.0039 lr: 0.02\n",
            "iteration: 284010 loss: 0.0040 lr: 0.02\n",
            "iteration: 284020 loss: 0.0025 lr: 0.02\n",
            "iteration: 284030 loss: 0.0026 lr: 0.02\n",
            "iteration: 284040 loss: 0.0034 lr: 0.02\n",
            "iteration: 284050 loss: 0.0030 lr: 0.02\n",
            "iteration: 284060 loss: 0.0026 lr: 0.02\n",
            "iteration: 284070 loss: 0.0030 lr: 0.02\n",
            "iteration: 284080 loss: 0.0037 lr: 0.02\n",
            "iteration: 284090 loss: 0.0039 lr: 0.02\n",
            "iteration: 284100 loss: 0.0061 lr: 0.02\n",
            "iteration: 284110 loss: 0.0033 lr: 0.02\n",
            "iteration: 284120 loss: 0.0033 lr: 0.02\n",
            "iteration: 284130 loss: 0.0032 lr: 0.02\n",
            "iteration: 284140 loss: 0.0038 lr: 0.02\n",
            "iteration: 284150 loss: 0.0035 lr: 0.02\n",
            "iteration: 284160 loss: 0.0039 lr: 0.02\n",
            "iteration: 284170 loss: 0.0030 lr: 0.02\n",
            "iteration: 284180 loss: 0.0028 lr: 0.02\n",
            "iteration: 284190 loss: 0.0035 lr: 0.02\n",
            "iteration: 284200 loss: 0.0037 lr: 0.02\n",
            "iteration: 284210 loss: 0.0038 lr: 0.02\n",
            "iteration: 284220 loss: 0.0039 lr: 0.02\n",
            "iteration: 284230 loss: 0.0042 lr: 0.02\n",
            "iteration: 284240 loss: 0.0048 lr: 0.02\n",
            "iteration: 284250 loss: 0.0045 lr: 0.02\n",
            "iteration: 284260 loss: 0.0033 lr: 0.02\n",
            "iteration: 284270 loss: 0.0031 lr: 0.02\n",
            "iteration: 284280 loss: 0.0033 lr: 0.02\n",
            "iteration: 284290 loss: 0.0037 lr: 0.02\n",
            "iteration: 284300 loss: 0.0035 lr: 0.02\n",
            "iteration: 284310 loss: 0.0035 lr: 0.02\n",
            "iteration: 284320 loss: 0.0032 lr: 0.02\n",
            "iteration: 284330 loss: 0.0027 lr: 0.02\n",
            "iteration: 284340 loss: 0.0032 lr: 0.02\n",
            "iteration: 284350 loss: 0.0041 lr: 0.02\n",
            "iteration: 284360 loss: 0.0034 lr: 0.02\n",
            "iteration: 284370 loss: 0.0043 lr: 0.02\n",
            "iteration: 284380 loss: 0.0038 lr: 0.02\n",
            "iteration: 284390 loss: 0.0039 lr: 0.02\n",
            "iteration: 284400 loss: 0.0036 lr: 0.02\n",
            "iteration: 284410 loss: 0.0038 lr: 0.02\n",
            "iteration: 284420 loss: 0.0038 lr: 0.02\n",
            "iteration: 284430 loss: 0.0036 lr: 0.02\n",
            "iteration: 284440 loss: 0.0027 lr: 0.02\n",
            "iteration: 284450 loss: 0.0037 lr: 0.02\n",
            "iteration: 284460 loss: 0.0032 lr: 0.02\n",
            "iteration: 284470 loss: 0.0038 lr: 0.02\n",
            "iteration: 284480 loss: 0.0035 lr: 0.02\n",
            "iteration: 284490 loss: 0.0043 lr: 0.02\n",
            "iteration: 284500 loss: 0.0041 lr: 0.02\n",
            "iteration: 284510 loss: 0.0039 lr: 0.02\n",
            "iteration: 284520 loss: 0.0040 lr: 0.02\n",
            "iteration: 284530 loss: 0.0038 lr: 0.02\n",
            "iteration: 284540 loss: 0.0041 lr: 0.02\n",
            "iteration: 284550 loss: 0.0030 lr: 0.02\n",
            "iteration: 284560 loss: 0.0031 lr: 0.02\n",
            "iteration: 284570 loss: 0.0030 lr: 0.02\n",
            "iteration: 284580 loss: 0.0035 lr: 0.02\n",
            "iteration: 284590 loss: 0.0031 lr: 0.02\n",
            "iteration: 284600 loss: 0.0029 lr: 0.02\n",
            "iteration: 284610 loss: 0.0033 lr: 0.02\n",
            "iteration: 284620 loss: 0.0029 lr: 0.02\n",
            "iteration: 284630 loss: 0.0043 lr: 0.02\n",
            "iteration: 284640 loss: 0.0042 lr: 0.02\n",
            "iteration: 284650 loss: 0.0048 lr: 0.02\n",
            "iteration: 284660 loss: 0.0040 lr: 0.02\n",
            "iteration: 284670 loss: 0.0029 lr: 0.02\n",
            "iteration: 284680 loss: 0.0045 lr: 0.02\n",
            "iteration: 284690 loss: 0.0040 lr: 0.02\n",
            "iteration: 284700 loss: 0.0035 lr: 0.02\n",
            "iteration: 284710 loss: 0.0036 lr: 0.02\n",
            "iteration: 284720 loss: 0.0046 lr: 0.02\n",
            "iteration: 284730 loss: 0.0041 lr: 0.02\n",
            "iteration: 284740 loss: 0.0032 lr: 0.02\n",
            "iteration: 284750 loss: 0.0040 lr: 0.02\n",
            "iteration: 284760 loss: 0.0026 lr: 0.02\n",
            "iteration: 284770 loss: 0.0042 lr: 0.02\n",
            "iteration: 284780 loss: 0.0053 lr: 0.02\n",
            "iteration: 284790 loss: 0.0034 lr: 0.02\n",
            "iteration: 284800 loss: 0.0038 lr: 0.02\n",
            "iteration: 284810 loss: 0.0029 lr: 0.02\n",
            "iteration: 284820 loss: 0.0035 lr: 0.02\n",
            "iteration: 284830 loss: 0.0033 lr: 0.02\n",
            "iteration: 284840 loss: 0.0039 lr: 0.02\n",
            "iteration: 284850 loss: 0.0029 lr: 0.02\n",
            "iteration: 284860 loss: 0.0031 lr: 0.02\n",
            "iteration: 284870 loss: 0.0034 lr: 0.02\n",
            "iteration: 284880 loss: 0.0031 lr: 0.02\n",
            "iteration: 284890 loss: 0.0037 lr: 0.02\n",
            "iteration: 284900 loss: 0.0029 lr: 0.02\n",
            "iteration: 284910 loss: 0.0031 lr: 0.02\n",
            "iteration: 284920 loss: 0.0029 lr: 0.02\n",
            "iteration: 284930 loss: 0.0044 lr: 0.02\n",
            "iteration: 284940 loss: 0.0043 lr: 0.02\n",
            "iteration: 284950 loss: 0.0049 lr: 0.02\n",
            "iteration: 284960 loss: 0.0035 lr: 0.02\n",
            "iteration: 284970 loss: 0.0044 lr: 0.02\n",
            "iteration: 284980 loss: 0.0030 lr: 0.02\n",
            "iteration: 284990 loss: 0.0046 lr: 0.02\n",
            "iteration: 285000 loss: 0.0031 lr: 0.02\n",
            "iteration: 285010 loss: 0.0030 lr: 0.02\n",
            "iteration: 285020 loss: 0.0030 lr: 0.02\n",
            "iteration: 285030 loss: 0.0040 lr: 0.02\n",
            "iteration: 285040 loss: 0.0031 lr: 0.02\n",
            "iteration: 285050 loss: 0.0040 lr: 0.02\n",
            "iteration: 285060 loss: 0.0037 lr: 0.02\n",
            "iteration: 285070 loss: 0.0028 lr: 0.02\n",
            "iteration: 285080 loss: 0.0037 lr: 0.02\n",
            "iteration: 285090 loss: 0.0044 lr: 0.02\n",
            "iteration: 285100 loss: 0.0028 lr: 0.02\n",
            "iteration: 285110 loss: 0.0032 lr: 0.02\n",
            "iteration: 285120 loss: 0.0030 lr: 0.02\n",
            "iteration: 285130 loss: 0.0032 lr: 0.02\n",
            "iteration: 285140 loss: 0.0038 lr: 0.02\n",
            "iteration: 285150 loss: 0.0040 lr: 0.02\n",
            "iteration: 285160 loss: 0.0035 lr: 0.02\n",
            "iteration: 285170 loss: 0.0036 lr: 0.02\n",
            "iteration: 285180 loss: 0.0030 lr: 0.02\n",
            "iteration: 285190 loss: 0.0036 lr: 0.02\n",
            "iteration: 285200 loss: 0.0039 lr: 0.02\n",
            "iteration: 285210 loss: 0.0035 lr: 0.02\n",
            "iteration: 285220 loss: 0.0039 lr: 0.02\n",
            "iteration: 285230 loss: 0.0037 lr: 0.02\n",
            "iteration: 285240 loss: 0.0032 lr: 0.02\n",
            "iteration: 285250 loss: 0.0037 lr: 0.02\n",
            "iteration: 285260 loss: 0.0032 lr: 0.02\n",
            "iteration: 285270 loss: 0.0035 lr: 0.02\n",
            "iteration: 285280 loss: 0.0029 lr: 0.02\n",
            "iteration: 285290 loss: 0.0046 lr: 0.02\n",
            "iteration: 285300 loss: 0.0037 lr: 0.02\n",
            "iteration: 285310 loss: 0.0057 lr: 0.02\n",
            "iteration: 285320 loss: 0.0036 lr: 0.02\n",
            "iteration: 285330 loss: 0.0038 lr: 0.02\n",
            "iteration: 285340 loss: 0.0027 lr: 0.02\n",
            "iteration: 285350 loss: 0.0034 lr: 0.02\n",
            "iteration: 285360 loss: 0.0048 lr: 0.02\n",
            "iteration: 285370 loss: 0.0035 lr: 0.02\n",
            "iteration: 285380 loss: 0.0022 lr: 0.02\n",
            "iteration: 285390 loss: 0.0031 lr: 0.02\n",
            "iteration: 285400 loss: 0.0036 lr: 0.02\n",
            "iteration: 285410 loss: 0.0030 lr: 0.02\n",
            "iteration: 285420 loss: 0.0033 lr: 0.02\n",
            "iteration: 285430 loss: 0.0028 lr: 0.02\n",
            "iteration: 285440 loss: 0.0042 lr: 0.02\n",
            "iteration: 285450 loss: 0.0034 lr: 0.02\n",
            "iteration: 285460 loss: 0.0034 lr: 0.02\n",
            "iteration: 285470 loss: 0.0038 lr: 0.02\n",
            "iteration: 285480 loss: 0.0047 lr: 0.02\n",
            "iteration: 285490 loss: 0.0045 lr: 0.02\n",
            "iteration: 285500 loss: 0.0034 lr: 0.02\n",
            "iteration: 285510 loss: 0.0030 lr: 0.02\n",
            "iteration: 285520 loss: 0.0036 lr: 0.02\n",
            "iteration: 285530 loss: 0.0047 lr: 0.02\n",
            "iteration: 285540 loss: 0.0036 lr: 0.02\n",
            "iteration: 285550 loss: 0.0032 lr: 0.02\n",
            "iteration: 285560 loss: 0.0036 lr: 0.02\n",
            "iteration: 285570 loss: 0.0035 lr: 0.02\n",
            "iteration: 285580 loss: 0.0029 lr: 0.02\n",
            "iteration: 285590 loss: 0.0044 lr: 0.02\n",
            "iteration: 285600 loss: 0.0033 lr: 0.02\n",
            "iteration: 285610 loss: 0.0036 lr: 0.02\n",
            "iteration: 285620 loss: 0.0026 lr: 0.02\n",
            "iteration: 285630 loss: 0.0032 lr: 0.02\n",
            "iteration: 285640 loss: 0.0034 lr: 0.02\n",
            "iteration: 285650 loss: 0.0043 lr: 0.02\n",
            "iteration: 285660 loss: 0.0031 lr: 0.02\n",
            "iteration: 285670 loss: 0.0030 lr: 0.02\n",
            "iteration: 285680 loss: 0.0033 lr: 0.02\n",
            "iteration: 285690 loss: 0.0044 lr: 0.02\n",
            "iteration: 285700 loss: 0.0032 lr: 0.02\n",
            "iteration: 285710 loss: 0.0039 lr: 0.02\n",
            "iteration: 285720 loss: 0.0038 lr: 0.02\n",
            "iteration: 285730 loss: 0.0035 lr: 0.02\n",
            "iteration: 285740 loss: 0.0030 lr: 0.02\n",
            "iteration: 285750 loss: 0.0039 lr: 0.02\n",
            "iteration: 285760 loss: 0.0041 lr: 0.02\n",
            "iteration: 285770 loss: 0.0039 lr: 0.02\n",
            "iteration: 285780 loss: 0.0032 lr: 0.02\n",
            "iteration: 285790 loss: 0.0036 lr: 0.02\n",
            "iteration: 285800 loss: 0.0041 lr: 0.02\n",
            "iteration: 285810 loss: 0.0032 lr: 0.02\n",
            "iteration: 285820 loss: 0.0032 lr: 0.02\n",
            "iteration: 285830 loss: 0.0031 lr: 0.02\n",
            "iteration: 285840 loss: 0.0030 lr: 0.02\n",
            "iteration: 285850 loss: 0.0026 lr: 0.02\n",
            "iteration: 285860 loss: 0.0028 lr: 0.02\n",
            "iteration: 285870 loss: 0.0033 lr: 0.02\n",
            "iteration: 285880 loss: 0.0035 lr: 0.02\n",
            "iteration: 285890 loss: 0.0042 lr: 0.02\n",
            "iteration: 285900 loss: 0.0060 lr: 0.02\n",
            "iteration: 285910 loss: 0.0031 lr: 0.02\n",
            "iteration: 285920 loss: 0.0039 lr: 0.02\n",
            "iteration: 285930 loss: 0.0047 lr: 0.02\n",
            "iteration: 285940 loss: 0.0042 lr: 0.02\n",
            "iteration: 285950 loss: 0.0034 lr: 0.02\n",
            "iteration: 285960 loss: 0.0043 lr: 0.02\n",
            "iteration: 285970 loss: 0.0040 lr: 0.02\n",
            "iteration: 285980 loss: 0.0035 lr: 0.02\n",
            "iteration: 285990 loss: 0.0035 lr: 0.02\n",
            "iteration: 286000 loss: 0.0036 lr: 0.02\n",
            "iteration: 286010 loss: 0.0045 lr: 0.02\n",
            "iteration: 286020 loss: 0.0032 lr: 0.02\n",
            "iteration: 286030 loss: 0.0036 lr: 0.02\n",
            "iteration: 286040 loss: 0.0031 lr: 0.02\n",
            "iteration: 286050 loss: 0.0047 lr: 0.02\n",
            "iteration: 286060 loss: 0.0047 lr: 0.02\n",
            "iteration: 286070 loss: 0.0030 lr: 0.02\n",
            "iteration: 286080 loss: 0.0037 lr: 0.02\n",
            "iteration: 286090 loss: 0.0034 lr: 0.02\n",
            "iteration: 286100 loss: 0.0026 lr: 0.02\n",
            "iteration: 286110 loss: 0.0027 lr: 0.02\n",
            "iteration: 286120 loss: 0.0035 lr: 0.02\n",
            "iteration: 286130 loss: 0.0041 lr: 0.02\n",
            "iteration: 286140 loss: 0.0030 lr: 0.02\n",
            "iteration: 286150 loss: 0.0032 lr: 0.02\n",
            "iteration: 286160 loss: 0.0033 lr: 0.02\n",
            "iteration: 286170 loss: 0.0033 lr: 0.02\n",
            "iteration: 286180 loss: 0.0042 lr: 0.02\n",
            "iteration: 286190 loss: 0.0036 lr: 0.02\n",
            "iteration: 286200 loss: 0.0035 lr: 0.02\n",
            "iteration: 286210 loss: 0.0041 lr: 0.02\n",
            "iteration: 286220 loss: 0.0033 lr: 0.02\n",
            "iteration: 286230 loss: 0.0029 lr: 0.02\n",
            "iteration: 286240 loss: 0.0036 lr: 0.02\n",
            "iteration: 286250 loss: 0.0043 lr: 0.02\n",
            "iteration: 286260 loss: 0.0043 lr: 0.02\n",
            "iteration: 286270 loss: 0.0029 lr: 0.02\n",
            "iteration: 286280 loss: 0.0025 lr: 0.02\n",
            "iteration: 286290 loss: 0.0036 lr: 0.02\n",
            "iteration: 286300 loss: 0.0034 lr: 0.02\n",
            "iteration: 286310 loss: 0.0027 lr: 0.02\n",
            "iteration: 286320 loss: 0.0039 lr: 0.02\n",
            "iteration: 286330 loss: 0.0035 lr: 0.02\n",
            "iteration: 286340 loss: 0.0041 lr: 0.02\n",
            "iteration: 286350 loss: 0.0035 lr: 0.02\n",
            "iteration: 286360 loss: 0.0035 lr: 0.02\n",
            "iteration: 286370 loss: 0.0029 lr: 0.02\n",
            "iteration: 286380 loss: 0.0037 lr: 0.02\n",
            "iteration: 286390 loss: 0.0038 lr: 0.02\n",
            "iteration: 286400 loss: 0.0038 lr: 0.02\n",
            "iteration: 286410 loss: 0.0030 lr: 0.02\n",
            "iteration: 286420 loss: 0.0037 lr: 0.02\n",
            "iteration: 286430 loss: 0.0044 lr: 0.02\n",
            "iteration: 286440 loss: 0.0035 lr: 0.02\n",
            "iteration: 286450 loss: 0.0039 lr: 0.02\n",
            "iteration: 286460 loss: 0.0033 lr: 0.02\n",
            "iteration: 286470 loss: 0.0038 lr: 0.02\n",
            "iteration: 286480 loss: 0.0029 lr: 0.02\n",
            "iteration: 286490 loss: 0.0044 lr: 0.02\n",
            "iteration: 286500 loss: 0.0037 lr: 0.02\n",
            "iteration: 286510 loss: 0.0037 lr: 0.02\n",
            "iteration: 286520 loss: 0.0036 lr: 0.02\n",
            "iteration: 286530 loss: 0.0038 lr: 0.02\n",
            "iteration: 286540 loss: 0.0041 lr: 0.02\n",
            "iteration: 286550 loss: 0.0032 lr: 0.02\n",
            "iteration: 286560 loss: 0.0037 lr: 0.02\n",
            "iteration: 286570 loss: 0.0037 lr: 0.02\n",
            "iteration: 286580 loss: 0.0037 lr: 0.02\n",
            "iteration: 286590 loss: 0.0030 lr: 0.02\n",
            "iteration: 286600 loss: 0.0032 lr: 0.02\n",
            "iteration: 286610 loss: 0.0038 lr: 0.02\n",
            "iteration: 286620 loss: 0.0036 lr: 0.02\n",
            "iteration: 286630 loss: 0.0025 lr: 0.02\n",
            "iteration: 286640 loss: 0.0029 lr: 0.02\n",
            "iteration: 286650 loss: 0.0033 lr: 0.02\n",
            "iteration: 286660 loss: 0.0040 lr: 0.02\n",
            "iteration: 286670 loss: 0.0038 lr: 0.02\n",
            "iteration: 286680 loss: 0.0029 lr: 0.02\n",
            "iteration: 286690 loss: 0.0034 lr: 0.02\n",
            "iteration: 286700 loss: 0.0031 lr: 0.02\n",
            "iteration: 286710 loss: 0.0037 lr: 0.02\n",
            "iteration: 286720 loss: 0.0035 lr: 0.02\n",
            "iteration: 286730 loss: 0.0041 lr: 0.02\n",
            "iteration: 286740 loss: 0.0033 lr: 0.02\n",
            "iteration: 286750 loss: 0.0052 lr: 0.02\n",
            "iteration: 286760 loss: 0.0046 lr: 0.02\n",
            "iteration: 286770 loss: 0.0033 lr: 0.02\n",
            "iteration: 286780 loss: 0.0041 lr: 0.02\n",
            "iteration: 286790 loss: 0.0038 lr: 0.02\n",
            "iteration: 286800 loss: 0.0038 lr: 0.02\n",
            "iteration: 286810 loss: 0.0045 lr: 0.02\n",
            "iteration: 286820 loss: 0.0040 lr: 0.02\n",
            "iteration: 286830 loss: 0.0029 lr: 0.02\n",
            "iteration: 286840 loss: 0.0034 lr: 0.02\n",
            "iteration: 286850 loss: 0.0036 lr: 0.02\n",
            "iteration: 286860 loss: 0.0034 lr: 0.02\n",
            "iteration: 286870 loss: 0.0029 lr: 0.02\n",
            "iteration: 286880 loss: 0.0044 lr: 0.02\n",
            "iteration: 286890 loss: 0.0036 lr: 0.02\n",
            "iteration: 286900 loss: 0.0032 lr: 0.02\n",
            "iteration: 286910 loss: 0.0030 lr: 0.02\n",
            "iteration: 286920 loss: 0.0038 lr: 0.02\n",
            "iteration: 286930 loss: 0.0037 lr: 0.02\n",
            "iteration: 286940 loss: 0.0040 lr: 0.02\n",
            "iteration: 286950 loss: 0.0031 lr: 0.02\n",
            "iteration: 286960 loss: 0.0041 lr: 0.02\n",
            "iteration: 286970 loss: 0.0034 lr: 0.02\n",
            "iteration: 286980 loss: 0.0035 lr: 0.02\n",
            "iteration: 286990 loss: 0.0024 lr: 0.02\n",
            "iteration: 287000 loss: 0.0033 lr: 0.02\n",
            "iteration: 287010 loss: 0.0040 lr: 0.02\n",
            "iteration: 287020 loss: 0.0030 lr: 0.02\n",
            "iteration: 287030 loss: 0.0035 lr: 0.02\n",
            "iteration: 287040 loss: 0.0034 lr: 0.02\n",
            "iteration: 287050 loss: 0.0053 lr: 0.02\n",
            "iteration: 287060 loss: 0.0037 lr: 0.02\n",
            "iteration: 287070 loss: 0.0024 lr: 0.02\n",
            "iteration: 287080 loss: 0.0030 lr: 0.02\n",
            "iteration: 287090 loss: 0.0041 lr: 0.02\n",
            "iteration: 287100 loss: 0.0027 lr: 0.02\n",
            "iteration: 287110 loss: 0.0036 lr: 0.02\n",
            "iteration: 287120 loss: 0.0037 lr: 0.02\n",
            "iteration: 287130 loss: 0.0035 lr: 0.02\n",
            "iteration: 287140 loss: 0.0032 lr: 0.02\n",
            "iteration: 287150 loss: 0.0028 lr: 0.02\n",
            "iteration: 287160 loss: 0.0030 lr: 0.02\n",
            "iteration: 287170 loss: 0.0028 lr: 0.02\n",
            "iteration: 287180 loss: 0.0041 lr: 0.02\n",
            "iteration: 287190 loss: 0.0037 lr: 0.02\n",
            "iteration: 287200 loss: 0.0054 lr: 0.02\n",
            "iteration: 287210 loss: 0.0043 lr: 0.02\n",
            "iteration: 287220 loss: 0.0036 lr: 0.02\n",
            "iteration: 287230 loss: 0.0030 lr: 0.02\n",
            "iteration: 287240 loss: 0.0034 lr: 0.02\n",
            "iteration: 287250 loss: 0.0028 lr: 0.02\n",
            "iteration: 287260 loss: 0.0033 lr: 0.02\n",
            "iteration: 287270 loss: 0.0030 lr: 0.02\n",
            "iteration: 287280 loss: 0.0038 lr: 0.02\n",
            "iteration: 287290 loss: 0.0036 lr: 0.02\n",
            "iteration: 287300 loss: 0.0036 lr: 0.02\n",
            "iteration: 287310 loss: 0.0039 lr: 0.02\n",
            "iteration: 287320 loss: 0.0034 lr: 0.02\n",
            "iteration: 287330 loss: 0.0038 lr: 0.02\n",
            "iteration: 287340 loss: 0.0039 lr: 0.02\n",
            "iteration: 287350 loss: 0.0040 lr: 0.02\n",
            "iteration: 287360 loss: 0.0028 lr: 0.02\n",
            "iteration: 287370 loss: 0.0038 lr: 0.02\n",
            "iteration: 287380 loss: 0.0032 lr: 0.02\n",
            "iteration: 287390 loss: 0.0030 lr: 0.02\n",
            "iteration: 287400 loss: 0.0037 lr: 0.02\n",
            "iteration: 287410 loss: 0.0068 lr: 0.02\n",
            "iteration: 287420 loss: 0.0044 lr: 0.02\n",
            "iteration: 287430 loss: 0.0038 lr: 0.02\n",
            "iteration: 287440 loss: 0.0043 lr: 0.02\n",
            "iteration: 287450 loss: 0.0032 lr: 0.02\n",
            "iteration: 287460 loss: 0.0032 lr: 0.02\n",
            "iteration: 287470 loss: 0.0038 lr: 0.02\n",
            "iteration: 287480 loss: 0.0031 lr: 0.02\n",
            "iteration: 287490 loss: 0.0048 lr: 0.02\n",
            "iteration: 287500 loss: 0.0026 lr: 0.02\n",
            "iteration: 287510 loss: 0.0037 lr: 0.02\n",
            "iteration: 287520 loss: 0.0033 lr: 0.02\n",
            "iteration: 287530 loss: 0.0032 lr: 0.02\n",
            "iteration: 287540 loss: 0.0042 lr: 0.02\n",
            "iteration: 287550 loss: 0.0037 lr: 0.02\n",
            "iteration: 287560 loss: 0.0033 lr: 0.02\n",
            "iteration: 287570 loss: 0.0043 lr: 0.02\n",
            "iteration: 287580 loss: 0.0038 lr: 0.02\n",
            "iteration: 287590 loss: 0.0049 lr: 0.02\n",
            "iteration: 287600 loss: 0.0040 lr: 0.02\n",
            "iteration: 287610 loss: 0.0042 lr: 0.02\n",
            "iteration: 287620 loss: 0.0034 lr: 0.02\n",
            "iteration: 287630 loss: 0.0035 lr: 0.02\n",
            "iteration: 287640 loss: 0.0038 lr: 0.02\n",
            "iteration: 287650 loss: 0.0035 lr: 0.02\n",
            "iteration: 287660 loss: 0.0032 lr: 0.02\n",
            "iteration: 287670 loss: 0.0036 lr: 0.02\n",
            "iteration: 287680 loss: 0.0032 lr: 0.02\n",
            "iteration: 287690 loss: 0.0032 lr: 0.02\n",
            "iteration: 287700 loss: 0.0039 lr: 0.02\n",
            "iteration: 287710 loss: 0.0032 lr: 0.02\n",
            "iteration: 287720 loss: 0.0029 lr: 0.02\n",
            "iteration: 287730 loss: 0.0037 lr: 0.02\n",
            "iteration: 287740 loss: 0.0035 lr: 0.02\n",
            "iteration: 287750 loss: 0.0036 lr: 0.02\n",
            "iteration: 287760 loss: 0.0034 lr: 0.02\n",
            "iteration: 287770 loss: 0.0038 lr: 0.02\n",
            "iteration: 287780 loss: 0.0040 lr: 0.02\n",
            "iteration: 287790 loss: 0.0036 lr: 0.02\n",
            "iteration: 287800 loss: 0.0037 lr: 0.02\n",
            "iteration: 287810 loss: 0.0026 lr: 0.02\n",
            "iteration: 287820 loss: 0.0030 lr: 0.02\n",
            "iteration: 287830 loss: 0.0036 lr: 0.02\n",
            "iteration: 287840 loss: 0.0029 lr: 0.02\n",
            "iteration: 287850 loss: 0.0042 lr: 0.02\n",
            "iteration: 287860 loss: 0.0032 lr: 0.02\n",
            "iteration: 287870 loss: 0.0025 lr: 0.02\n",
            "iteration: 287880 loss: 0.0036 lr: 0.02\n",
            "iteration: 287890 loss: 0.0036 lr: 0.02\n",
            "iteration: 287900 loss: 0.0040 lr: 0.02\n",
            "iteration: 287910 loss: 0.0028 lr: 0.02\n",
            "iteration: 287920 loss: 0.0035 lr: 0.02\n",
            "iteration: 287930 loss: 0.0035 lr: 0.02\n",
            "iteration: 287940 loss: 0.0033 lr: 0.02\n",
            "iteration: 287950 loss: 0.0036 lr: 0.02\n",
            "iteration: 287960 loss: 0.0045 lr: 0.02\n",
            "iteration: 287970 loss: 0.0025 lr: 0.02\n",
            "iteration: 287980 loss: 0.0036 lr: 0.02\n",
            "iteration: 287990 loss: 0.0045 lr: 0.02\n",
            "iteration: 288000 loss: 0.0052 lr: 0.02\n",
            "iteration: 288010 loss: 0.0027 lr: 0.02\n",
            "iteration: 288020 loss: 0.0029 lr: 0.02\n",
            "iteration: 288030 loss: 0.0039 lr: 0.02\n",
            "iteration: 288040 loss: 0.0031 lr: 0.02\n",
            "iteration: 288050 loss: 0.0032 lr: 0.02\n",
            "iteration: 288060 loss: 0.0036 lr: 0.02\n",
            "iteration: 288070 loss: 0.0038 lr: 0.02\n",
            "iteration: 288080 loss: 0.0031 lr: 0.02\n",
            "iteration: 288090 loss: 0.0040 lr: 0.02\n",
            "iteration: 288100 loss: 0.0040 lr: 0.02\n",
            "iteration: 288110 loss: 0.0040 lr: 0.02\n",
            "iteration: 288120 loss: 0.0030 lr: 0.02\n",
            "iteration: 288130 loss: 0.0052 lr: 0.02\n",
            "iteration: 288140 loss: 0.0031 lr: 0.02\n",
            "iteration: 288150 loss: 0.0027 lr: 0.02\n",
            "iteration: 288160 loss: 0.0032 lr: 0.02\n",
            "iteration: 288170 loss: 0.0036 lr: 0.02\n",
            "iteration: 288180 loss: 0.0043 lr: 0.02\n",
            "iteration: 288190 loss: 0.0039 lr: 0.02\n",
            "iteration: 288200 loss: 0.0038 lr: 0.02\n",
            "iteration: 288210 loss: 0.0035 lr: 0.02\n",
            "iteration: 288220 loss: 0.0034 lr: 0.02\n",
            "iteration: 288230 loss: 0.0034 lr: 0.02\n",
            "iteration: 288240 loss: 0.0036 lr: 0.02\n",
            "iteration: 288250 loss: 0.0024 lr: 0.02\n",
            "iteration: 288260 loss: 0.0026 lr: 0.02\n",
            "iteration: 288270 loss: 0.0033 lr: 0.02\n",
            "iteration: 288280 loss: 0.0034 lr: 0.02\n",
            "iteration: 288290 loss: 0.0040 lr: 0.02\n",
            "iteration: 288300 loss: 0.0032 lr: 0.02\n",
            "iteration: 288310 loss: 0.0043 lr: 0.02\n",
            "iteration: 288320 loss: 0.0035 lr: 0.02\n",
            "iteration: 288330 loss: 0.0037 lr: 0.02\n",
            "iteration: 288340 loss: 0.0023 lr: 0.02\n",
            "iteration: 288350 loss: 0.0032 lr: 0.02\n",
            "iteration: 288360 loss: 0.0032 lr: 0.02\n",
            "iteration: 288370 loss: 0.0029 lr: 0.02\n",
            "iteration: 288380 loss: 0.0035 lr: 0.02\n",
            "iteration: 288390 loss: 0.0041 lr: 0.02\n",
            "iteration: 288400 loss: 0.0040 lr: 0.02\n",
            "iteration: 288410 loss: 0.0035 lr: 0.02\n",
            "iteration: 288420 loss: 0.0039 lr: 0.02\n",
            "iteration: 288430 loss: 0.0037 lr: 0.02\n",
            "iteration: 288440 loss: 0.0037 lr: 0.02\n",
            "iteration: 288450 loss: 0.0027 lr: 0.02\n",
            "iteration: 288460 loss: 0.0039 lr: 0.02\n",
            "iteration: 288470 loss: 0.0044 lr: 0.02\n",
            "iteration: 288480 loss: 0.0040 lr: 0.02\n",
            "iteration: 288490 loss: 0.0029 lr: 0.02\n",
            "iteration: 288500 loss: 0.0030 lr: 0.02\n",
            "iteration: 288510 loss: 0.0043 lr: 0.02\n",
            "iteration: 288520 loss: 0.0033 lr: 0.02\n",
            "iteration: 288530 loss: 0.0031 lr: 0.02\n",
            "iteration: 288540 loss: 0.0038 lr: 0.02\n",
            "iteration: 288550 loss: 0.0041 lr: 0.02\n",
            "iteration: 288560 loss: 0.0036 lr: 0.02\n",
            "iteration: 288570 loss: 0.0039 lr: 0.02\n",
            "iteration: 288580 loss: 0.0033 lr: 0.02\n",
            "iteration: 288590 loss: 0.0042 lr: 0.02\n",
            "iteration: 288600 loss: 0.0043 lr: 0.02\n",
            "iteration: 288610 loss: 0.0040 lr: 0.02\n",
            "iteration: 288620 loss: 0.0038 lr: 0.02\n",
            "iteration: 288630 loss: 0.0036 lr: 0.02\n",
            "iteration: 288640 loss: 0.0043 lr: 0.02\n",
            "iteration: 288650 loss: 0.0033 lr: 0.02\n",
            "iteration: 288660 loss: 0.0032 lr: 0.02\n",
            "iteration: 288670 loss: 0.0034 lr: 0.02\n",
            "iteration: 288680 loss: 0.0034 lr: 0.02\n",
            "iteration: 288690 loss: 0.0036 lr: 0.02\n",
            "iteration: 288700 loss: 0.0029 lr: 0.02\n",
            "iteration: 288710 loss: 0.0039 lr: 0.02\n",
            "iteration: 288720 loss: 0.0029 lr: 0.02\n",
            "iteration: 288730 loss: 0.0039 lr: 0.02\n",
            "iteration: 288740 loss: 0.0034 lr: 0.02\n",
            "iteration: 288750 loss: 0.0042 lr: 0.02\n",
            "iteration: 288760 loss: 0.0032 lr: 0.02\n",
            "iteration: 288770 loss: 0.0039 lr: 0.02\n",
            "iteration: 288780 loss: 0.0039 lr: 0.02\n",
            "iteration: 288790 loss: 0.0046 lr: 0.02\n",
            "iteration: 288800 loss: 0.0043 lr: 0.02\n",
            "iteration: 288810 loss: 0.0029 lr: 0.02\n",
            "iteration: 288820 loss: 0.0031 lr: 0.02\n",
            "iteration: 288830 loss: 0.0026 lr: 0.02\n",
            "iteration: 288840 loss: 0.0035 lr: 0.02\n",
            "iteration: 288850 loss: 0.0040 lr: 0.02\n",
            "iteration: 288860 loss: 0.0033 lr: 0.02\n",
            "iteration: 288870 loss: 0.0026 lr: 0.02\n",
            "iteration: 288880 loss: 0.0040 lr: 0.02\n",
            "iteration: 288890 loss: 0.0029 lr: 0.02\n",
            "iteration: 288900 loss: 0.0027 lr: 0.02\n",
            "iteration: 288910 loss: 0.0035 lr: 0.02\n",
            "iteration: 288920 loss: 0.0024 lr: 0.02\n",
            "iteration: 288930 loss: 0.0043 lr: 0.02\n",
            "iteration: 288940 loss: 0.0030 lr: 0.02\n",
            "iteration: 288950 loss: 0.0039 lr: 0.02\n",
            "iteration: 288960 loss: 0.0032 lr: 0.02\n",
            "iteration: 288970 loss: 0.0030 lr: 0.02\n",
            "iteration: 288980 loss: 0.0033 lr: 0.02\n",
            "iteration: 288990 loss: 0.0033 lr: 0.02\n",
            "iteration: 289000 loss: 0.0036 lr: 0.02\n",
            "iteration: 289010 loss: 0.0030 lr: 0.02\n",
            "iteration: 289020 loss: 0.0043 lr: 0.02\n",
            "iteration: 289030 loss: 0.0043 lr: 0.02\n",
            "iteration: 289040 loss: 0.0030 lr: 0.02\n",
            "iteration: 289050 loss: 0.0036 lr: 0.02\n",
            "iteration: 289060 loss: 0.0032 lr: 0.02\n",
            "iteration: 289070 loss: 0.0027 lr: 0.02\n",
            "iteration: 289080 loss: 0.0049 lr: 0.02\n",
            "iteration: 289090 loss: 0.0028 lr: 0.02\n",
            "iteration: 289100 loss: 0.0043 lr: 0.02\n",
            "iteration: 289110 loss: 0.0038 lr: 0.02\n",
            "iteration: 289120 loss: 0.0049 lr: 0.02\n",
            "iteration: 289130 loss: 0.0032 lr: 0.02\n",
            "iteration: 289140 loss: 0.0031 lr: 0.02\n",
            "iteration: 289150 loss: 0.0035 lr: 0.02\n",
            "iteration: 289160 loss: 0.0030 lr: 0.02\n",
            "iteration: 289170 loss: 0.0026 lr: 0.02\n",
            "iteration: 289180 loss: 0.0034 lr: 0.02\n",
            "iteration: 289190 loss: 0.0031 lr: 0.02\n",
            "iteration: 289200 loss: 0.0035 lr: 0.02\n",
            "iteration: 289210 loss: 0.0039 lr: 0.02\n",
            "iteration: 289220 loss: 0.0038 lr: 0.02\n",
            "iteration: 289230 loss: 0.0038 lr: 0.02\n",
            "iteration: 289240 loss: 0.0037 lr: 0.02\n",
            "iteration: 289250 loss: 0.0033 lr: 0.02\n",
            "iteration: 289260 loss: 0.0028 lr: 0.02\n",
            "iteration: 289270 loss: 0.0050 lr: 0.02\n",
            "iteration: 289280 loss: 0.0026 lr: 0.02\n",
            "iteration: 289290 loss: 0.0034 lr: 0.02\n",
            "iteration: 289300 loss: 0.0035 lr: 0.02\n",
            "iteration: 289310 loss: 0.0033 lr: 0.02\n",
            "iteration: 289320 loss: 0.0038 lr: 0.02\n",
            "iteration: 289330 loss: 0.0031 lr: 0.02\n",
            "iteration: 289340 loss: 0.0029 lr: 0.02\n",
            "iteration: 289350 loss: 0.0037 lr: 0.02\n",
            "iteration: 289360 loss: 0.0042 lr: 0.02\n",
            "iteration: 289370 loss: 0.0033 lr: 0.02\n",
            "iteration: 289380 loss: 0.0035 lr: 0.02\n",
            "iteration: 289390 loss: 0.0044 lr: 0.02\n",
            "iteration: 289400 loss: 0.0037 lr: 0.02\n",
            "iteration: 289410 loss: 0.0045 lr: 0.02\n",
            "iteration: 289420 loss: 0.0043 lr: 0.02\n",
            "iteration: 289430 loss: 0.0025 lr: 0.02\n",
            "iteration: 289440 loss: 0.0030 lr: 0.02\n",
            "iteration: 289450 loss: 0.0045 lr: 0.02\n",
            "iteration: 289460 loss: 0.0041 lr: 0.02\n",
            "iteration: 289470 loss: 0.0037 lr: 0.02\n",
            "iteration: 289480 loss: 0.0042 lr: 0.02\n",
            "iteration: 289490 loss: 0.0042 lr: 0.02\n",
            "iteration: 289500 loss: 0.0044 lr: 0.02\n",
            "iteration: 289510 loss: 0.0036 lr: 0.02\n",
            "iteration: 289520 loss: 0.0033 lr: 0.02\n",
            "iteration: 289530 loss: 0.0041 lr: 0.02\n",
            "iteration: 289540 loss: 0.0041 lr: 0.02\n",
            "iteration: 289550 loss: 0.0034 lr: 0.02\n",
            "iteration: 289560 loss: 0.0027 lr: 0.02\n",
            "iteration: 289570 loss: 0.0023 lr: 0.02\n",
            "iteration: 289580 loss: 0.0035 lr: 0.02\n",
            "iteration: 289590 loss: 0.0034 lr: 0.02\n",
            "iteration: 289600 loss: 0.0036 lr: 0.02\n",
            "iteration: 289610 loss: 0.0027 lr: 0.02\n",
            "iteration: 289620 loss: 0.0029 lr: 0.02\n",
            "iteration: 289630 loss: 0.0044 lr: 0.02\n",
            "iteration: 289640 loss: 0.0033 lr: 0.02\n",
            "iteration: 289650 loss: 0.0036 lr: 0.02\n",
            "iteration: 289660 loss: 0.0038 lr: 0.02\n",
            "iteration: 289670 loss: 0.0030 lr: 0.02\n",
            "iteration: 289680 loss: 0.0036 lr: 0.02\n",
            "iteration: 289690 loss: 0.0029 lr: 0.02\n",
            "iteration: 289700 loss: 0.0039 lr: 0.02\n",
            "iteration: 289710 loss: 0.0043 lr: 0.02\n",
            "iteration: 289720 loss: 0.0029 lr: 0.02\n",
            "iteration: 289730 loss: 0.0034 lr: 0.02\n",
            "iteration: 289740 loss: 0.0033 lr: 0.02\n",
            "iteration: 289750 loss: 0.0028 lr: 0.02\n",
            "iteration: 289760 loss: 0.0035 lr: 0.02\n",
            "iteration: 289770 loss: 0.0033 lr: 0.02\n",
            "iteration: 289780 loss: 0.0036 lr: 0.02\n",
            "iteration: 289790 loss: 0.0033 lr: 0.02\n",
            "iteration: 289800 loss: 0.0051 lr: 0.02\n",
            "iteration: 289810 loss: 0.0034 lr: 0.02\n",
            "iteration: 289820 loss: 0.0044 lr: 0.02\n",
            "iteration: 289830 loss: 0.0050 lr: 0.02\n",
            "iteration: 289840 loss: 0.0040 lr: 0.02\n",
            "iteration: 289850 loss: 0.0044 lr: 0.02\n",
            "iteration: 289860 loss: 0.0038 lr: 0.02\n",
            "iteration: 289870 loss: 0.0039 lr: 0.02\n",
            "iteration: 289880 loss: 0.0030 lr: 0.02\n",
            "iteration: 289890 loss: 0.0024 lr: 0.02\n",
            "iteration: 289900 loss: 0.0032 lr: 0.02\n",
            "iteration: 289910 loss: 0.0032 lr: 0.02\n",
            "iteration: 289920 loss: 0.0036 lr: 0.02\n",
            "iteration: 289930 loss: 0.0031 lr: 0.02\n",
            "iteration: 289940 loss: 0.0035 lr: 0.02\n",
            "iteration: 289950 loss: 0.0034 lr: 0.02\n",
            "iteration: 289960 loss: 0.0032 lr: 0.02\n",
            "iteration: 289970 loss: 0.0037 lr: 0.02\n",
            "iteration: 289980 loss: 0.0039 lr: 0.02\n",
            "iteration: 289990 loss: 0.0037 lr: 0.02\n",
            "iteration: 290000 loss: 0.0028 lr: 0.02\n",
            "iteration: 290010 loss: 0.0029 lr: 0.02\n",
            "iteration: 290020 loss: 0.0034 lr: 0.02\n",
            "iteration: 290030 loss: 0.0035 lr: 0.02\n",
            "iteration: 290040 loss: 0.0028 lr: 0.02\n",
            "iteration: 290050 loss: 0.0048 lr: 0.02\n",
            "iteration: 290060 loss: 0.0034 lr: 0.02\n",
            "iteration: 290070 loss: 0.0032 lr: 0.02\n",
            "iteration: 290080 loss: 0.0036 lr: 0.02\n",
            "iteration: 290090 loss: 0.0035 lr: 0.02\n",
            "iteration: 290100 loss: 0.0028 lr: 0.02\n",
            "iteration: 290110 loss: 0.0032 lr: 0.02\n",
            "iteration: 290120 loss: 0.0034 lr: 0.02\n",
            "iteration: 290130 loss: 0.0035 lr: 0.02\n",
            "iteration: 290140 loss: 0.0035 lr: 0.02\n",
            "iteration: 290150 loss: 0.0034 lr: 0.02\n",
            "iteration: 290160 loss: 0.0048 lr: 0.02\n",
            "iteration: 290170 loss: 0.0022 lr: 0.02\n",
            "iteration: 290180 loss: 0.0034 lr: 0.02\n",
            "iteration: 290190 loss: 0.0039 lr: 0.02\n",
            "iteration: 290200 loss: 0.0035 lr: 0.02\n",
            "iteration: 290210 loss: 0.0052 lr: 0.02\n",
            "iteration: 290220 loss: 0.0036 lr: 0.02\n",
            "iteration: 290230 loss: 0.0034 lr: 0.02\n",
            "iteration: 290240 loss: 0.0038 lr: 0.02\n",
            "iteration: 290250 loss: 0.0037 lr: 0.02\n",
            "iteration: 290260 loss: 0.0031 lr: 0.02\n",
            "iteration: 290270 loss: 0.0039 lr: 0.02\n",
            "iteration: 290280 loss: 0.0030 lr: 0.02\n",
            "iteration: 290290 loss: 0.0042 lr: 0.02\n",
            "iteration: 290300 loss: 0.0032 lr: 0.02\n",
            "iteration: 290310 loss: 0.0053 lr: 0.02\n",
            "iteration: 290320 loss: 0.0034 lr: 0.02\n",
            "iteration: 290330 loss: 0.0037 lr: 0.02\n",
            "iteration: 290340 loss: 0.0046 lr: 0.02\n",
            "iteration: 290350 loss: 0.0036 lr: 0.02\n",
            "iteration: 290360 loss: 0.0037 lr: 0.02\n",
            "iteration: 290370 loss: 0.0031 lr: 0.02\n",
            "iteration: 290380 loss: 0.0027 lr: 0.02\n",
            "iteration: 290390 loss: 0.0042 lr: 0.02\n",
            "iteration: 290400 loss: 0.0031 lr: 0.02\n",
            "iteration: 290410 loss: 0.0052 lr: 0.02\n",
            "iteration: 290420 loss: 0.0031 lr: 0.02\n",
            "iteration: 290430 loss: 0.0034 lr: 0.02\n",
            "iteration: 290440 loss: 0.0026 lr: 0.02\n",
            "iteration: 290450 loss: 0.0037 lr: 0.02\n",
            "iteration: 290460 loss: 0.0036 lr: 0.02\n",
            "iteration: 290470 loss: 0.0038 lr: 0.02\n",
            "iteration: 290480 loss: 0.0033 lr: 0.02\n",
            "iteration: 290490 loss: 0.0051 lr: 0.02\n",
            "iteration: 290500 loss: 0.0034 lr: 0.02\n",
            "iteration: 290510 loss: 0.0042 lr: 0.02\n",
            "iteration: 290520 loss: 0.0037 lr: 0.02\n",
            "iteration: 290530 loss: 0.0022 lr: 0.02\n",
            "iteration: 290540 loss: 0.0035 lr: 0.02\n",
            "iteration: 290550 loss: 0.0034 lr: 0.02\n",
            "iteration: 290560 loss: 0.0034 lr: 0.02\n",
            "iteration: 290570 loss: 0.0034 lr: 0.02\n",
            "iteration: 290580 loss: 0.0033 lr: 0.02\n",
            "iteration: 290590 loss: 0.0043 lr: 0.02\n",
            "iteration: 290600 loss: 0.0044 lr: 0.02\n",
            "iteration: 290610 loss: 0.0034 lr: 0.02\n",
            "iteration: 290620 loss: 0.0037 lr: 0.02\n",
            "iteration: 290630 loss: 0.0033 lr: 0.02\n",
            "iteration: 290640 loss: 0.0033 lr: 0.02\n",
            "iteration: 290650 loss: 0.0035 lr: 0.02\n",
            "iteration: 290660 loss: 0.0030 lr: 0.02\n",
            "iteration: 290670 loss: 0.0049 lr: 0.02\n",
            "iteration: 290680 loss: 0.0054 lr: 0.02\n",
            "iteration: 290690 loss: 0.0039 lr: 0.02\n",
            "iteration: 290700 loss: 0.0033 lr: 0.02\n",
            "iteration: 290710 loss: 0.0039 lr: 0.02\n",
            "iteration: 290720 loss: 0.0042 lr: 0.02\n",
            "iteration: 290730 loss: 0.0050 lr: 0.02\n",
            "iteration: 290740 loss: 0.0035 lr: 0.02\n",
            "iteration: 290750 loss: 0.0037 lr: 0.02\n",
            "iteration: 290760 loss: 0.0036 lr: 0.02\n",
            "iteration: 290770 loss: 0.0026 lr: 0.02\n",
            "iteration: 290780 loss: 0.0043 lr: 0.02\n",
            "iteration: 290790 loss: 0.0039 lr: 0.02\n",
            "iteration: 290800 loss: 0.0030 lr: 0.02\n",
            "iteration: 290810 loss: 0.0031 lr: 0.02\n",
            "iteration: 290820 loss: 0.0032 lr: 0.02\n",
            "iteration: 290830 loss: 0.0032 lr: 0.02\n",
            "iteration: 290840 loss: 0.0029 lr: 0.02\n",
            "iteration: 290850 loss: 0.0028 lr: 0.02\n",
            "iteration: 290860 loss: 0.0034 lr: 0.02\n",
            "iteration: 290870 loss: 0.0035 lr: 0.02\n",
            "iteration: 290880 loss: 0.0032 lr: 0.02\n",
            "iteration: 290890 loss: 0.0035 lr: 0.02\n",
            "iteration: 290900 loss: 0.0041 lr: 0.02\n",
            "iteration: 290910 loss: 0.0039 lr: 0.02\n",
            "iteration: 290920 loss: 0.0029 lr: 0.02\n",
            "iteration: 290930 loss: 0.0034 lr: 0.02\n",
            "iteration: 290940 loss: 0.0042 lr: 0.02\n",
            "iteration: 290950 loss: 0.0031 lr: 0.02\n",
            "iteration: 290960 loss: 0.0031 lr: 0.02\n",
            "iteration: 290970 loss: 0.0044 lr: 0.02\n",
            "iteration: 290980 loss: 0.0036 lr: 0.02\n",
            "iteration: 290990 loss: 0.0038 lr: 0.02\n",
            "iteration: 291000 loss: 0.0031 lr: 0.02\n",
            "iteration: 291010 loss: 0.0037 lr: 0.02\n",
            "iteration: 291020 loss: 0.0041 lr: 0.02\n",
            "iteration: 291030 loss: 0.0039 lr: 0.02\n",
            "iteration: 291040 loss: 0.0048 lr: 0.02\n",
            "iteration: 291050 loss: 0.0031 lr: 0.02\n",
            "iteration: 291060 loss: 0.0025 lr: 0.02\n",
            "iteration: 291070 loss: 0.0039 lr: 0.02\n",
            "iteration: 291080 loss: 0.0029 lr: 0.02\n",
            "iteration: 291090 loss: 0.0030 lr: 0.02\n",
            "iteration: 291100 loss: 0.0036 lr: 0.02\n",
            "iteration: 291110 loss: 0.0036 lr: 0.02\n",
            "iteration: 291120 loss: 0.0028 lr: 0.02\n",
            "iteration: 291130 loss: 0.0049 lr: 0.02\n",
            "iteration: 291140 loss: 0.0038 lr: 0.02\n",
            "iteration: 291150 loss: 0.0029 lr: 0.02\n",
            "iteration: 291160 loss: 0.0040 lr: 0.02\n",
            "iteration: 291170 loss: 0.0035 lr: 0.02\n",
            "iteration: 291180 loss: 0.0034 lr: 0.02\n",
            "iteration: 291190 loss: 0.0035 lr: 0.02\n",
            "iteration: 291200 loss: 0.0030 lr: 0.02\n",
            "iteration: 291210 loss: 0.0043 lr: 0.02\n",
            "iteration: 291220 loss: 0.0030 lr: 0.02\n",
            "iteration: 291230 loss: 0.0045 lr: 0.02\n",
            "iteration: 291240 loss: 0.0036 lr: 0.02\n",
            "iteration: 291250 loss: 0.0036 lr: 0.02\n",
            "iteration: 291260 loss: 0.0044 lr: 0.02\n",
            "iteration: 291270 loss: 0.0026 lr: 0.02\n",
            "iteration: 291280 loss: 0.0044 lr: 0.02\n",
            "iteration: 291290 loss: 0.0040 lr: 0.02\n",
            "iteration: 291300 loss: 0.0034 lr: 0.02\n",
            "iteration: 291310 loss: 0.0035 lr: 0.02\n",
            "iteration: 291320 loss: 0.0040 lr: 0.02\n",
            "iteration: 291330 loss: 0.0041 lr: 0.02\n",
            "iteration: 291340 loss: 0.0037 lr: 0.02\n",
            "iteration: 291350 loss: 0.0041 lr: 0.02\n",
            "iteration: 291360 loss: 0.0045 lr: 0.02\n",
            "iteration: 291370 loss: 0.0051 lr: 0.02\n",
            "iteration: 291380 loss: 0.0038 lr: 0.02\n",
            "iteration: 291390 loss: 0.0038 lr: 0.02\n",
            "iteration: 291400 loss: 0.0032 lr: 0.02\n",
            "iteration: 291410 loss: 0.0038 lr: 0.02\n",
            "iteration: 291420 loss: 0.0033 lr: 0.02\n",
            "iteration: 291430 loss: 0.0031 lr: 0.02\n",
            "iteration: 291440 loss: 0.0035 lr: 0.02\n",
            "iteration: 291450 loss: 0.0033 lr: 0.02\n",
            "iteration: 291460 loss: 0.0037 lr: 0.02\n",
            "iteration: 291470 loss: 0.0032 lr: 0.02\n",
            "iteration: 291480 loss: 0.0045 lr: 0.02\n",
            "iteration: 291490 loss: 0.0043 lr: 0.02\n",
            "iteration: 291500 loss: 0.0027 lr: 0.02\n",
            "iteration: 291510 loss: 0.0034 lr: 0.02\n",
            "iteration: 291520 loss: 0.0034 lr: 0.02\n",
            "iteration: 291530 loss: 0.0043 lr: 0.02\n",
            "iteration: 291540 loss: 0.0036 lr: 0.02\n",
            "iteration: 291550 loss: 0.0033 lr: 0.02\n",
            "iteration: 291560 loss: 0.0027 lr: 0.02\n",
            "iteration: 291570 loss: 0.0036 lr: 0.02\n",
            "iteration: 291580 loss: 0.0039 lr: 0.02\n",
            "iteration: 291590 loss: 0.0025 lr: 0.02\n",
            "iteration: 291600 loss: 0.0025 lr: 0.02\n",
            "iteration: 291610 loss: 0.0035 lr: 0.02\n",
            "iteration: 291620 loss: 0.0044 lr: 0.02\n",
            "iteration: 291630 loss: 0.0040 lr: 0.02\n",
            "iteration: 291640 loss: 0.0036 lr: 0.02\n",
            "iteration: 291650 loss: 0.0036 lr: 0.02\n",
            "iteration: 291660 loss: 0.0038 lr: 0.02\n",
            "iteration: 291670 loss: 0.0036 lr: 0.02\n",
            "iteration: 291680 loss: 0.0034 lr: 0.02\n",
            "iteration: 291690 loss: 0.0035 lr: 0.02\n",
            "iteration: 291700 loss: 0.0031 lr: 0.02\n",
            "iteration: 291710 loss: 0.0028 lr: 0.02\n",
            "iteration: 291720 loss: 0.0035 lr: 0.02\n",
            "iteration: 291730 loss: 0.0034 lr: 0.02\n",
            "iteration: 291740 loss: 0.0032 lr: 0.02\n",
            "iteration: 291750 loss: 0.0029 lr: 0.02\n",
            "iteration: 291760 loss: 0.0039 lr: 0.02\n",
            "iteration: 291770 loss: 0.0029 lr: 0.02\n",
            "iteration: 291780 loss: 0.0049 lr: 0.02\n",
            "iteration: 291790 loss: 0.0029 lr: 0.02\n",
            "iteration: 291800 loss: 0.0032 lr: 0.02\n",
            "iteration: 291810 loss: 0.0022 lr: 0.02\n",
            "iteration: 291820 loss: 0.0029 lr: 0.02\n",
            "iteration: 291830 loss: 0.0047 lr: 0.02\n",
            "iteration: 291840 loss: 0.0043 lr: 0.02\n",
            "iteration: 291850 loss: 0.0037 lr: 0.02\n",
            "iteration: 291860 loss: 0.0039 lr: 0.02\n",
            "iteration: 291870 loss: 0.0053 lr: 0.02\n",
            "iteration: 291880 loss: 0.0031 lr: 0.02\n",
            "iteration: 291890 loss: 0.0034 lr: 0.02\n",
            "iteration: 291900 loss: 0.0041 lr: 0.02\n",
            "iteration: 291910 loss: 0.0034 lr: 0.02\n",
            "iteration: 291920 loss: 0.0034 lr: 0.02\n",
            "iteration: 291930 loss: 0.0041 lr: 0.02\n",
            "iteration: 291940 loss: 0.0033 lr: 0.02\n",
            "iteration: 291950 loss: 0.0034 lr: 0.02\n",
            "iteration: 291960 loss: 0.0032 lr: 0.02\n",
            "iteration: 291970 loss: 0.0043 lr: 0.02\n",
            "iteration: 291980 loss: 0.0032 lr: 0.02\n",
            "iteration: 291990 loss: 0.0025 lr: 0.02\n",
            "iteration: 292000 loss: 0.0035 lr: 0.02\n",
            "iteration: 292010 loss: 0.0031 lr: 0.02\n",
            "iteration: 292020 loss: 0.0037 lr: 0.02\n",
            "iteration: 292030 loss: 0.0039 lr: 0.02\n",
            "iteration: 292040 loss: 0.0039 lr: 0.02\n",
            "iteration: 292050 loss: 0.0038 lr: 0.02\n",
            "iteration: 292060 loss: 0.0041 lr: 0.02\n",
            "iteration: 292070 loss: 0.0026 lr: 0.02\n",
            "iteration: 292080 loss: 0.0036 lr: 0.02\n",
            "iteration: 292090 loss: 0.0032 lr: 0.02\n",
            "iteration: 292100 loss: 0.0030 lr: 0.02\n",
            "iteration: 292110 loss: 0.0037 lr: 0.02\n",
            "iteration: 292120 loss: 0.0048 lr: 0.02\n",
            "iteration: 292130 loss: 0.0039 lr: 0.02\n",
            "iteration: 292140 loss: 0.0038 lr: 0.02\n",
            "iteration: 292150 loss: 0.0033 lr: 0.02\n",
            "iteration: 292160 loss: 0.0044 lr: 0.02\n",
            "iteration: 292170 loss: 0.0041 lr: 0.02\n",
            "iteration: 292180 loss: 0.0030 lr: 0.02\n",
            "iteration: 292190 loss: 0.0036 lr: 0.02\n",
            "iteration: 292200 loss: 0.0031 lr: 0.02\n",
            "iteration: 292210 loss: 0.0030 lr: 0.02\n",
            "iteration: 292220 loss: 0.0035 lr: 0.02\n",
            "iteration: 292230 loss: 0.0040 lr: 0.02\n",
            "iteration: 292240 loss: 0.0042 lr: 0.02\n",
            "iteration: 292250 loss: 0.0031 lr: 0.02\n",
            "iteration: 292260 loss: 0.0029 lr: 0.02\n",
            "iteration: 292270 loss: 0.0034 lr: 0.02\n",
            "iteration: 292280 loss: 0.0034 lr: 0.02\n",
            "iteration: 292290 loss: 0.0030 lr: 0.02\n",
            "iteration: 292300 loss: 0.0029 lr: 0.02\n",
            "iteration: 292310 loss: 0.0050 lr: 0.02\n",
            "iteration: 292320 loss: 0.0026 lr: 0.02\n",
            "iteration: 292330 loss: 0.0039 lr: 0.02\n",
            "iteration: 292340 loss: 0.0037 lr: 0.02\n",
            "iteration: 292350 loss: 0.0041 lr: 0.02\n",
            "iteration: 292360 loss: 0.0029 lr: 0.02\n",
            "iteration: 292370 loss: 0.0029 lr: 0.02\n",
            "iteration: 292380 loss: 0.0031 lr: 0.02\n",
            "iteration: 292390 loss: 0.0033 lr: 0.02\n",
            "iteration: 292400 loss: 0.0027 lr: 0.02\n",
            "iteration: 292410 loss: 0.0034 lr: 0.02\n",
            "iteration: 292420 loss: 0.0028 lr: 0.02\n",
            "iteration: 292430 loss: 0.0050 lr: 0.02\n",
            "iteration: 292440 loss: 0.0051 lr: 0.02\n",
            "iteration: 292450 loss: 0.0031 lr: 0.02\n",
            "iteration: 292460 loss: 0.0035 lr: 0.02\n",
            "iteration: 292470 loss: 0.0034 lr: 0.02\n",
            "iteration: 292480 loss: 0.0032 lr: 0.02\n",
            "iteration: 292490 loss: 0.0033 lr: 0.02\n",
            "iteration: 292500 loss: 0.0042 lr: 0.02\n",
            "iteration: 292510 loss: 0.0047 lr: 0.02\n",
            "iteration: 292520 loss: 0.0038 lr: 0.02\n",
            "iteration: 292530 loss: 0.0037 lr: 0.02\n",
            "iteration: 292540 loss: 0.0040 lr: 0.02\n",
            "iteration: 292550 loss: 0.0037 lr: 0.02\n",
            "iteration: 292560 loss: 0.0029 lr: 0.02\n",
            "iteration: 292570 loss: 0.0038 lr: 0.02\n",
            "iteration: 292580 loss: 0.0033 lr: 0.02\n",
            "iteration: 292590 loss: 0.0034 lr: 0.02\n",
            "iteration: 292600 loss: 0.0040 lr: 0.02\n",
            "iteration: 292610 loss: 0.0033 lr: 0.02\n",
            "iteration: 292620 loss: 0.0041 lr: 0.02\n",
            "iteration: 292630 loss: 0.0031 lr: 0.02\n",
            "iteration: 292640 loss: 0.0046 lr: 0.02\n",
            "iteration: 292650 loss: 0.0034 lr: 0.02\n",
            "iteration: 292660 loss: 0.0030 lr: 0.02\n",
            "iteration: 292670 loss: 0.0039 lr: 0.02\n",
            "iteration: 292680 loss: 0.0042 lr: 0.02\n",
            "iteration: 292690 loss: 0.0040 lr: 0.02\n",
            "iteration: 292700 loss: 0.0047 lr: 0.02\n",
            "iteration: 292710 loss: 0.0037 lr: 0.02\n",
            "iteration: 292720 loss: 0.0039 lr: 0.02\n",
            "iteration: 292730 loss: 0.0034 lr: 0.02\n",
            "iteration: 292740 loss: 0.0035 lr: 0.02\n",
            "iteration: 292750 loss: 0.0038 lr: 0.02\n",
            "iteration: 292760 loss: 0.0024 lr: 0.02\n",
            "iteration: 292770 loss: 0.0041 lr: 0.02\n",
            "iteration: 292780 loss: 0.0038 lr: 0.02\n",
            "iteration: 292790 loss: 0.0030 lr: 0.02\n",
            "iteration: 292800 loss: 0.0030 lr: 0.02\n",
            "iteration: 292810 loss: 0.0031 lr: 0.02\n",
            "iteration: 292820 loss: 0.0034 lr: 0.02\n",
            "iteration: 292830 loss: 0.0039 lr: 0.02\n",
            "iteration: 292840 loss: 0.0027 lr: 0.02\n",
            "iteration: 292850 loss: 0.0032 lr: 0.02\n",
            "iteration: 292860 loss: 0.0035 lr: 0.02\n",
            "iteration: 292870 loss: 0.0033 lr: 0.02\n",
            "iteration: 292880 loss: 0.0030 lr: 0.02\n",
            "iteration: 292890 loss: 0.0044 lr: 0.02\n",
            "iteration: 292900 loss: 0.0034 lr: 0.02\n",
            "iteration: 292910 loss: 0.0034 lr: 0.02\n",
            "iteration: 292920 loss: 0.0034 lr: 0.02\n",
            "iteration: 292930 loss: 0.0038 lr: 0.02\n",
            "iteration: 292940 loss: 0.0031 lr: 0.02\n",
            "iteration: 292950 loss: 0.0039 lr: 0.02\n",
            "iteration: 292960 loss: 0.0033 lr: 0.02\n",
            "iteration: 292970 loss: 0.0033 lr: 0.02\n",
            "iteration: 292980 loss: 0.0031 lr: 0.02\n",
            "iteration: 292990 loss: 0.0027 lr: 0.02\n",
            "iteration: 293000 loss: 0.0030 lr: 0.02\n",
            "iteration: 293010 loss: 0.0030 lr: 0.02\n",
            "iteration: 293020 loss: 0.0041 lr: 0.02\n",
            "iteration: 293030 loss: 0.0032 lr: 0.02\n",
            "iteration: 293040 loss: 0.0026 lr: 0.02\n",
            "iteration: 293050 loss: 0.0036 lr: 0.02\n",
            "iteration: 293060 loss: 0.0034 lr: 0.02\n",
            "iteration: 293070 loss: 0.0028 lr: 0.02\n",
            "iteration: 293080 loss: 0.0036 lr: 0.02\n",
            "iteration: 293090 loss: 0.0036 lr: 0.02\n",
            "iteration: 293100 loss: 0.0028 lr: 0.02\n",
            "iteration: 293110 loss: 0.0043 lr: 0.02\n",
            "iteration: 293120 loss: 0.0035 lr: 0.02\n",
            "iteration: 293130 loss: 0.0027 lr: 0.02\n",
            "iteration: 293140 loss: 0.0022 lr: 0.02\n",
            "iteration: 293150 loss: 0.0045 lr: 0.02\n",
            "iteration: 293160 loss: 0.0038 lr: 0.02\n",
            "iteration: 293170 loss: 0.0037 lr: 0.02\n",
            "iteration: 293180 loss: 0.0043 lr: 0.02\n",
            "iteration: 293190 loss: 0.0030 lr: 0.02\n",
            "iteration: 293200 loss: 0.0044 lr: 0.02\n",
            "iteration: 293210 loss: 0.0037 lr: 0.02\n",
            "iteration: 293220 loss: 0.0042 lr: 0.02\n",
            "iteration: 293230 loss: 0.0040 lr: 0.02\n",
            "iteration: 293240 loss: 0.0033 lr: 0.02\n",
            "iteration: 293250 loss: 0.0038 lr: 0.02\n",
            "iteration: 293260 loss: 0.0036 lr: 0.02\n",
            "iteration: 293270 loss: 0.0028 lr: 0.02\n",
            "iteration: 293280 loss: 0.0031 lr: 0.02\n",
            "iteration: 293290 loss: 0.0027 lr: 0.02\n",
            "iteration: 293300 loss: 0.0034 lr: 0.02\n",
            "iteration: 293310 loss: 0.0029 lr: 0.02\n",
            "iteration: 293320 loss: 0.0039 lr: 0.02\n",
            "iteration: 293330 loss: 0.0040 lr: 0.02\n",
            "iteration: 293340 loss: 0.0034 lr: 0.02\n",
            "iteration: 293350 loss: 0.0038 lr: 0.02\n",
            "iteration: 293360 loss: 0.0035 lr: 0.02\n",
            "iteration: 293370 loss: 0.0033 lr: 0.02\n",
            "iteration: 293380 loss: 0.0035 lr: 0.02\n",
            "iteration: 293390 loss: 0.0040 lr: 0.02\n",
            "iteration: 293400 loss: 0.0035 lr: 0.02\n",
            "iteration: 293410 loss: 0.0035 lr: 0.02\n",
            "iteration: 293420 loss: 0.0037 lr: 0.02\n",
            "iteration: 293430 loss: 0.0033 lr: 0.02\n",
            "iteration: 293440 loss: 0.0042 lr: 0.02\n",
            "iteration: 293450 loss: 0.0036 lr: 0.02\n",
            "iteration: 293460 loss: 0.0028 lr: 0.02\n",
            "iteration: 293470 loss: 0.0034 lr: 0.02\n",
            "iteration: 293480 loss: 0.0040 lr: 0.02\n",
            "iteration: 293490 loss: 0.0044 lr: 0.02\n",
            "iteration: 293500 loss: 0.0031 lr: 0.02\n",
            "iteration: 293510 loss: 0.0034 lr: 0.02\n",
            "iteration: 293520 loss: 0.0043 lr: 0.02\n",
            "iteration: 293530 loss: 0.0043 lr: 0.02\n",
            "iteration: 293540 loss: 0.0033 lr: 0.02\n",
            "iteration: 293550 loss: 0.0037 lr: 0.02\n",
            "iteration: 293560 loss: 0.0032 lr: 0.02\n",
            "iteration: 293570 loss: 0.0042 lr: 0.02\n",
            "iteration: 293580 loss: 0.0044 lr: 0.02\n",
            "iteration: 293590 loss: 0.0036 lr: 0.02\n",
            "iteration: 293600 loss: 0.0036 lr: 0.02\n",
            "iteration: 293610 loss: 0.0037 lr: 0.02\n",
            "iteration: 293620 loss: 0.0026 lr: 0.02\n",
            "iteration: 293630 loss: 0.0039 lr: 0.02\n",
            "iteration: 293640 loss: 0.0037 lr: 0.02\n",
            "iteration: 293650 loss: 0.0034 lr: 0.02\n",
            "iteration: 293660 loss: 0.0035 lr: 0.02\n",
            "iteration: 293670 loss: 0.0026 lr: 0.02\n",
            "iteration: 293680 loss: 0.0034 lr: 0.02\n",
            "iteration: 293690 loss: 0.0035 lr: 0.02\n",
            "iteration: 293700 loss: 0.0038 lr: 0.02\n",
            "iteration: 293710 loss: 0.0042 lr: 0.02\n",
            "iteration: 293720 loss: 0.0037 lr: 0.02\n",
            "iteration: 293730 loss: 0.0035 lr: 0.02\n",
            "iteration: 293740 loss: 0.0041 lr: 0.02\n",
            "iteration: 293750 loss: 0.0035 lr: 0.02\n",
            "iteration: 293760 loss: 0.0032 lr: 0.02\n",
            "iteration: 293770 loss: 0.0033 lr: 0.02\n",
            "iteration: 293780 loss: 0.0030 lr: 0.02\n",
            "iteration: 293790 loss: 0.0029 lr: 0.02\n",
            "iteration: 293800 loss: 0.0036 lr: 0.02\n",
            "iteration: 293810 loss: 0.0052 lr: 0.02\n",
            "iteration: 293820 loss: 0.0050 lr: 0.02\n",
            "iteration: 293830 loss: 0.0051 lr: 0.02\n",
            "iteration: 293840 loss: 0.0049 lr: 0.02\n",
            "iteration: 293850 loss: 0.0037 lr: 0.02\n",
            "iteration: 293860 loss: 0.0060 lr: 0.02\n",
            "iteration: 293870 loss: 0.0048 lr: 0.02\n",
            "iteration: 293880 loss: 0.0033 lr: 0.02\n",
            "iteration: 293890 loss: 0.0033 lr: 0.02\n",
            "iteration: 293900 loss: 0.0039 lr: 0.02\n",
            "iteration: 293910 loss: 0.0033 lr: 0.02\n",
            "iteration: 293920 loss: 0.0033 lr: 0.02\n",
            "iteration: 293930 loss: 0.0039 lr: 0.02\n",
            "iteration: 293940 loss: 0.0031 lr: 0.02\n",
            "iteration: 293950 loss: 0.0042 lr: 0.02\n",
            "iteration: 293960 loss: 0.0030 lr: 0.02\n",
            "iteration: 293970 loss: 0.0032 lr: 0.02\n",
            "iteration: 293980 loss: 0.0038 lr: 0.02\n",
            "iteration: 293990 loss: 0.0043 lr: 0.02\n",
            "iteration: 294000 loss: 0.0024 lr: 0.02\n",
            "iteration: 294010 loss: 0.0033 lr: 0.02\n",
            "iteration: 294020 loss: 0.0037 lr: 0.02\n",
            "iteration: 294030 loss: 0.0038 lr: 0.02\n",
            "iteration: 294040 loss: 0.0038 lr: 0.02\n",
            "iteration: 294050 loss: 0.0043 lr: 0.02\n",
            "iteration: 294060 loss: 0.0047 lr: 0.02\n",
            "iteration: 294070 loss: 0.0039 lr: 0.02\n",
            "iteration: 294080 loss: 0.0029 lr: 0.02\n",
            "iteration: 294090 loss: 0.0039 lr: 0.02\n",
            "iteration: 294100 loss: 0.0034 lr: 0.02\n",
            "iteration: 294110 loss: 0.0033 lr: 0.02\n",
            "iteration: 294120 loss: 0.0038 lr: 0.02\n",
            "iteration: 294130 loss: 0.0039 lr: 0.02\n",
            "iteration: 294140 loss: 0.0035 lr: 0.02\n",
            "iteration: 294150 loss: 0.0032 lr: 0.02\n",
            "iteration: 294160 loss: 0.0033 lr: 0.02\n",
            "iteration: 294170 loss: 0.0033 lr: 0.02\n",
            "iteration: 294180 loss: 0.0035 lr: 0.02\n",
            "iteration: 294190 loss: 0.0036 lr: 0.02\n",
            "iteration: 294200 loss: 0.0029 lr: 0.02\n",
            "iteration: 294210 loss: 0.0031 lr: 0.02\n",
            "iteration: 294220 loss: 0.0033 lr: 0.02\n",
            "iteration: 294230 loss: 0.0048 lr: 0.02\n",
            "iteration: 294240 loss: 0.0041 lr: 0.02\n",
            "iteration: 294250 loss: 0.0029 lr: 0.02\n",
            "iteration: 294260 loss: 0.0043 lr: 0.02\n",
            "iteration: 294270 loss: 0.0045 lr: 0.02\n",
            "iteration: 294280 loss: 0.0043 lr: 0.02\n",
            "iteration: 294290 loss: 0.0037 lr: 0.02\n",
            "iteration: 294300 loss: 0.0044 lr: 0.02\n",
            "iteration: 294310 loss: 0.0047 lr: 0.02\n",
            "iteration: 294320 loss: 0.0034 lr: 0.02\n",
            "iteration: 294330 loss: 0.0038 lr: 0.02\n",
            "iteration: 294340 loss: 0.0028 lr: 0.02\n",
            "iteration: 294350 loss: 0.0043 lr: 0.02\n",
            "iteration: 294360 loss: 0.0035 lr: 0.02\n",
            "iteration: 294370 loss: 0.0035 lr: 0.02\n",
            "iteration: 294380 loss: 0.0035 lr: 0.02\n",
            "iteration: 294390 loss: 0.0034 lr: 0.02\n",
            "iteration: 294400 loss: 0.0035 lr: 0.02\n",
            "iteration: 294410 loss: 0.0040 lr: 0.02\n",
            "iteration: 294420 loss: 0.0028 lr: 0.02\n",
            "iteration: 294430 loss: 0.0031 lr: 0.02\n",
            "iteration: 294440 loss: 0.0031 lr: 0.02\n",
            "iteration: 294450 loss: 0.0038 lr: 0.02\n",
            "iteration: 294460 loss: 0.0038 lr: 0.02\n",
            "iteration: 294470 loss: 0.0036 lr: 0.02\n",
            "iteration: 294480 loss: 0.0034 lr: 0.02\n",
            "iteration: 294490 loss: 0.0036 lr: 0.02\n",
            "iteration: 294500 loss: 0.0037 lr: 0.02\n",
            "iteration: 294510 loss: 0.0036 lr: 0.02\n",
            "iteration: 294520 loss: 0.0047 lr: 0.02\n",
            "iteration: 294530 loss: 0.0032 lr: 0.02\n",
            "iteration: 294540 loss: 0.0034 lr: 0.02\n",
            "iteration: 294550 loss: 0.0035 lr: 0.02\n",
            "iteration: 294560 loss: 0.0039 lr: 0.02\n",
            "iteration: 294570 loss: 0.0037 lr: 0.02\n",
            "iteration: 294580 loss: 0.0037 lr: 0.02\n",
            "iteration: 294590 loss: 0.0036 lr: 0.02\n",
            "iteration: 294600 loss: 0.0037 lr: 0.02\n",
            "iteration: 294610 loss: 0.0032 lr: 0.02\n",
            "iteration: 294620 loss: 0.0045 lr: 0.02\n",
            "iteration: 294630 loss: 0.0031 lr: 0.02\n",
            "iteration: 294640 loss: 0.0037 lr: 0.02\n",
            "iteration: 294650 loss: 0.0025 lr: 0.02\n",
            "iteration: 294660 loss: 0.0045 lr: 0.02\n",
            "iteration: 294670 loss: 0.0043 lr: 0.02\n",
            "iteration: 294680 loss: 0.0040 lr: 0.02\n",
            "iteration: 294690 loss: 0.0028 lr: 0.02\n",
            "iteration: 294700 loss: 0.0040 lr: 0.02\n",
            "iteration: 294710 loss: 0.0033 lr: 0.02\n",
            "iteration: 294720 loss: 0.0033 lr: 0.02\n",
            "iteration: 294730 loss: 0.0048 lr: 0.02\n",
            "iteration: 294740 loss: 0.0038 lr: 0.02\n",
            "iteration: 294750 loss: 0.0038 lr: 0.02\n",
            "iteration: 294760 loss: 0.0027 lr: 0.02\n",
            "iteration: 294770 loss: 0.0024 lr: 0.02\n",
            "iteration: 294780 loss: 0.0041 lr: 0.02\n",
            "iteration: 294790 loss: 0.0037 lr: 0.02\n",
            "iteration: 294800 loss: 0.0031 lr: 0.02\n",
            "iteration: 294810 loss: 0.0031 lr: 0.02\n",
            "iteration: 294820 loss: 0.0040 lr: 0.02\n",
            "iteration: 294830 loss: 0.0033 lr: 0.02\n",
            "iteration: 294840 loss: 0.0049 lr: 0.02\n",
            "iteration: 294850 loss: 0.0041 lr: 0.02\n",
            "iteration: 294860 loss: 0.0027 lr: 0.02\n",
            "iteration: 294870 loss: 0.0042 lr: 0.02\n",
            "iteration: 294880 loss: 0.0028 lr: 0.02\n",
            "iteration: 294890 loss: 0.0037 lr: 0.02\n",
            "iteration: 294900 loss: 0.0031 lr: 0.02\n",
            "iteration: 294910 loss: 0.0031 lr: 0.02\n",
            "iteration: 294920 loss: 0.0028 lr: 0.02\n",
            "iteration: 294930 loss: 0.0042 lr: 0.02\n",
            "iteration: 294940 loss: 0.0033 lr: 0.02\n",
            "iteration: 294950 loss: 0.0039 lr: 0.02\n",
            "iteration: 294960 loss: 0.0037 lr: 0.02\n",
            "iteration: 294970 loss: 0.0058 lr: 0.02\n",
            "iteration: 294980 loss: 0.0065 lr: 0.02\n",
            "iteration: 294990 loss: 0.0050 lr: 0.02\n",
            "iteration: 295000 loss: 0.0056 lr: 0.02\n",
            "iteration: 295010 loss: 0.0050 lr: 0.02\n",
            "iteration: 295020 loss: 0.0038 lr: 0.02\n",
            "iteration: 295030 loss: 0.0036 lr: 0.02\n",
            "iteration: 295040 loss: 0.0040 lr: 0.02\n",
            "iteration: 295050 loss: 0.0031 lr: 0.02\n",
            "iteration: 295060 loss: 0.0027 lr: 0.02\n",
            "iteration: 295070 loss: 0.0034 lr: 0.02\n",
            "iteration: 295080 loss: 0.0037 lr: 0.02\n",
            "iteration: 295090 loss: 0.0029 lr: 0.02\n",
            "iteration: 295100 loss: 0.0040 lr: 0.02\n",
            "iteration: 295110 loss: 0.0038 lr: 0.02\n",
            "iteration: 295120 loss: 0.0041 lr: 0.02\n",
            "iteration: 295130 loss: 0.0029 lr: 0.02\n",
            "iteration: 295140 loss: 0.0044 lr: 0.02\n",
            "iteration: 295150 loss: 0.0035 lr: 0.02\n",
            "iteration: 295160 loss: 0.0035 lr: 0.02\n",
            "iteration: 295170 loss: 0.0039 lr: 0.02\n",
            "iteration: 295180 loss: 0.0044 lr: 0.02\n",
            "iteration: 295190 loss: 0.0042 lr: 0.02\n",
            "iteration: 295200 loss: 0.0031 lr: 0.02\n",
            "iteration: 295210 loss: 0.0028 lr: 0.02\n",
            "iteration: 295220 loss: 0.0035 lr: 0.02\n",
            "iteration: 295230 loss: 0.0037 lr: 0.02\n",
            "iteration: 295240 loss: 0.0036 lr: 0.02\n",
            "iteration: 295250 loss: 0.0026 lr: 0.02\n",
            "iteration: 295260 loss: 0.0031 lr: 0.02\n",
            "iteration: 295270 loss: 0.0030 lr: 0.02\n",
            "iteration: 295280 loss: 0.0044 lr: 0.02\n",
            "iteration: 295290 loss: 0.0032 lr: 0.02\n",
            "iteration: 295300 loss: 0.0022 lr: 0.02\n",
            "iteration: 295310 loss: 0.0034 lr: 0.02\n",
            "iteration: 295320 loss: 0.0032 lr: 0.02\n",
            "iteration: 295330 loss: 0.0044 lr: 0.02\n",
            "iteration: 295340 loss: 0.0036 lr: 0.02\n",
            "iteration: 295350 loss: 0.0040 lr: 0.02\n",
            "iteration: 295360 loss: 0.0032 lr: 0.02\n",
            "iteration: 295370 loss: 0.0031 lr: 0.02\n",
            "iteration: 295380 loss: 0.0027 lr: 0.02\n",
            "iteration: 295390 loss: 0.0036 lr: 0.02\n",
            "iteration: 295400 loss: 0.0037 lr: 0.02\n",
            "iteration: 295410 loss: 0.0033 lr: 0.02\n",
            "iteration: 295420 loss: 0.0042 lr: 0.02\n",
            "iteration: 295430 loss: 0.0044 lr: 0.02\n",
            "iteration: 295440 loss: 0.0040 lr: 0.02\n",
            "iteration: 295450 loss: 0.0034 lr: 0.02\n",
            "iteration: 295460 loss: 0.0043 lr: 0.02\n",
            "iteration: 295470 loss: 0.0029 lr: 0.02\n",
            "iteration: 295480 loss: 0.0030 lr: 0.02\n",
            "iteration: 295490 loss: 0.0042 lr: 0.02\n",
            "iteration: 295500 loss: 0.0034 lr: 0.02\n",
            "iteration: 295510 loss: 0.0046 lr: 0.02\n",
            "iteration: 295520 loss: 0.0033 lr: 0.02\n",
            "iteration: 295530 loss: 0.0036 lr: 0.02\n",
            "iteration: 295540 loss: 0.0040 lr: 0.02\n",
            "iteration: 295550 loss: 0.0041 lr: 0.02\n",
            "iteration: 295560 loss: 0.0028 lr: 0.02\n",
            "iteration: 295570 loss: 0.0036 lr: 0.02\n",
            "iteration: 295580 loss: 0.0023 lr: 0.02\n",
            "iteration: 295590 loss: 0.0033 lr: 0.02\n",
            "iteration: 295600 loss: 0.0030 lr: 0.02\n",
            "iteration: 295610 loss: 0.0027 lr: 0.02\n",
            "iteration: 295620 loss: 0.0036 lr: 0.02\n",
            "iteration: 295630 loss: 0.0036 lr: 0.02\n",
            "iteration: 295640 loss: 0.0044 lr: 0.02\n",
            "iteration: 295650 loss: 0.0034 lr: 0.02\n",
            "iteration: 295660 loss: 0.0025 lr: 0.02\n",
            "iteration: 295670 loss: 0.0032 lr: 0.02\n",
            "iteration: 295680 loss: 0.0033 lr: 0.02\n",
            "iteration: 295690 loss: 0.0029 lr: 0.02\n",
            "iteration: 295700 loss: 0.0050 lr: 0.02\n",
            "iteration: 295710 loss: 0.0044 lr: 0.02\n",
            "iteration: 295720 loss: 0.0038 lr: 0.02\n",
            "iteration: 295730 loss: 0.0032 lr: 0.02\n",
            "iteration: 295740 loss: 0.0036 lr: 0.02\n",
            "iteration: 295750 loss: 0.0033 lr: 0.02\n",
            "iteration: 295760 loss: 0.0044 lr: 0.02\n",
            "iteration: 295770 loss: 0.0039 lr: 0.02\n",
            "iteration: 295780 loss: 0.0037 lr: 0.02\n",
            "iteration: 295790 loss: 0.0028 lr: 0.02\n",
            "iteration: 295800 loss: 0.0034 lr: 0.02\n",
            "iteration: 295810 loss: 0.0023 lr: 0.02\n",
            "iteration: 295820 loss: 0.0031 lr: 0.02\n",
            "iteration: 295830 loss: 0.0044 lr: 0.02\n",
            "iteration: 295840 loss: 0.0048 lr: 0.02\n",
            "iteration: 295850 loss: 0.0031 lr: 0.02\n",
            "iteration: 295860 loss: 0.0032 lr: 0.02\n",
            "iteration: 295870 loss: 0.0032 lr: 0.02\n",
            "iteration: 295880 loss: 0.0040 lr: 0.02\n",
            "iteration: 295890 loss: 0.0042 lr: 0.02\n",
            "iteration: 295900 loss: 0.0033 lr: 0.02\n",
            "iteration: 295910 loss: 0.0029 lr: 0.02\n",
            "iteration: 295920 loss: 0.0032 lr: 0.02\n",
            "iteration: 295930 loss: 0.0030 lr: 0.02\n",
            "iteration: 295940 loss: 0.0028 lr: 0.02\n",
            "iteration: 295950 loss: 0.0027 lr: 0.02\n",
            "iteration: 295960 loss: 0.0031 lr: 0.02\n",
            "iteration: 295970 loss: 0.0029 lr: 0.02\n",
            "iteration: 295980 loss: 0.0033 lr: 0.02\n",
            "iteration: 295990 loss: 0.0027 lr: 0.02\n",
            "iteration: 296000 loss: 0.0039 lr: 0.02\n",
            "iteration: 296010 loss: 0.0025 lr: 0.02\n",
            "iteration: 296020 loss: 0.0026 lr: 0.02\n",
            "iteration: 296030 loss: 0.0046 lr: 0.02\n",
            "iteration: 296040 loss: 0.0043 lr: 0.02\n",
            "iteration: 296050 loss: 0.0057 lr: 0.02\n",
            "iteration: 296060 loss: 0.0045 lr: 0.02\n",
            "iteration: 296070 loss: 0.0052 lr: 0.02\n",
            "iteration: 296080 loss: 0.0034 lr: 0.02\n",
            "iteration: 296090 loss: 0.0039 lr: 0.02\n",
            "iteration: 296100 loss: 0.0052 lr: 0.02\n",
            "iteration: 296110 loss: 0.0042 lr: 0.02\n",
            "iteration: 296120 loss: 0.0041 lr: 0.02\n",
            "iteration: 296130 loss: 0.0029 lr: 0.02\n",
            "iteration: 296140 loss: 0.0042 lr: 0.02\n",
            "iteration: 296150 loss: 0.0054 lr: 0.02\n",
            "iteration: 296160 loss: 0.0043 lr: 0.02\n",
            "iteration: 296170 loss: 0.0034 lr: 0.02\n",
            "iteration: 296180 loss: 0.0037 lr: 0.02\n",
            "iteration: 296190 loss: 0.0046 lr: 0.02\n",
            "iteration: 296200 loss: 0.0041 lr: 0.02\n",
            "iteration: 296210 loss: 0.0072 lr: 0.02\n",
            "iteration: 296220 loss: 0.0054 lr: 0.02\n",
            "iteration: 296230 loss: 0.0082 lr: 0.02\n",
            "iteration: 296240 loss: 0.0043 lr: 0.02\n",
            "iteration: 296250 loss: 0.0050 lr: 0.02\n",
            "iteration: 296260 loss: 0.0037 lr: 0.02\n",
            "iteration: 296270 loss: 0.0033 lr: 0.02\n",
            "iteration: 296280 loss: 0.0027 lr: 0.02\n",
            "iteration: 296290 loss: 0.0037 lr: 0.02\n",
            "iteration: 296300 loss: 0.0045 lr: 0.02\n",
            "iteration: 296310 loss: 0.0030 lr: 0.02\n",
            "iteration: 296320 loss: 0.0035 lr: 0.02\n",
            "iteration: 296330 loss: 0.0048 lr: 0.02\n",
            "iteration: 296340 loss: 0.0051 lr: 0.02\n",
            "iteration: 296350 loss: 0.0037 lr: 0.02\n",
            "iteration: 296360 loss: 0.0040 lr: 0.02\n",
            "iteration: 296370 loss: 0.0046 lr: 0.02\n",
            "iteration: 296380 loss: 0.0050 lr: 0.02\n",
            "iteration: 296390 loss: 0.0044 lr: 0.02\n",
            "iteration: 296400 loss: 0.0034 lr: 0.02\n",
            "iteration: 296410 loss: 0.0031 lr: 0.02\n",
            "iteration: 296420 loss: 0.0031 lr: 0.02\n",
            "iteration: 296430 loss: 0.0031 lr: 0.02\n",
            "iteration: 296440 loss: 0.0038 lr: 0.02\n",
            "iteration: 296450 loss: 0.0035 lr: 0.02\n",
            "iteration: 296460 loss: 0.0045 lr: 0.02\n",
            "iteration: 296470 loss: 0.0035 lr: 0.02\n",
            "iteration: 296480 loss: 0.0035 lr: 0.02\n",
            "iteration: 296490 loss: 0.0040 lr: 0.02\n",
            "iteration: 296500 loss: 0.0039 lr: 0.02\n",
            "iteration: 296510 loss: 0.0035 lr: 0.02\n",
            "iteration: 296520 loss: 0.0043 lr: 0.02\n",
            "iteration: 296530 loss: 0.0029 lr: 0.02\n",
            "iteration: 296540 loss: 0.0030 lr: 0.02\n",
            "iteration: 296550 loss: 0.0037 lr: 0.02\n",
            "iteration: 296560 loss: 0.0037 lr: 0.02\n",
            "iteration: 296570 loss: 0.0031 lr: 0.02\n",
            "iteration: 296580 loss: 0.0036 lr: 0.02\n",
            "iteration: 296590 loss: 0.0039 lr: 0.02\n",
            "iteration: 296600 loss: 0.0050 lr: 0.02\n",
            "iteration: 296610 loss: 0.0036 lr: 0.02\n",
            "iteration: 296620 loss: 0.0027 lr: 0.02\n",
            "iteration: 296630 loss: 0.0034 lr: 0.02\n",
            "iteration: 296640 loss: 0.0032 lr: 0.02\n",
            "iteration: 296650 loss: 0.0033 lr: 0.02\n",
            "iteration: 296660 loss: 0.0041 lr: 0.02\n",
            "iteration: 296670 loss: 0.0028 lr: 0.02\n",
            "iteration: 296680 loss: 0.0044 lr: 0.02\n",
            "iteration: 296690 loss: 0.0029 lr: 0.02\n",
            "iteration: 296700 loss: 0.0042 lr: 0.02\n",
            "iteration: 296710 loss: 0.0046 lr: 0.02\n",
            "iteration: 296720 loss: 0.0046 lr: 0.02\n",
            "iteration: 296730 loss: 0.0041 lr: 0.02\n",
            "iteration: 296740 loss: 0.0039 lr: 0.02\n",
            "iteration: 296750 loss: 0.0030 lr: 0.02\n",
            "iteration: 296760 loss: 0.0034 lr: 0.02\n",
            "iteration: 296770 loss: 0.0035 lr: 0.02\n",
            "iteration: 296780 loss: 0.0045 lr: 0.02\n",
            "iteration: 296790 loss: 0.0033 lr: 0.02\n",
            "iteration: 296800 loss: 0.0027 lr: 0.02\n",
            "iteration: 296810 loss: 0.0034 lr: 0.02\n",
            "iteration: 296820 loss: 0.0048 lr: 0.02\n",
            "iteration: 296830 loss: 0.0027 lr: 0.02\n",
            "iteration: 296840 loss: 0.0049 lr: 0.02\n",
            "iteration: 296850 loss: 0.0042 lr: 0.02\n",
            "iteration: 296860 loss: 0.0034 lr: 0.02\n",
            "iteration: 296870 loss: 0.0039 lr: 0.02\n",
            "iteration: 296880 loss: 0.0047 lr: 0.02\n",
            "iteration: 296890 loss: 0.0039 lr: 0.02\n",
            "iteration: 296900 loss: 0.0038 lr: 0.02\n",
            "iteration: 296910 loss: 0.0028 lr: 0.02\n",
            "iteration: 296920 loss: 0.0036 lr: 0.02\n",
            "iteration: 296930 loss: 0.0025 lr: 0.02\n",
            "iteration: 296940 loss: 0.0023 lr: 0.02\n",
            "iteration: 296950 loss: 0.0035 lr: 0.02\n",
            "iteration: 296960 loss: 0.0037 lr: 0.02\n",
            "iteration: 296970 loss: 0.0044 lr: 0.02\n",
            "iteration: 296980 loss: 0.0055 lr: 0.02\n",
            "iteration: 296990 loss: 0.0031 lr: 0.02\n",
            "iteration: 297000 loss: 0.0033 lr: 0.02\n",
            "iteration: 297010 loss: 0.0039 lr: 0.02\n",
            "iteration: 297020 loss: 0.0036 lr: 0.02\n",
            "iteration: 297030 loss: 0.0028 lr: 0.02\n",
            "iteration: 297040 loss: 0.0032 lr: 0.02\n",
            "iteration: 297050 loss: 0.0025 lr: 0.02\n",
            "iteration: 297060 loss: 0.0028 lr: 0.02\n",
            "iteration: 297070 loss: 0.0040 lr: 0.02\n",
            "iteration: 297080 loss: 0.0032 lr: 0.02\n",
            "iteration: 297090 loss: 0.0048 lr: 0.02\n",
            "iteration: 297100 loss: 0.0032 lr: 0.02\n",
            "iteration: 297110 loss: 0.0035 lr: 0.02\n",
            "iteration: 297120 loss: 0.0042 lr: 0.02\n",
            "iteration: 297130 loss: 0.0036 lr: 0.02\n",
            "iteration: 297140 loss: 0.0028 lr: 0.02\n",
            "iteration: 297150 loss: 0.0032 lr: 0.02\n",
            "iteration: 297160 loss: 0.0050 lr: 0.02\n",
            "iteration: 297170 loss: 0.0047 lr: 0.02\n",
            "iteration: 297180 loss: 0.0028 lr: 0.02\n",
            "iteration: 297190 loss: 0.0034 lr: 0.02\n",
            "iteration: 297200 loss: 0.0031 lr: 0.02\n",
            "iteration: 297210 loss: 0.0035 lr: 0.02\n",
            "iteration: 297220 loss: 0.0034 lr: 0.02\n",
            "iteration: 297230 loss: 0.0030 lr: 0.02\n",
            "iteration: 297240 loss: 0.0033 lr: 0.02\n",
            "iteration: 297250 loss: 0.0022 lr: 0.02\n",
            "iteration: 297260 loss: 0.0031 lr: 0.02\n",
            "iteration: 297270 loss: 0.0040 lr: 0.02\n",
            "iteration: 297280 loss: 0.0037 lr: 0.02\n",
            "iteration: 297290 loss: 0.0030 lr: 0.02\n",
            "iteration: 297300 loss: 0.0038 lr: 0.02\n",
            "iteration: 297310 loss: 0.0037 lr: 0.02\n",
            "iteration: 297320 loss: 0.0032 lr: 0.02\n",
            "iteration: 297330 loss: 0.0040 lr: 0.02\n",
            "iteration: 297340 loss: 0.0039 lr: 0.02\n",
            "iteration: 297350 loss: 0.0044 lr: 0.02\n",
            "iteration: 297360 loss: 0.0036 lr: 0.02\n",
            "iteration: 297370 loss: 0.0038 lr: 0.02\n",
            "iteration: 297380 loss: 0.0033 lr: 0.02\n",
            "iteration: 297390 loss: 0.0037 lr: 0.02\n",
            "iteration: 297400 loss: 0.0032 lr: 0.02\n",
            "iteration: 297410 loss: 0.0033 lr: 0.02\n",
            "iteration: 297420 loss: 0.0038 lr: 0.02\n",
            "iteration: 297430 loss: 0.0040 lr: 0.02\n",
            "iteration: 297440 loss: 0.0027 lr: 0.02\n",
            "iteration: 297450 loss: 0.0035 lr: 0.02\n",
            "iteration: 297460 loss: 0.0036 lr: 0.02\n",
            "iteration: 297470 loss: 0.0038 lr: 0.02\n",
            "iteration: 297480 loss: 0.0029 lr: 0.02\n",
            "iteration: 297490 loss: 0.0031 lr: 0.02\n",
            "iteration: 297500 loss: 0.0035 lr: 0.02\n",
            "iteration: 297510 loss: 0.0031 lr: 0.02\n",
            "iteration: 297520 loss: 0.0032 lr: 0.02\n",
            "iteration: 297530 loss: 0.0043 lr: 0.02\n",
            "iteration: 297540 loss: 0.0030 lr: 0.02\n",
            "iteration: 297550 loss: 0.0029 lr: 0.02\n",
            "iteration: 297560 loss: 0.0029 lr: 0.02\n",
            "iteration: 297570 loss: 0.0038 lr: 0.02\n",
            "iteration: 297580 loss: 0.0038 lr: 0.02\n",
            "iteration: 297590 loss: 0.0036 lr: 0.02\n",
            "iteration: 297600 loss: 0.0037 lr: 0.02\n",
            "iteration: 297610 loss: 0.0036 lr: 0.02\n",
            "iteration: 297620 loss: 0.0051 lr: 0.02\n",
            "iteration: 297630 loss: 0.0027 lr: 0.02\n",
            "iteration: 297640 loss: 0.0046 lr: 0.02\n",
            "iteration: 297650 loss: 0.0038 lr: 0.02\n",
            "iteration: 297660 loss: 0.0031 lr: 0.02\n",
            "iteration: 297670 loss: 0.0038 lr: 0.02\n",
            "iteration: 297680 loss: 0.0040 lr: 0.02\n",
            "iteration: 297690 loss: 0.0041 lr: 0.02\n",
            "iteration: 297700 loss: 0.0037 lr: 0.02\n",
            "iteration: 297710 loss: 0.0043 lr: 0.02\n",
            "iteration: 297720 loss: 0.0028 lr: 0.02\n",
            "iteration: 297730 loss: 0.0029 lr: 0.02\n",
            "iteration: 297740 loss: 0.0051 lr: 0.02\n",
            "iteration: 297750 loss: 0.0034 lr: 0.02\n",
            "iteration: 297760 loss: 0.0045 lr: 0.02\n",
            "iteration: 297770 loss: 0.0045 lr: 0.02\n",
            "iteration: 297780 loss: 0.0037 lr: 0.02\n",
            "iteration: 297790 loss: 0.0031 lr: 0.02\n",
            "iteration: 297800 loss: 0.0027 lr: 0.02\n",
            "iteration: 297810 loss: 0.0037 lr: 0.02\n",
            "iteration: 297820 loss: 0.0037 lr: 0.02\n",
            "iteration: 297830 loss: 0.0033 lr: 0.02\n",
            "iteration: 297840 loss: 0.0033 lr: 0.02\n",
            "iteration: 297850 loss: 0.0041 lr: 0.02\n",
            "iteration: 297860 loss: 0.0034 lr: 0.02\n",
            "iteration: 297870 loss: 0.0037 lr: 0.02\n",
            "iteration: 297880 loss: 0.0038 lr: 0.02\n",
            "iteration: 297890 loss: 0.0041 lr: 0.02\n",
            "iteration: 297900 loss: 0.0034 lr: 0.02\n",
            "iteration: 297910 loss: 0.0040 lr: 0.02\n",
            "iteration: 297920 loss: 0.0044 lr: 0.02\n",
            "iteration: 297930 loss: 0.0030 lr: 0.02\n",
            "iteration: 297940 loss: 0.0047 lr: 0.02\n",
            "iteration: 297950 loss: 0.0027 lr: 0.02\n",
            "iteration: 297960 loss: 0.0034 lr: 0.02\n",
            "iteration: 297970 loss: 0.0022 lr: 0.02\n",
            "iteration: 297980 loss: 0.0031 lr: 0.02\n",
            "iteration: 297990 loss: 0.0031 lr: 0.02\n",
            "iteration: 298000 loss: 0.0029 lr: 0.02\n",
            "iteration: 298010 loss: 0.0032 lr: 0.02\n",
            "iteration: 298020 loss: 0.0031 lr: 0.02\n",
            "iteration: 298030 loss: 0.0031 lr: 0.02\n",
            "iteration: 298040 loss: 0.0037 lr: 0.02\n",
            "iteration: 298050 loss: 0.0043 lr: 0.02\n",
            "iteration: 298060 loss: 0.0026 lr: 0.02\n",
            "iteration: 298070 loss: 0.0035 lr: 0.02\n",
            "iteration: 298080 loss: 0.0036 lr: 0.02\n",
            "iteration: 298090 loss: 0.0035 lr: 0.02\n",
            "iteration: 298100 loss: 0.0046 lr: 0.02\n",
            "iteration: 298110 loss: 0.0039 lr: 0.02\n",
            "iteration: 298120 loss: 0.0041 lr: 0.02\n",
            "iteration: 298130 loss: 0.0036 lr: 0.02\n",
            "iteration: 298140 loss: 0.0034 lr: 0.02\n",
            "iteration: 298150 loss: 0.0034 lr: 0.02\n",
            "iteration: 298160 loss: 0.0039 lr: 0.02\n",
            "iteration: 298170 loss: 0.0037 lr: 0.02\n",
            "iteration: 298180 loss: 0.0033 lr: 0.02\n",
            "iteration: 298190 loss: 0.0043 lr: 0.02\n",
            "iteration: 298200 loss: 0.0036 lr: 0.02\n",
            "iteration: 298210 loss: 0.0040 lr: 0.02\n",
            "iteration: 298220 loss: 0.0030 lr: 0.02\n",
            "iteration: 298230 loss: 0.0037 lr: 0.02\n",
            "iteration: 298240 loss: 0.0035 lr: 0.02\n",
            "iteration: 298250 loss: 0.0053 lr: 0.02\n",
            "iteration: 298260 loss: 0.0037 lr: 0.02\n",
            "iteration: 298270 loss: 0.0046 lr: 0.02\n",
            "iteration: 298280 loss: 0.0035 lr: 0.02\n",
            "iteration: 298290 loss: 0.0043 lr: 0.02\n",
            "iteration: 298300 loss: 0.0034 lr: 0.02\n",
            "iteration: 298310 loss: 0.0044 lr: 0.02\n",
            "iteration: 298320 loss: 0.0031 lr: 0.02\n",
            "iteration: 298330 loss: 0.0033 lr: 0.02\n",
            "iteration: 298340 loss: 0.0031 lr: 0.02\n",
            "iteration: 298350 loss: 0.0041 lr: 0.02\n",
            "iteration: 298360 loss: 0.0038 lr: 0.02\n",
            "iteration: 298370 loss: 0.0035 lr: 0.02\n",
            "iteration: 298380 loss: 0.0029 lr: 0.02\n",
            "iteration: 298390 loss: 0.0039 lr: 0.02\n",
            "iteration: 298400 loss: 0.0039 lr: 0.02\n",
            "iteration: 298410 loss: 0.0044 lr: 0.02\n",
            "iteration: 298420 loss: 0.0023 lr: 0.02\n",
            "iteration: 298430 loss: 0.0033 lr: 0.02\n",
            "iteration: 298440 loss: 0.0043 lr: 0.02\n",
            "iteration: 298450 loss: 0.0043 lr: 0.02\n",
            "iteration: 298460 loss: 0.0030 lr: 0.02\n",
            "iteration: 298470 loss: 0.0036 lr: 0.02\n",
            "iteration: 298480 loss: 0.0039 lr: 0.02\n",
            "iteration: 298490 loss: 0.0036 lr: 0.02\n",
            "iteration: 298500 loss: 0.0029 lr: 0.02\n",
            "iteration: 298510 loss: 0.0032 lr: 0.02\n",
            "iteration: 298520 loss: 0.0049 lr: 0.02\n",
            "iteration: 298530 loss: 0.0034 lr: 0.02\n",
            "iteration: 298540 loss: 0.0027 lr: 0.02\n",
            "iteration: 298550 loss: 0.0034 lr: 0.02\n",
            "iteration: 298560 loss: 0.0035 lr: 0.02\n",
            "iteration: 298570 loss: 0.0034 lr: 0.02\n",
            "iteration: 298580 loss: 0.0030 lr: 0.02\n",
            "iteration: 298590 loss: 0.0029 lr: 0.02\n",
            "iteration: 298600 loss: 0.0032 lr: 0.02\n",
            "iteration: 298610 loss: 0.0034 lr: 0.02\n",
            "iteration: 298620 loss: 0.0035 lr: 0.02\n",
            "iteration: 298630 loss: 0.0031 lr: 0.02\n",
            "iteration: 298640 loss: 0.0038 lr: 0.02\n",
            "iteration: 298650 loss: 0.0028 lr: 0.02\n",
            "iteration: 298660 loss: 0.0025 lr: 0.02\n",
            "iteration: 298670 loss: 0.0053 lr: 0.02\n",
            "iteration: 298680 loss: 0.0034 lr: 0.02\n",
            "iteration: 298690 loss: 0.0044 lr: 0.02\n",
            "iteration: 298700 loss: 0.0044 lr: 0.02\n",
            "iteration: 298710 loss: 0.0033 lr: 0.02\n",
            "iteration: 298720 loss: 0.0036 lr: 0.02\n",
            "iteration: 298730 loss: 0.0030 lr: 0.02\n",
            "iteration: 298740 loss: 0.0031 lr: 0.02\n",
            "iteration: 298750 loss: 0.0026 lr: 0.02\n",
            "iteration: 298760 loss: 0.0028 lr: 0.02\n",
            "iteration: 298770 loss: 0.0025 lr: 0.02\n",
            "iteration: 298780 loss: 0.0023 lr: 0.02\n",
            "iteration: 298790 loss: 0.0053 lr: 0.02\n",
            "iteration: 298800 loss: 0.0049 lr: 0.02\n",
            "iteration: 298810 loss: 0.0038 lr: 0.02\n",
            "iteration: 298820 loss: 0.0025 lr: 0.02\n",
            "iteration: 298830 loss: 0.0035 lr: 0.02\n",
            "iteration: 298840 loss: 0.0041 lr: 0.02\n",
            "iteration: 298850 loss: 0.0028 lr: 0.02\n",
            "iteration: 298860 loss: 0.0032 lr: 0.02\n",
            "iteration: 298870 loss: 0.0045 lr: 0.02\n",
            "iteration: 298880 loss: 0.0029 lr: 0.02\n",
            "iteration: 298890 loss: 0.0032 lr: 0.02\n",
            "iteration: 298900 loss: 0.0032 lr: 0.02\n",
            "iteration: 298910 loss: 0.0031 lr: 0.02\n",
            "iteration: 298920 loss: 0.0031 lr: 0.02\n",
            "iteration: 298930 loss: 0.0044 lr: 0.02\n",
            "iteration: 298940 loss: 0.0022 lr: 0.02\n",
            "iteration: 298950 loss: 0.0047 lr: 0.02\n",
            "iteration: 298960 loss: 0.0035 lr: 0.02\n",
            "iteration: 298970 loss: 0.0038 lr: 0.02\n",
            "iteration: 298980 loss: 0.0029 lr: 0.02\n",
            "iteration: 298990 loss: 0.0032 lr: 0.02\n",
            "iteration: 299000 loss: 0.0047 lr: 0.02\n",
            "iteration: 299010 loss: 0.0041 lr: 0.02\n",
            "iteration: 299020 loss: 0.0044 lr: 0.02\n",
            "iteration: 299030 loss: 0.0036 lr: 0.02\n",
            "iteration: 299040 loss: 0.0034 lr: 0.02\n",
            "iteration: 299050 loss: 0.0031 lr: 0.02\n",
            "iteration: 299060 loss: 0.0027 lr: 0.02\n",
            "iteration: 299070 loss: 0.0035 lr: 0.02\n",
            "iteration: 299080 loss: 0.0028 lr: 0.02\n",
            "iteration: 299090 loss: 0.0040 lr: 0.02\n",
            "iteration: 299100 loss: 0.0037 lr: 0.02\n",
            "iteration: 299110 loss: 0.0051 lr: 0.02\n",
            "iteration: 299120 loss: 0.0030 lr: 0.02\n",
            "iteration: 299130 loss: 0.0034 lr: 0.02\n",
            "iteration: 299140 loss: 0.0034 lr: 0.02\n",
            "iteration: 299150 loss: 0.0034 lr: 0.02\n",
            "iteration: 299160 loss: 0.0034 lr: 0.02\n",
            "iteration: 299170 loss: 0.0026 lr: 0.02\n",
            "iteration: 299180 loss: 0.0028 lr: 0.02\n",
            "iteration: 299190 loss: 0.0038 lr: 0.02\n",
            "iteration: 299200 loss: 0.0049 lr: 0.02\n",
            "iteration: 299210 loss: 0.0036 lr: 0.02\n",
            "iteration: 299220 loss: 0.0039 lr: 0.02\n",
            "iteration: 299230 loss: 0.0024 lr: 0.02\n",
            "iteration: 299240 loss: 0.0034 lr: 0.02\n",
            "iteration: 299250 loss: 0.0042 lr: 0.02\n",
            "iteration: 299260 loss: 0.0034 lr: 0.02\n",
            "iteration: 299270 loss: 0.0028 lr: 0.02\n",
            "iteration: 299280 loss: 0.0031 lr: 0.02\n",
            "iteration: 299290 loss: 0.0029 lr: 0.02\n",
            "iteration: 299300 loss: 0.0036 lr: 0.02\n",
            "iteration: 299310 loss: 0.0025 lr: 0.02\n",
            "iteration: 299320 loss: 0.0028 lr: 0.02\n",
            "iteration: 299330 loss: 0.0034 lr: 0.02\n",
            "iteration: 299340 loss: 0.0025 lr: 0.02\n",
            "iteration: 299350 loss: 0.0038 lr: 0.02\n",
            "iteration: 299360 loss: 0.0030 lr: 0.02\n",
            "iteration: 299370 loss: 0.0028 lr: 0.02\n",
            "iteration: 299380 loss: 0.0048 lr: 0.02\n",
            "iteration: 299390 loss: 0.0041 lr: 0.02\n",
            "iteration: 299400 loss: 0.0036 lr: 0.02\n",
            "iteration: 299410 loss: 0.0036 lr: 0.02\n",
            "iteration: 299420 loss: 0.0037 lr: 0.02\n",
            "iteration: 299430 loss: 0.0045 lr: 0.02\n",
            "iteration: 299440 loss: 0.0039 lr: 0.02\n",
            "iteration: 299450 loss: 0.0045 lr: 0.02\n",
            "iteration: 299460 loss: 0.0027 lr: 0.02\n",
            "iteration: 299470 loss: 0.0033 lr: 0.02\n",
            "iteration: 299480 loss: 0.0034 lr: 0.02\n",
            "iteration: 299490 loss: 0.0041 lr: 0.02\n",
            "iteration: 299500 loss: 0.0031 lr: 0.02\n",
            "iteration: 299510 loss: 0.0042 lr: 0.02\n",
            "iteration: 299520 loss: 0.0039 lr: 0.02\n",
            "iteration: 299530 loss: 0.0037 lr: 0.02\n",
            "iteration: 299540 loss: 0.0035 lr: 0.02\n",
            "iteration: 299550 loss: 0.0029 lr: 0.02\n",
            "iteration: 299560 loss: 0.0038 lr: 0.02\n",
            "iteration: 299570 loss: 0.0039 lr: 0.02\n",
            "iteration: 299580 loss: 0.0035 lr: 0.02\n",
            "iteration: 299590 loss: 0.0033 lr: 0.02\n",
            "iteration: 299600 loss: 0.0031 lr: 0.02\n",
            "iteration: 299610 loss: 0.0033 lr: 0.02\n",
            "iteration: 299620 loss: 0.0039 lr: 0.02\n",
            "iteration: 299630 loss: 0.0034 lr: 0.02\n",
            "iteration: 299640 loss: 0.0017 lr: 0.02\n",
            "iteration: 299650 loss: 0.0034 lr: 0.02\n",
            "iteration: 299660 loss: 0.0025 lr: 0.02\n",
            "iteration: 299670 loss: 0.0034 lr: 0.02\n",
            "iteration: 299680 loss: 0.0027 lr: 0.02\n",
            "iteration: 299690 loss: 0.0031 lr: 0.02\n",
            "iteration: 299700 loss: 0.0030 lr: 0.02\n",
            "iteration: 299710 loss: 0.0046 lr: 0.02\n",
            "iteration: 299720 loss: 0.0033 lr: 0.02\n",
            "iteration: 299730 loss: 0.0034 lr: 0.02\n",
            "iteration: 299740 loss: 0.0036 lr: 0.02\n",
            "iteration: 299750 loss: 0.0027 lr: 0.02\n",
            "iteration: 299760 loss: 0.0037 lr: 0.02\n",
            "iteration: 299770 loss: 0.0036 lr: 0.02\n",
            "iteration: 299780 loss: 0.0039 lr: 0.02\n",
            "iteration: 299790 loss: 0.0033 lr: 0.02\n",
            "iteration: 299800 loss: 0.0040 lr: 0.02\n",
            "iteration: 299810 loss: 0.0034 lr: 0.02\n",
            "iteration: 299820 loss: 0.0034 lr: 0.02\n",
            "iteration: 299830 loss: 0.0049 lr: 0.02\n",
            "iteration: 299840 loss: 0.0027 lr: 0.02\n",
            "iteration: 299850 loss: 0.0032 lr: 0.02\n",
            "iteration: 299860 loss: 0.0028 lr: 0.02\n",
            "iteration: 299870 loss: 0.0034 lr: 0.02\n",
            "iteration: 299880 loss: 0.0034 lr: 0.02\n",
            "iteration: 299890 loss: 0.0031 lr: 0.02\n",
            "iteration: 299900 loss: 0.0035 lr: 0.02\n",
            "iteration: 299910 loss: 0.0035 lr: 0.02\n",
            "iteration: 299920 loss: 0.0040 lr: 0.02\n",
            "iteration: 299930 loss: 0.0032 lr: 0.02\n",
            "iteration: 299940 loss: 0.0034 lr: 0.02\n",
            "iteration: 299950 loss: 0.0030 lr: 0.02\n",
            "iteration: 299960 loss: 0.0030 lr: 0.02\n",
            "iteration: 299970 loss: 0.0029 lr: 0.02\n",
            "iteration: 299980 loss: 0.0034 lr: 0.02\n",
            "iteration: 299990 loss: 0.0034 lr: 0.02\n",
            "iteration: 300000 loss: 0.0034 lr: 0.02\n",
            "iteration: 300010 loss: 0.0042 lr: 0.02\n",
            "iteration: 300020 loss: 0.0032 lr: 0.02\n",
            "iteration: 300030 loss: 0.0030 lr: 0.02\n",
            "iteration: 300040 loss: 0.0039 lr: 0.02\n",
            "iteration: 300050 loss: 0.0034 lr: 0.02\n",
            "iteration: 300060 loss: 0.0032 lr: 0.02\n",
            "iteration: 300070 loss: 0.0034 lr: 0.02\n",
            "iteration: 300080 loss: 0.0029 lr: 0.02\n",
            "iteration: 300090 loss: 0.0039 lr: 0.02\n",
            "iteration: 300100 loss: 0.0032 lr: 0.02\n",
            "iteration: 300110 loss: 0.0044 lr: 0.02\n",
            "iteration: 300120 loss: 0.0033 lr: 0.02\n",
            "iteration: 300130 loss: 0.0037 lr: 0.02\n",
            "iteration: 300140 loss: 0.0029 lr: 0.02\n",
            "iteration: 300150 loss: 0.0039 lr: 0.02\n",
            "iteration: 300160 loss: 0.0040 lr: 0.02\n",
            "iteration: 300170 loss: 0.0038 lr: 0.02\n",
            "iteration: 300180 loss: 0.0035 lr: 0.02\n",
            "iteration: 300190 loss: 0.0030 lr: 0.02\n",
            "iteration: 300200 loss: 0.0034 lr: 0.02\n",
            "iteration: 300210 loss: 0.0040 lr: 0.02\n",
            "iteration: 300220 loss: 0.0047 lr: 0.02\n",
            "iteration: 300230 loss: 0.0036 lr: 0.02\n",
            "iteration: 300240 loss: 0.0040 lr: 0.02\n",
            "iteration: 300250 loss: 0.0032 lr: 0.02\n",
            "iteration: 300260 loss: 0.0030 lr: 0.02\n",
            "iteration: 300270 loss: 0.0023 lr: 0.02\n",
            "iteration: 300280 loss: 0.0037 lr: 0.02\n",
            "iteration: 300290 loss: 0.0033 lr: 0.02\n",
            "iteration: 300300 loss: 0.0048 lr: 0.02\n",
            "iteration: 300310 loss: 0.0040 lr: 0.02\n",
            "iteration: 300320 loss: 0.0026 lr: 0.02\n",
            "iteration: 300330 loss: 0.0028 lr: 0.02\n",
            "iteration: 300340 loss: 0.0034 lr: 0.02\n",
            "iteration: 300350 loss: 0.0038 lr: 0.02\n",
            "iteration: 300360 loss: 0.0040 lr: 0.02\n",
            "iteration: 300370 loss: 0.0032 lr: 0.02\n",
            "iteration: 300380 loss: 0.0030 lr: 0.02\n",
            "iteration: 300390 loss: 0.0034 lr: 0.02\n",
            "iteration: 300400 loss: 0.0034 lr: 0.02\n",
            "iteration: 300410 loss: 0.0033 lr: 0.02\n",
            "iteration: 300420 loss: 0.0032 lr: 0.02\n",
            "iteration: 300430 loss: 0.0037 lr: 0.02\n",
            "iteration: 300440 loss: 0.0031 lr: 0.02\n",
            "iteration: 300450 loss: 0.0037 lr: 0.02\n",
            "iteration: 300460 loss: 0.0050 lr: 0.02\n",
            "iteration: 300470 loss: 0.0034 lr: 0.02\n",
            "iteration: 300480 loss: 0.0034 lr: 0.02\n",
            "iteration: 300490 loss: 0.0030 lr: 0.02\n",
            "iteration: 300500 loss: 0.0043 lr: 0.02\n",
            "iteration: 300510 loss: 0.0031 lr: 0.02\n",
            "iteration: 300520 loss: 0.0036 lr: 0.02\n",
            "iteration: 300530 loss: 0.0032 lr: 0.02\n",
            "iteration: 300540 loss: 0.0034 lr: 0.02\n",
            "iteration: 300550 loss: 0.0034 lr: 0.02\n",
            "iteration: 300560 loss: 0.0042 lr: 0.02\n",
            "iteration: 300570 loss: 0.0027 lr: 0.02\n",
            "iteration: 300580 loss: 0.0039 lr: 0.02\n",
            "iteration: 300590 loss: 0.0032 lr: 0.02\n",
            "iteration: 300600 loss: 0.0036 lr: 0.02\n",
            "iteration: 300610 loss: 0.0040 lr: 0.02\n",
            "iteration: 300620 loss: 0.0031 lr: 0.02\n",
            "iteration: 300630 loss: 0.0039 lr: 0.02\n",
            "iteration: 300640 loss: 0.0026 lr: 0.02\n",
            "iteration: 300650 loss: 0.0037 lr: 0.02\n",
            "iteration: 300660 loss: 0.0031 lr: 0.02\n",
            "iteration: 300670 loss: 0.0027 lr: 0.02\n",
            "iteration: 300680 loss: 0.0032 lr: 0.02\n",
            "iteration: 300690 loss: 0.0047 lr: 0.02\n",
            "iteration: 300700 loss: 0.0043 lr: 0.02\n",
            "iteration: 300710 loss: 0.0027 lr: 0.02\n",
            "iteration: 300720 loss: 0.0033 lr: 0.02\n",
            "iteration: 300730 loss: 0.0025 lr: 0.02\n",
            "iteration: 300740 loss: 0.0050 lr: 0.02\n",
            "iteration: 300750 loss: 0.0024 lr: 0.02\n",
            "iteration: 300760 loss: 0.0033 lr: 0.02\n",
            "iteration: 300770 loss: 0.0031 lr: 0.02\n",
            "iteration: 300780 loss: 0.0033 lr: 0.02\n",
            "iteration: 300790 loss: 0.0034 lr: 0.02\n",
            "iteration: 300800 loss: 0.0043 lr: 0.02\n",
            "iteration: 300810 loss: 0.0036 lr: 0.02\n",
            "iteration: 300820 loss: 0.0041 lr: 0.02\n",
            "iteration: 300830 loss: 0.0034 lr: 0.02\n",
            "iteration: 300840 loss: 0.0039 lr: 0.02\n",
            "iteration: 300850 loss: 0.0035 lr: 0.02\n",
            "iteration: 300860 loss: 0.0025 lr: 0.02\n",
            "iteration: 300870 loss: 0.0045 lr: 0.02\n",
            "iteration: 300880 loss: 0.0029 lr: 0.02\n",
            "iteration: 300890 loss: 0.0034 lr: 0.02\n",
            "iteration: 300900 loss: 0.0038 lr: 0.02\n",
            "iteration: 300910 loss: 0.0039 lr: 0.02\n",
            "iteration: 300920 loss: 0.0032 lr: 0.02\n",
            "iteration: 300930 loss: 0.0033 lr: 0.02\n",
            "iteration: 300940 loss: 0.0033 lr: 0.02\n",
            "iteration: 300950 loss: 0.0034 lr: 0.02\n",
            "iteration: 300960 loss: 0.0019 lr: 0.02\n",
            "iteration: 300970 loss: 0.0037 lr: 0.02\n",
            "iteration: 300980 loss: 0.0028 lr: 0.02\n",
            "iteration: 300990 loss: 0.0040 lr: 0.02\n",
            "iteration: 301000 loss: 0.0045 lr: 0.02\n",
            "iteration: 301010 loss: 0.0037 lr: 0.02\n",
            "iteration: 301020 loss: 0.0040 lr: 0.02\n",
            "iteration: 301030 loss: 0.0028 lr: 0.02\n",
            "iteration: 301040 loss: 0.0032 lr: 0.02\n",
            "iteration: 301050 loss: 0.0042 lr: 0.02\n",
            "iteration: 301060 loss: 0.0030 lr: 0.02\n",
            "iteration: 301070 loss: 0.0042 lr: 0.02\n",
            "iteration: 301080 loss: 0.0045 lr: 0.02\n",
            "iteration: 301090 loss: 0.0034 lr: 0.02\n",
            "iteration: 301100 loss: 0.0034 lr: 0.02\n",
            "iteration: 301110 loss: 0.0043 lr: 0.02\n",
            "iteration: 301120 loss: 0.0031 lr: 0.02\n",
            "iteration: 301130 loss: 0.0039 lr: 0.02\n",
            "iteration: 301140 loss: 0.0028 lr: 0.02\n",
            "iteration: 301150 loss: 0.0029 lr: 0.02\n",
            "iteration: 301160 loss: 0.0030 lr: 0.02\n",
            "iteration: 301170 loss: 0.0051 lr: 0.02\n",
            "iteration: 301180 loss: 0.0037 lr: 0.02\n",
            "iteration: 301190 loss: 0.0039 lr: 0.02\n",
            "iteration: 301200 loss: 0.0033 lr: 0.02\n",
            "iteration: 301210 loss: 0.0041 lr: 0.02\n",
            "iteration: 301220 loss: 0.0034 lr: 0.02\n",
            "iteration: 301230 loss: 0.0027 lr: 0.02\n",
            "iteration: 301240 loss: 0.0026 lr: 0.02\n",
            "iteration: 301250 loss: 0.0030 lr: 0.02\n",
            "iteration: 301260 loss: 0.0037 lr: 0.02\n",
            "iteration: 301270 loss: 0.0031 lr: 0.02\n",
            "iteration: 301280 loss: 0.0034 lr: 0.02\n",
            "iteration: 301290 loss: 0.0036 lr: 0.02\n",
            "iteration: 301300 loss: 0.0031 lr: 0.02\n",
            "iteration: 301310 loss: 0.0034 lr: 0.02\n",
            "iteration: 301320 loss: 0.0025 lr: 0.02\n",
            "iteration: 301330 loss: 0.0027 lr: 0.02\n",
            "iteration: 301340 loss: 0.0031 lr: 0.02\n",
            "iteration: 301350 loss: 0.0035 lr: 0.02\n",
            "iteration: 301360 loss: 0.0046 lr: 0.02\n",
            "iteration: 301370 loss: 0.0032 lr: 0.02\n",
            "iteration: 301380 loss: 0.0037 lr: 0.02\n",
            "iteration: 301390 loss: 0.0030 lr: 0.02\n",
            "iteration: 301400 loss: 0.0034 lr: 0.02\n",
            "iteration: 301410 loss: 0.0043 lr: 0.02\n",
            "iteration: 301420 loss: 0.0055 lr: 0.02\n",
            "iteration: 301430 loss: 0.0037 lr: 0.02\n",
            "iteration: 301440 loss: 0.0041 lr: 0.02\n",
            "iteration: 301450 loss: 0.0035 lr: 0.02\n",
            "iteration: 301460 loss: 0.0043 lr: 0.02\n",
            "iteration: 301470 loss: 0.0036 lr: 0.02\n",
            "iteration: 301480 loss: 0.0029 lr: 0.02\n",
            "iteration: 301490 loss: 0.0043 lr: 0.02\n",
            "iteration: 301500 loss: 0.0033 lr: 0.02\n",
            "iteration: 301510 loss: 0.0041 lr: 0.02\n",
            "iteration: 301520 loss: 0.0036 lr: 0.02\n",
            "iteration: 301530 loss: 0.0032 lr: 0.02\n",
            "iteration: 301540 loss: 0.0036 lr: 0.02\n",
            "iteration: 301550 loss: 0.0034 lr: 0.02\n",
            "iteration: 301560 loss: 0.0038 lr: 0.02\n",
            "iteration: 301570 loss: 0.0038 lr: 0.02\n",
            "iteration: 301580 loss: 0.0032 lr: 0.02\n",
            "iteration: 301590 loss: 0.0032 lr: 0.02\n",
            "iteration: 301600 loss: 0.0035 lr: 0.02\n",
            "iteration: 301610 loss: 0.0027 lr: 0.02\n",
            "iteration: 301620 loss: 0.0032 lr: 0.02\n",
            "iteration: 301630 loss: 0.0037 lr: 0.02\n",
            "iteration: 301640 loss: 0.0038 lr: 0.02\n",
            "iteration: 301650 loss: 0.0043 lr: 0.02\n",
            "iteration: 301660 loss: 0.0033 lr: 0.02\n",
            "iteration: 301670 loss: 0.0045 lr: 0.02\n",
            "iteration: 301680 loss: 0.0041 lr: 0.02\n",
            "iteration: 301690 loss: 0.0045 lr: 0.02\n",
            "iteration: 301700 loss: 0.0030 lr: 0.02\n",
            "iteration: 301710 loss: 0.0033 lr: 0.02\n",
            "iteration: 301720 loss: 0.0035 lr: 0.02\n",
            "iteration: 301730 loss: 0.0034 lr: 0.02\n",
            "iteration: 301740 loss: 0.0036 lr: 0.02\n",
            "iteration: 301750 loss: 0.0028 lr: 0.02\n",
            "iteration: 301760 loss: 0.0039 lr: 0.02\n",
            "iteration: 301770 loss: 0.0035 lr: 0.02\n",
            "iteration: 301780 loss: 0.0037 lr: 0.02\n",
            "iteration: 301790 loss: 0.0029 lr: 0.02\n",
            "iteration: 301800 loss: 0.0042 lr: 0.02\n",
            "iteration: 301810 loss: 0.0034 lr: 0.02\n",
            "iteration: 301820 loss: 0.0032 lr: 0.02\n",
            "iteration: 301830 loss: 0.0046 lr: 0.02\n",
            "iteration: 301840 loss: 0.0034 lr: 0.02\n",
            "iteration: 301850 loss: 0.0031 lr: 0.02\n",
            "iteration: 301860 loss: 0.0040 lr: 0.02\n",
            "iteration: 301870 loss: 0.0033 lr: 0.02\n",
            "iteration: 301880 loss: 0.0042 lr: 0.02\n",
            "iteration: 301890 loss: 0.0041 lr: 0.02\n",
            "iteration: 301900 loss: 0.0038 lr: 0.02\n",
            "iteration: 301910 loss: 0.0040 lr: 0.02\n",
            "iteration: 301920 loss: 0.0030 lr: 0.02\n",
            "iteration: 301930 loss: 0.0026 lr: 0.02\n",
            "iteration: 301940 loss: 0.0029 lr: 0.02\n",
            "iteration: 301950 loss: 0.0035 lr: 0.02\n",
            "iteration: 301960 loss: 0.0041 lr: 0.02\n",
            "iteration: 301970 loss: 0.0039 lr: 0.02\n",
            "iteration: 301980 loss: 0.0033 lr: 0.02\n",
            "iteration: 301990 loss: 0.0028 lr: 0.02\n",
            "iteration: 302000 loss: 0.0040 lr: 0.02\n",
            "iteration: 302010 loss: 0.0044 lr: 0.02\n",
            "iteration: 302020 loss: 0.0034 lr: 0.02\n",
            "iteration: 302030 loss: 0.0031 lr: 0.02\n",
            "iteration: 302040 loss: 0.0043 lr: 0.02\n",
            "iteration: 302050 loss: 0.0024 lr: 0.02\n",
            "iteration: 302060 loss: 0.0042 lr: 0.02\n",
            "iteration: 302070 loss: 0.0048 lr: 0.02\n",
            "iteration: 302080 loss: 0.0046 lr: 0.02\n",
            "iteration: 302090 loss: 0.0037 lr: 0.02\n",
            "iteration: 302100 loss: 0.0025 lr: 0.02\n",
            "iteration: 302110 loss: 0.0045 lr: 0.02\n",
            "iteration: 302120 loss: 0.0041 lr: 0.02\n",
            "iteration: 302130 loss: 0.0036 lr: 0.02\n",
            "iteration: 302140 loss: 0.0032 lr: 0.02\n",
            "iteration: 302150 loss: 0.0038 lr: 0.02\n",
            "iteration: 302160 loss: 0.0035 lr: 0.02\n",
            "iteration: 302170 loss: 0.0036 lr: 0.02\n",
            "iteration: 302180 loss: 0.0043 lr: 0.02\n",
            "iteration: 302190 loss: 0.0046 lr: 0.02\n",
            "iteration: 302200 loss: 0.0039 lr: 0.02\n",
            "iteration: 302210 loss: 0.0031 lr: 0.02\n",
            "iteration: 302220 loss: 0.0035 lr: 0.02\n",
            "iteration: 302230 loss: 0.0035 lr: 0.02\n",
            "iteration: 302240 loss: 0.0039 lr: 0.02\n",
            "iteration: 302250 loss: 0.0054 lr: 0.02\n",
            "iteration: 302260 loss: 0.0047 lr: 0.02\n",
            "iteration: 302270 loss: 0.0038 lr: 0.02\n",
            "iteration: 302280 loss: 0.0032 lr: 0.02\n",
            "iteration: 302290 loss: 0.0034 lr: 0.02\n",
            "iteration: 302300 loss: 0.0032 lr: 0.02\n",
            "iteration: 302310 loss: 0.0030 lr: 0.02\n",
            "iteration: 302320 loss: 0.0038 lr: 0.02\n",
            "iteration: 302330 loss: 0.0029 lr: 0.02\n",
            "iteration: 302340 loss: 0.0034 lr: 0.02\n",
            "iteration: 302350 loss: 0.0027 lr: 0.02\n",
            "iteration: 302360 loss: 0.0032 lr: 0.02\n",
            "iteration: 302370 loss: 0.0045 lr: 0.02\n",
            "iteration: 302380 loss: 0.0038 lr: 0.02\n",
            "iteration: 302390 loss: 0.0035 lr: 0.02\n",
            "iteration: 302400 loss: 0.0030 lr: 0.02\n",
            "iteration: 302410 loss: 0.0045 lr: 0.02\n",
            "iteration: 302420 loss: 0.0042 lr: 0.02\n",
            "iteration: 302430 loss: 0.0034 lr: 0.02\n",
            "iteration: 302440 loss: 0.0032 lr: 0.02\n",
            "iteration: 302450 loss: 0.0039 lr: 0.02\n",
            "iteration: 302460 loss: 0.0036 lr: 0.02\n",
            "iteration: 302470 loss: 0.0039 lr: 0.02\n",
            "iteration: 302480 loss: 0.0034 lr: 0.02\n",
            "iteration: 302490 loss: 0.0030 lr: 0.02\n",
            "iteration: 302500 loss: 0.0029 lr: 0.02\n",
            "iteration: 302510 loss: 0.0043 lr: 0.02\n",
            "iteration: 302520 loss: 0.0041 lr: 0.02\n",
            "iteration: 302530 loss: 0.0039 lr: 0.02\n",
            "iteration: 302540 loss: 0.0027 lr: 0.02\n",
            "iteration: 302550 loss: 0.0029 lr: 0.02\n",
            "iteration: 302560 loss: 0.0033 lr: 0.02\n",
            "iteration: 302570 loss: 0.0027 lr: 0.02\n",
            "iteration: 302580 loss: 0.0038 lr: 0.02\n",
            "iteration: 302590 loss: 0.0044 lr: 0.02\n",
            "iteration: 302600 loss: 0.0034 lr: 0.02\n",
            "iteration: 302610 loss: 0.0030 lr: 0.02\n",
            "iteration: 302620 loss: 0.0034 lr: 0.02\n",
            "iteration: 302630 loss: 0.0034 lr: 0.02\n",
            "iteration: 302640 loss: 0.0029 lr: 0.02\n",
            "iteration: 302650 loss: 0.0027 lr: 0.02\n",
            "iteration: 302660 loss: 0.0041 lr: 0.02\n",
            "iteration: 302670 loss: 0.0029 lr: 0.02\n",
            "iteration: 302680 loss: 0.0034 lr: 0.02\n",
            "iteration: 302690 loss: 0.0033 lr: 0.02\n",
            "iteration: 302700 loss: 0.0039 lr: 0.02\n",
            "iteration: 302710 loss: 0.0040 lr: 0.02\n",
            "iteration: 302720 loss: 0.0038 lr: 0.02\n",
            "iteration: 302730 loss: 0.0035 lr: 0.02\n",
            "iteration: 302740 loss: 0.0049 lr: 0.02\n",
            "iteration: 302750 loss: 0.0040 lr: 0.02\n",
            "iteration: 302760 loss: 0.0052 lr: 0.02\n",
            "iteration: 302770 loss: 0.0030 lr: 0.02\n",
            "iteration: 302780 loss: 0.0031 lr: 0.02\n",
            "iteration: 302790 loss: 0.0031 lr: 0.02\n",
            "iteration: 302800 loss: 0.0033 lr: 0.02\n",
            "iteration: 302810 loss: 0.0036 lr: 0.02\n",
            "iteration: 302820 loss: 0.0029 lr: 0.02\n",
            "iteration: 302830 loss: 0.0036 lr: 0.02\n",
            "iteration: 302840 loss: 0.0050 lr: 0.02\n",
            "iteration: 302850 loss: 0.0028 lr: 0.02\n",
            "iteration: 302860 loss: 0.0035 lr: 0.02\n",
            "iteration: 302870 loss: 0.0033 lr: 0.02\n",
            "iteration: 302880 loss: 0.0026 lr: 0.02\n",
            "iteration: 302890 loss: 0.0030 lr: 0.02\n",
            "iteration: 302900 loss: 0.0046 lr: 0.02\n",
            "iteration: 302910 loss: 0.0032 lr: 0.02\n",
            "iteration: 302920 loss: 0.0035 lr: 0.02\n",
            "iteration: 302930 loss: 0.0040 lr: 0.02\n",
            "iteration: 302940 loss: 0.0050 lr: 0.02\n",
            "iteration: 302950 loss: 0.0033 lr: 0.02\n",
            "iteration: 302960 loss: 0.0034 lr: 0.02\n",
            "iteration: 302970 loss: 0.0041 lr: 0.02\n",
            "iteration: 302980 loss: 0.0040 lr: 0.02\n",
            "iteration: 302990 loss: 0.0035 lr: 0.02\n",
            "iteration: 303000 loss: 0.0038 lr: 0.02\n",
            "iteration: 303010 loss: 0.0029 lr: 0.02\n",
            "iteration: 303020 loss: 0.0041 lr: 0.02\n",
            "iteration: 303030 loss: 0.0042 lr: 0.02\n",
            "iteration: 303040 loss: 0.0030 lr: 0.02\n",
            "iteration: 303050 loss: 0.0038 lr: 0.02\n",
            "iteration: 303060 loss: 0.0030 lr: 0.02\n",
            "iteration: 303070 loss: 0.0040 lr: 0.02\n",
            "iteration: 303080 loss: 0.0041 lr: 0.02\n",
            "iteration: 303090 loss: 0.0041 lr: 0.02\n",
            "iteration: 303100 loss: 0.0033 lr: 0.02\n",
            "iteration: 303110 loss: 0.0027 lr: 0.02\n",
            "iteration: 303120 loss: 0.0045 lr: 0.02\n",
            "iteration: 303130 loss: 0.0032 lr: 0.02\n",
            "iteration: 303140 loss: 0.0036 lr: 0.02\n",
            "iteration: 303150 loss: 0.0033 lr: 0.02\n",
            "iteration: 303160 loss: 0.0037 lr: 0.02\n",
            "iteration: 303170 loss: 0.0032 lr: 0.02\n",
            "iteration: 303180 loss: 0.0035 lr: 0.02\n",
            "iteration: 303190 loss: 0.0033 lr: 0.02\n",
            "iteration: 303200 loss: 0.0036 lr: 0.02\n",
            "iteration: 303210 loss: 0.0051 lr: 0.02\n",
            "iteration: 303220 loss: 0.0036 lr: 0.02\n",
            "iteration: 303230 loss: 0.0032 lr: 0.02\n",
            "iteration: 303240 loss: 0.0043 lr: 0.02\n",
            "iteration: 303250 loss: 0.0032 lr: 0.02\n",
            "iteration: 303260 loss: 0.0028 lr: 0.02\n",
            "iteration: 303270 loss: 0.0034 lr: 0.02\n",
            "iteration: 303280 loss: 0.0041 lr: 0.02\n",
            "iteration: 303290 loss: 0.0032 lr: 0.02\n",
            "iteration: 303300 loss: 0.0040 lr: 0.02\n",
            "iteration: 303310 loss: 0.0033 lr: 0.02\n",
            "iteration: 303320 loss: 0.0043 lr: 0.02\n",
            "iteration: 303330 loss: 0.0028 lr: 0.02\n",
            "iteration: 303340 loss: 0.0031 lr: 0.02\n",
            "iteration: 303350 loss: 0.0045 lr: 0.02\n",
            "iteration: 303360 loss: 0.0038 lr: 0.02\n",
            "iteration: 303370 loss: 0.0034 lr: 0.02\n",
            "iteration: 303380 loss: 0.0038 lr: 0.02\n",
            "iteration: 303390 loss: 0.0026 lr: 0.02\n",
            "iteration: 303400 loss: 0.0034 lr: 0.02\n",
            "iteration: 303410 loss: 0.0032 lr: 0.02\n",
            "iteration: 303420 loss: 0.0038 lr: 0.02\n",
            "iteration: 303430 loss: 0.0033 lr: 0.02\n",
            "iteration: 303440 loss: 0.0036 lr: 0.02\n",
            "iteration: 303450 loss: 0.0038 lr: 0.02\n",
            "iteration: 303460 loss: 0.0029 lr: 0.02\n",
            "iteration: 303470 loss: 0.0037 lr: 0.02\n",
            "iteration: 303480 loss: 0.0033 lr: 0.02\n",
            "iteration: 303490 loss: 0.0042 lr: 0.02\n",
            "iteration: 303500 loss: 0.0036 lr: 0.02\n",
            "iteration: 303510 loss: 0.0031 lr: 0.02\n",
            "iteration: 303520 loss: 0.0053 lr: 0.02\n",
            "iteration: 303530 loss: 0.0042 lr: 0.02\n",
            "iteration: 303540 loss: 0.0033 lr: 0.02\n",
            "iteration: 303550 loss: 0.0028 lr: 0.02\n",
            "iteration: 303560 loss: 0.0041 lr: 0.02\n",
            "iteration: 303570 loss: 0.0042 lr: 0.02\n",
            "iteration: 303580 loss: 0.0047 lr: 0.02\n",
            "iteration: 303590 loss: 0.0046 lr: 0.02\n",
            "iteration: 303600 loss: 0.0034 lr: 0.02\n",
            "iteration: 303610 loss: 0.0029 lr: 0.02\n",
            "iteration: 303620 loss: 0.0042 lr: 0.02\n",
            "iteration: 303630 loss: 0.0037 lr: 0.02\n",
            "iteration: 303640 loss: 0.0031 lr: 0.02\n",
            "iteration: 303650 loss: 0.0034 lr: 0.02\n",
            "iteration: 303660 loss: 0.0031 lr: 0.02\n",
            "iteration: 303670 loss: 0.0037 lr: 0.02\n",
            "iteration: 303680 loss: 0.0042 lr: 0.02\n",
            "iteration: 303690 loss: 0.0040 lr: 0.02\n",
            "iteration: 303700 loss: 0.0034 lr: 0.02\n",
            "iteration: 303710 loss: 0.0045 lr: 0.02\n",
            "iteration: 303720 loss: 0.0041 lr: 0.02\n",
            "iteration: 303730 loss: 0.0030 lr: 0.02\n",
            "iteration: 303740 loss: 0.0033 lr: 0.02\n",
            "iteration: 303750 loss: 0.0025 lr: 0.02\n",
            "iteration: 303760 loss: 0.0036 lr: 0.02\n",
            "iteration: 303770 loss: 0.0034 lr: 0.02\n",
            "iteration: 303780 loss: 0.0024 lr: 0.02\n",
            "iteration: 303790 loss: 0.0036 lr: 0.02\n",
            "iteration: 303800 loss: 0.0032 lr: 0.02\n",
            "iteration: 303810 loss: 0.0041 lr: 0.02\n",
            "iteration: 303820 loss: 0.0035 lr: 0.02\n",
            "iteration: 303830 loss: 0.0045 lr: 0.02\n",
            "iteration: 303840 loss: 0.0032 lr: 0.02\n",
            "iteration: 303850 loss: 0.0025 lr: 0.02\n",
            "iteration: 303860 loss: 0.0033 lr: 0.02\n",
            "iteration: 303870 loss: 0.0036 lr: 0.02\n",
            "iteration: 303880 loss: 0.0043 lr: 0.02\n",
            "iteration: 303890 loss: 0.0038 lr: 0.02\n",
            "iteration: 303900 loss: 0.0036 lr: 0.02\n",
            "iteration: 303910 loss: 0.0032 lr: 0.02\n",
            "iteration: 303920 loss: 0.0028 lr: 0.02\n",
            "iteration: 303930 loss: 0.0033 lr: 0.02\n",
            "iteration: 303940 loss: 0.0030 lr: 0.02\n",
            "iteration: 303950 loss: 0.0027 lr: 0.02\n",
            "iteration: 303960 loss: 0.0034 lr: 0.02\n",
            "iteration: 303970 loss: 0.0034 lr: 0.02\n",
            "iteration: 303980 loss: 0.0029 lr: 0.02\n",
            "iteration: 303990 loss: 0.0043 lr: 0.02\n",
            "iteration: 304000 loss: 0.0034 lr: 0.02\n",
            "iteration: 304010 loss: 0.0033 lr: 0.02\n",
            "iteration: 304020 loss: 0.0031 lr: 0.02\n",
            "iteration: 304030 loss: 0.0035 lr: 0.02\n",
            "iteration: 304040 loss: 0.0048 lr: 0.02\n",
            "iteration: 304050 loss: 0.0023 lr: 0.02\n",
            "iteration: 304060 loss: 0.0039 lr: 0.02\n",
            "iteration: 304070 loss: 0.0031 lr: 0.02\n",
            "iteration: 304080 loss: 0.0023 lr: 0.02\n",
            "iteration: 304090 loss: 0.0030 lr: 0.02\n",
            "iteration: 304100 loss: 0.0036 lr: 0.02\n",
            "iteration: 304110 loss: 0.0026 lr: 0.02\n",
            "iteration: 304120 loss: 0.0032 lr: 0.02\n",
            "iteration: 304130 loss: 0.0026 lr: 0.02\n",
            "iteration: 304140 loss: 0.0042 lr: 0.02\n",
            "iteration: 304150 loss: 0.0036 lr: 0.02\n",
            "iteration: 304160 loss: 0.0034 lr: 0.02\n",
            "iteration: 304170 loss: 0.0031 lr: 0.02\n",
            "iteration: 304180 loss: 0.0046 lr: 0.02\n",
            "iteration: 304190 loss: 0.0035 lr: 0.02\n",
            "iteration: 304200 loss: 0.0038 lr: 0.02\n",
            "iteration: 304210 loss: 0.0036 lr: 0.02\n",
            "iteration: 304220 loss: 0.0049 lr: 0.02\n",
            "iteration: 304230 loss: 0.0047 lr: 0.02\n",
            "iteration: 304240 loss: 0.0046 lr: 0.02\n",
            "iteration: 304250 loss: 0.0032 lr: 0.02\n",
            "iteration: 304260 loss: 0.0023 lr: 0.02\n",
            "iteration: 304270 loss: 0.0028 lr: 0.02\n",
            "iteration: 304280 loss: 0.0029 lr: 0.02\n",
            "iteration: 304290 loss: 0.0036 lr: 0.02\n",
            "iteration: 304300 loss: 0.0032 lr: 0.02\n",
            "iteration: 304310 loss: 0.0036 lr: 0.02\n",
            "iteration: 304320 loss: 0.0032 lr: 0.02\n",
            "iteration: 304330 loss: 0.0036 lr: 0.02\n",
            "iteration: 304340 loss: 0.0039 lr: 0.02\n",
            "iteration: 304350 loss: 0.0036 lr: 0.02\n",
            "iteration: 304360 loss: 0.0025 lr: 0.02\n",
            "iteration: 304370 loss: 0.0037 lr: 0.02\n",
            "iteration: 304380 loss: 0.0036 lr: 0.02\n",
            "iteration: 304390 loss: 0.0035 lr: 0.02\n",
            "iteration: 304400 loss: 0.0051 lr: 0.02\n",
            "iteration: 304410 loss: 0.0029 lr: 0.02\n",
            "iteration: 304420 loss: 0.0041 lr: 0.02\n",
            "iteration: 304430 loss: 0.0033 lr: 0.02\n",
            "iteration: 304440 loss: 0.0034 lr: 0.02\n",
            "iteration: 304450 loss: 0.0034 lr: 0.02\n",
            "iteration: 304460 loss: 0.0035 lr: 0.02\n",
            "iteration: 304470 loss: 0.0036 lr: 0.02\n",
            "iteration: 304480 loss: 0.0037 lr: 0.02\n",
            "iteration: 304490 loss: 0.0039 lr: 0.02\n",
            "iteration: 304500 loss: 0.0039 lr: 0.02\n",
            "iteration: 304510 loss: 0.0029 lr: 0.02\n",
            "iteration: 304520 loss: 0.0035 lr: 0.02\n",
            "iteration: 304530 loss: 0.0040 lr: 0.02\n",
            "iteration: 304540 loss: 0.0033 lr: 0.02\n",
            "iteration: 304550 loss: 0.0033 lr: 0.02\n",
            "iteration: 304560 loss: 0.0032 lr: 0.02\n",
            "iteration: 304570 loss: 0.0034 lr: 0.02\n",
            "iteration: 304580 loss: 0.0045 lr: 0.02\n",
            "iteration: 304590 loss: 0.0043 lr: 0.02\n",
            "iteration: 304600 loss: 0.0046 lr: 0.02\n",
            "iteration: 304610 loss: 0.0038 lr: 0.02\n",
            "iteration: 304620 loss: 0.0033 lr: 0.02\n",
            "iteration: 304630 loss: 0.0042 lr: 0.02\n",
            "iteration: 304640 loss: 0.0039 lr: 0.02\n",
            "iteration: 304650 loss: 0.0033 lr: 0.02\n",
            "iteration: 304660 loss: 0.0040 lr: 0.02\n",
            "iteration: 304670 loss: 0.0043 lr: 0.02\n",
            "iteration: 304680 loss: 0.0038 lr: 0.02\n",
            "iteration: 304690 loss: 0.0027 lr: 0.02\n",
            "iteration: 304700 loss: 0.0033 lr: 0.02\n",
            "iteration: 304710 loss: 0.0035 lr: 0.02\n",
            "iteration: 304720 loss: 0.0034 lr: 0.02\n",
            "iteration: 304730 loss: 0.0034 lr: 0.02\n",
            "iteration: 304740 loss: 0.0034 lr: 0.02\n",
            "iteration: 304750 loss: 0.0029 lr: 0.02\n",
            "iteration: 304760 loss: 0.0037 lr: 0.02\n",
            "iteration: 304770 loss: 0.0027 lr: 0.02\n",
            "iteration: 304780 loss: 0.0029 lr: 0.02\n",
            "iteration: 304790 loss: 0.0033 lr: 0.02\n",
            "iteration: 304800 loss: 0.0039 lr: 0.02\n",
            "iteration: 304810 loss: 0.0039 lr: 0.02\n",
            "iteration: 304820 loss: 0.0044 lr: 0.02\n",
            "iteration: 304830 loss: 0.0023 lr: 0.02\n",
            "iteration: 304840 loss: 0.0050 lr: 0.02\n",
            "iteration: 304850 loss: 0.0042 lr: 0.02\n",
            "iteration: 304860 loss: 0.0039 lr: 0.02\n",
            "iteration: 304870 loss: 0.0031 lr: 0.02\n",
            "iteration: 304880 loss: 0.0032 lr: 0.02\n",
            "iteration: 304890 loss: 0.0035 lr: 0.02\n",
            "iteration: 304900 loss: 0.0031 lr: 0.02\n",
            "iteration: 304910 loss: 0.0032 lr: 0.02\n",
            "iteration: 304920 loss: 0.0045 lr: 0.02\n",
            "iteration: 304930 loss: 0.0035 lr: 0.02\n",
            "iteration: 304940 loss: 0.0044 lr: 0.02\n",
            "iteration: 304950 loss: 0.0045 lr: 0.02\n",
            "iteration: 304960 loss: 0.0046 lr: 0.02\n",
            "iteration: 304970 loss: 0.0030 lr: 0.02\n",
            "iteration: 304980 loss: 0.0029 lr: 0.02\n",
            "iteration: 304990 loss: 0.0037 lr: 0.02\n",
            "iteration: 305000 loss: 0.0030 lr: 0.02\n",
            "iteration: 305010 loss: 0.0047 lr: 0.02\n",
            "iteration: 305020 loss: 0.0038 lr: 0.02\n",
            "iteration: 305030 loss: 0.0037 lr: 0.02\n",
            "iteration: 305040 loss: 0.0044 lr: 0.02\n",
            "iteration: 305050 loss: 0.0035 lr: 0.02\n",
            "iteration: 305060 loss: 0.0024 lr: 0.02\n",
            "iteration: 305070 loss: 0.0035 lr: 0.02\n",
            "iteration: 305080 loss: 0.0034 lr: 0.02\n",
            "iteration: 305090 loss: 0.0033 lr: 0.02\n",
            "iteration: 305100 loss: 0.0035 lr: 0.02\n",
            "iteration: 305110 loss: 0.0034 lr: 0.02\n",
            "iteration: 305120 loss: 0.0029 lr: 0.02\n",
            "iteration: 305130 loss: 0.0037 lr: 0.02\n",
            "iteration: 305140 loss: 0.0045 lr: 0.02\n",
            "iteration: 305150 loss: 0.0032 lr: 0.02\n",
            "iteration: 305160 loss: 0.0050 lr: 0.02\n",
            "iteration: 305170 loss: 0.0036 lr: 0.02\n",
            "iteration: 305180 loss: 0.0033 lr: 0.02\n",
            "iteration: 305190 loss: 0.0033 lr: 0.02\n",
            "iteration: 305200 loss: 0.0028 lr: 0.02\n",
            "iteration: 305210 loss: 0.0046 lr: 0.02\n",
            "iteration: 305220 loss: 0.0037 lr: 0.02\n",
            "iteration: 305230 loss: 0.0047 lr: 0.02\n",
            "iteration: 305240 loss: 0.0038 lr: 0.02\n",
            "iteration: 305250 loss: 0.0041 lr: 0.02\n",
            "iteration: 305260 loss: 0.0034 lr: 0.02\n",
            "iteration: 305270 loss: 0.0041 lr: 0.02\n",
            "iteration: 305280 loss: 0.0034 lr: 0.02\n",
            "iteration: 305290 loss: 0.0040 lr: 0.02\n",
            "iteration: 305300 loss: 0.0031 lr: 0.02\n",
            "iteration: 305310 loss: 0.0033 lr: 0.02\n",
            "iteration: 305320 loss: 0.0030 lr: 0.02\n",
            "iteration: 305330 loss: 0.0050 lr: 0.02\n",
            "iteration: 305340 loss: 0.0029 lr: 0.02\n",
            "iteration: 305350 loss: 0.0038 lr: 0.02\n",
            "iteration: 305360 loss: 0.0037 lr: 0.02\n",
            "iteration: 305370 loss: 0.0026 lr: 0.02\n",
            "iteration: 305380 loss: 0.0034 lr: 0.02\n",
            "iteration: 305390 loss: 0.0045 lr: 0.02\n",
            "iteration: 305400 loss: 0.0028 lr: 0.02\n",
            "iteration: 305410 loss: 0.0037 lr: 0.02\n",
            "iteration: 305420 loss: 0.0029 lr: 0.02\n",
            "iteration: 305430 loss: 0.0038 lr: 0.02\n",
            "iteration: 305440 loss: 0.0032 lr: 0.02\n",
            "iteration: 305450 loss: 0.0035 lr: 0.02\n",
            "iteration: 305460 loss: 0.0036 lr: 0.02\n",
            "iteration: 305470 loss: 0.0035 lr: 0.02\n",
            "iteration: 305480 loss: 0.0035 lr: 0.02\n",
            "iteration: 305490 loss: 0.0035 lr: 0.02\n",
            "iteration: 305500 loss: 0.0029 lr: 0.02\n",
            "iteration: 305510 loss: 0.0032 lr: 0.02\n",
            "iteration: 305520 loss: 0.0033 lr: 0.02\n",
            "iteration: 305530 loss: 0.0033 lr: 0.02\n",
            "iteration: 305540 loss: 0.0044 lr: 0.02\n",
            "iteration: 305550 loss: 0.0034 lr: 0.02\n",
            "iteration: 305560 loss: 0.0040 lr: 0.02\n",
            "iteration: 305570 loss: 0.0025 lr: 0.02\n",
            "iteration: 305580 loss: 0.0032 lr: 0.02\n",
            "iteration: 305590 loss: 0.0032 lr: 0.02\n",
            "iteration: 305600 loss: 0.0026 lr: 0.02\n",
            "iteration: 305610 loss: 0.0033 lr: 0.02\n",
            "iteration: 305620 loss: 0.0037 lr: 0.02\n",
            "iteration: 305630 loss: 0.0028 lr: 0.02\n",
            "iteration: 305640 loss: 0.0029 lr: 0.02\n",
            "iteration: 305650 loss: 0.0032 lr: 0.02\n",
            "iteration: 305660 loss: 0.0036 lr: 0.02\n",
            "iteration: 305670 loss: 0.0034 lr: 0.02\n",
            "iteration: 305680 loss: 0.0033 lr: 0.02\n",
            "iteration: 305690 loss: 0.0044 lr: 0.02\n",
            "iteration: 305700 loss: 0.0038 lr: 0.02\n",
            "iteration: 305710 loss: 0.0028 lr: 0.02\n",
            "iteration: 305720 loss: 0.0030 lr: 0.02\n",
            "iteration: 305730 loss: 0.0032 lr: 0.02\n",
            "iteration: 305740 loss: 0.0041 lr: 0.02\n",
            "iteration: 305750 loss: 0.0040 lr: 0.02\n",
            "iteration: 305760 loss: 0.0038 lr: 0.02\n",
            "iteration: 305770 loss: 0.0033 lr: 0.02\n",
            "iteration: 305780 loss: 0.0039 lr: 0.02\n",
            "iteration: 305790 loss: 0.0035 lr: 0.02\n",
            "iteration: 305800 loss: 0.0031 lr: 0.02\n",
            "iteration: 305810 loss: 0.0033 lr: 0.02\n",
            "iteration: 305820 loss: 0.0038 lr: 0.02\n",
            "iteration: 305830 loss: 0.0033 lr: 0.02\n",
            "iteration: 305840 loss: 0.0050 lr: 0.02\n",
            "iteration: 305850 loss: 0.0036 lr: 0.02\n",
            "iteration: 305860 loss: 0.0029 lr: 0.02\n",
            "iteration: 305870 loss: 0.0030 lr: 0.02\n",
            "iteration: 305880 loss: 0.0033 lr: 0.02\n",
            "iteration: 305890 loss: 0.0036 lr: 0.02\n",
            "iteration: 305900 loss: 0.0039 lr: 0.02\n",
            "iteration: 305910 loss: 0.0030 lr: 0.02\n",
            "iteration: 305920 loss: 0.0037 lr: 0.02\n",
            "iteration: 305930 loss: 0.0043 lr: 0.02\n",
            "iteration: 305940 loss: 0.0038 lr: 0.02\n",
            "iteration: 305950 loss: 0.0036 lr: 0.02\n",
            "iteration: 305960 loss: 0.0043 lr: 0.02\n",
            "iteration: 305970 loss: 0.0029 lr: 0.02\n",
            "iteration: 305980 loss: 0.0044 lr: 0.02\n",
            "iteration: 305990 loss: 0.0045 lr: 0.02\n",
            "iteration: 306000 loss: 0.0039 lr: 0.02\n",
            "iteration: 306010 loss: 0.0037 lr: 0.02\n",
            "iteration: 306020 loss: 0.0035 lr: 0.02\n",
            "iteration: 306030 loss: 0.0029 lr: 0.02\n",
            "iteration: 306040 loss: 0.0027 lr: 0.02\n",
            "iteration: 306050 loss: 0.0024 lr: 0.02\n",
            "iteration: 306060 loss: 0.0037 lr: 0.02\n",
            "iteration: 306070 loss: 0.0027 lr: 0.02\n",
            "iteration: 306080 loss: 0.0032 lr: 0.02\n",
            "iteration: 306090 loss: 0.0026 lr: 0.02\n",
            "iteration: 306100 loss: 0.0031 lr: 0.02\n",
            "iteration: 306110 loss: 0.0035 lr: 0.02\n",
            "iteration: 306120 loss: 0.0041 lr: 0.02\n",
            "iteration: 306130 loss: 0.0033 lr: 0.02\n",
            "iteration: 306140 loss: 0.0033 lr: 0.02\n",
            "iteration: 306150 loss: 0.0022 lr: 0.02\n",
            "iteration: 306160 loss: 0.0033 lr: 0.02\n",
            "iteration: 306170 loss: 0.0036 lr: 0.02\n",
            "iteration: 306180 loss: 0.0032 lr: 0.02\n",
            "iteration: 306190 loss: 0.0044 lr: 0.02\n",
            "iteration: 306200 loss: 0.0024 lr: 0.02\n",
            "iteration: 306210 loss: 0.0029 lr: 0.02\n",
            "iteration: 306220 loss: 0.0034 lr: 0.02\n",
            "iteration: 306230 loss: 0.0036 lr: 0.02\n",
            "iteration: 306240 loss: 0.0031 lr: 0.02\n",
            "iteration: 306250 loss: 0.0028 lr: 0.02\n",
            "iteration: 306260 loss: 0.0034 lr: 0.02\n",
            "iteration: 306270 loss: 0.0039 lr: 0.02\n",
            "iteration: 306280 loss: 0.0039 lr: 0.02\n",
            "iteration: 306290 loss: 0.0035 lr: 0.02\n",
            "iteration: 306300 loss: 0.0029 lr: 0.02\n",
            "iteration: 306310 loss: 0.0038 lr: 0.02\n",
            "iteration: 306320 loss: 0.0031 lr: 0.02\n",
            "iteration: 306330 loss: 0.0034 lr: 0.02\n",
            "iteration: 306340 loss: 0.0030 lr: 0.02\n",
            "iteration: 306350 loss: 0.0039 lr: 0.02\n",
            "iteration: 306360 loss: 0.0026 lr: 0.02\n",
            "iteration: 306370 loss: 0.0028 lr: 0.02\n",
            "iteration: 306380 loss: 0.0040 lr: 0.02\n",
            "iteration: 306390 loss: 0.0042 lr: 0.02\n",
            "iteration: 306400 loss: 0.0037 lr: 0.02\n",
            "iteration: 306410 loss: 0.0042 lr: 0.02\n",
            "iteration: 306420 loss: 0.0029 lr: 0.02\n",
            "iteration: 306430 loss: 0.0039 lr: 0.02\n",
            "iteration: 306440 loss: 0.0039 lr: 0.02\n",
            "iteration: 306450 loss: 0.0019 lr: 0.02\n",
            "iteration: 306460 loss: 0.0032 lr: 0.02\n",
            "iteration: 306470 loss: 0.0037 lr: 0.02\n",
            "iteration: 306480 loss: 0.0039 lr: 0.02\n",
            "iteration: 306490 loss: 0.0037 lr: 0.02\n",
            "iteration: 306500 loss: 0.0033 lr: 0.02\n",
            "iteration: 306510 loss: 0.0037 lr: 0.02\n",
            "iteration: 306520 loss: 0.0030 lr: 0.02\n",
            "iteration: 306530 loss: 0.0030 lr: 0.02\n",
            "iteration: 306540 loss: 0.0050 lr: 0.02\n",
            "iteration: 306550 loss: 0.0036 lr: 0.02\n",
            "iteration: 306560 loss: 0.0034 lr: 0.02\n",
            "iteration: 306570 loss: 0.0031 lr: 0.02\n",
            "iteration: 306580 loss: 0.0036 lr: 0.02\n",
            "iteration: 306590 loss: 0.0025 lr: 0.02\n",
            "iteration: 306600 loss: 0.0040 lr: 0.02\n",
            "iteration: 306610 loss: 0.0028 lr: 0.02\n",
            "iteration: 306620 loss: 0.0037 lr: 0.02\n",
            "iteration: 306630 loss: 0.0031 lr: 0.02\n",
            "iteration: 306640 loss: 0.0037 lr: 0.02\n",
            "iteration: 306650 loss: 0.0044 lr: 0.02\n",
            "iteration: 306660 loss: 0.0026 lr: 0.02\n",
            "iteration: 306670 loss: 0.0031 lr: 0.02\n",
            "iteration: 306680 loss: 0.0029 lr: 0.02\n",
            "iteration: 306690 loss: 0.0041 lr: 0.02\n",
            "iteration: 306700 loss: 0.0030 lr: 0.02\n",
            "iteration: 306710 loss: 0.0047 lr: 0.02\n",
            "iteration: 306720 loss: 0.0027 lr: 0.02\n",
            "iteration: 306730 loss: 0.0027 lr: 0.02\n",
            "iteration: 306740 loss: 0.0044 lr: 0.02\n",
            "iteration: 306750 loss: 0.0026 lr: 0.02\n",
            "iteration: 306760 loss: 0.0033 lr: 0.02\n",
            "iteration: 306770 loss: 0.0032 lr: 0.02\n",
            "iteration: 306780 loss: 0.0031 lr: 0.02\n",
            "iteration: 306790 loss: 0.0039 lr: 0.02\n",
            "iteration: 306800 loss: 0.0033 lr: 0.02\n",
            "iteration: 306810 loss: 0.0039 lr: 0.02\n",
            "iteration: 306820 loss: 0.0044 lr: 0.02\n",
            "iteration: 306830 loss: 0.0031 lr: 0.02\n",
            "iteration: 306840 loss: 0.0038 lr: 0.02\n",
            "iteration: 306850 loss: 0.0031 lr: 0.02\n",
            "iteration: 306860 loss: 0.0038 lr: 0.02\n",
            "iteration: 306870 loss: 0.0034 lr: 0.02\n",
            "iteration: 306880 loss: 0.0042 lr: 0.02\n",
            "iteration: 306890 loss: 0.0034 lr: 0.02\n",
            "iteration: 306900 loss: 0.0036 lr: 0.02\n",
            "iteration: 306910 loss: 0.0030 lr: 0.02\n",
            "iteration: 306920 loss: 0.0037 lr: 0.02\n",
            "iteration: 306930 loss: 0.0035 lr: 0.02\n",
            "iteration: 306940 loss: 0.0039 lr: 0.02\n",
            "iteration: 306950 loss: 0.0045 lr: 0.02\n",
            "iteration: 306960 loss: 0.0031 lr: 0.02\n",
            "iteration: 306970 loss: 0.0038 lr: 0.02\n",
            "iteration: 306980 loss: 0.0031 lr: 0.02\n",
            "iteration: 306990 loss: 0.0029 lr: 0.02\n",
            "iteration: 307000 loss: 0.0039 lr: 0.02\n",
            "iteration: 307010 loss: 0.0033 lr: 0.02\n",
            "iteration: 307020 loss: 0.0033 lr: 0.02\n",
            "iteration: 307030 loss: 0.0027 lr: 0.02\n",
            "iteration: 307040 loss: 0.0039 lr: 0.02\n",
            "iteration: 307050 loss: 0.0034 lr: 0.02\n",
            "iteration: 307060 loss: 0.0028 lr: 0.02\n",
            "iteration: 307070 loss: 0.0040 lr: 0.02\n",
            "iteration: 307080 loss: 0.0033 lr: 0.02\n",
            "iteration: 307090 loss: 0.0030 lr: 0.02\n",
            "iteration: 307100 loss: 0.0032 lr: 0.02\n",
            "iteration: 307110 loss: 0.0030 lr: 0.02\n",
            "iteration: 307120 loss: 0.0041 lr: 0.02\n",
            "iteration: 307130 loss: 0.0029 lr: 0.02\n",
            "iteration: 307140 loss: 0.0032 lr: 0.02\n",
            "iteration: 307150 loss: 0.0037 lr: 0.02\n",
            "iteration: 307160 loss: 0.0033 lr: 0.02\n",
            "iteration: 307170 loss: 0.0028 lr: 0.02\n",
            "iteration: 307180 loss: 0.0031 lr: 0.02\n",
            "iteration: 307190 loss: 0.0033 lr: 0.02\n",
            "iteration: 307200 loss: 0.0031 lr: 0.02\n",
            "iteration: 307210 loss: 0.0026 lr: 0.02\n",
            "iteration: 307220 loss: 0.0033 lr: 0.02\n",
            "iteration: 307230 loss: 0.0036 lr: 0.02\n",
            "iteration: 307240 loss: 0.0037 lr: 0.02\n",
            "iteration: 307250 loss: 0.0031 lr: 0.02\n",
            "iteration: 307260 loss: 0.0033 lr: 0.02\n",
            "iteration: 307270 loss: 0.0027 lr: 0.02\n",
            "iteration: 307280 loss: 0.0035 lr: 0.02\n",
            "iteration: 307290 loss: 0.0039 lr: 0.02\n",
            "iteration: 307300 loss: 0.0044 lr: 0.02\n",
            "iteration: 307310 loss: 0.0036 lr: 0.02\n",
            "iteration: 307320 loss: 0.0035 lr: 0.02\n",
            "iteration: 307330 loss: 0.0033 lr: 0.02\n",
            "iteration: 307340 loss: 0.0039 lr: 0.02\n",
            "iteration: 307350 loss: 0.0031 lr: 0.02\n",
            "iteration: 307360 loss: 0.0035 lr: 0.02\n",
            "iteration: 307370 loss: 0.0031 lr: 0.02\n",
            "iteration: 307380 loss: 0.0033 lr: 0.02\n",
            "iteration: 307390 loss: 0.0036 lr: 0.02\n",
            "iteration: 307400 loss: 0.0030 lr: 0.02\n",
            "iteration: 307410 loss: 0.0041 lr: 0.02\n",
            "iteration: 307420 loss: 0.0027 lr: 0.02\n",
            "iteration: 307430 loss: 0.0030 lr: 0.02\n",
            "iteration: 307440 loss: 0.0033 lr: 0.02\n",
            "iteration: 307450 loss: 0.0027 lr: 0.02\n",
            "iteration: 307460 loss: 0.0028 lr: 0.02\n",
            "iteration: 307470 loss: 0.0035 lr: 0.02\n",
            "iteration: 307480 loss: 0.0040 lr: 0.02\n",
            "iteration: 307490 loss: 0.0029 lr: 0.02\n",
            "iteration: 307500 loss: 0.0037 lr: 0.02\n",
            "iteration: 307510 loss: 0.0043 lr: 0.02\n",
            "iteration: 307520 loss: 0.0034 lr: 0.02\n",
            "iteration: 307530 loss: 0.0039 lr: 0.02\n",
            "iteration: 307540 loss: 0.0043 lr: 0.02\n",
            "iteration: 307550 loss: 0.0040 lr: 0.02\n",
            "iteration: 307560 loss: 0.0041 lr: 0.02\n",
            "iteration: 307570 loss: 0.0031 lr: 0.02\n",
            "iteration: 307580 loss: 0.0028 lr: 0.02\n",
            "iteration: 307590 loss: 0.0025 lr: 0.02\n",
            "iteration: 307600 loss: 0.0034 lr: 0.02\n",
            "iteration: 307610 loss: 0.0039 lr: 0.02\n",
            "iteration: 307620 loss: 0.0049 lr: 0.02\n",
            "iteration: 307630 loss: 0.0034 lr: 0.02\n",
            "iteration: 307640 loss: 0.0038 lr: 0.02\n",
            "iteration: 307650 loss: 0.0039 lr: 0.02\n",
            "iteration: 307660 loss: 0.0023 lr: 0.02\n",
            "iteration: 307670 loss: 0.0027 lr: 0.02\n",
            "iteration: 307680 loss: 0.0034 lr: 0.02\n",
            "iteration: 307690 loss: 0.0032 lr: 0.02\n",
            "iteration: 307700 loss: 0.0033 lr: 0.02\n",
            "iteration: 307710 loss: 0.0033 lr: 0.02\n",
            "iteration: 307720 loss: 0.0030 lr: 0.02\n",
            "iteration: 307730 loss: 0.0029 lr: 0.02\n",
            "iteration: 307740 loss: 0.0037 lr: 0.02\n",
            "iteration: 307750 loss: 0.0032 lr: 0.02\n",
            "iteration: 307760 loss: 0.0031 lr: 0.02\n",
            "iteration: 307770 loss: 0.0043 lr: 0.02\n",
            "iteration: 307780 loss: 0.0028 lr: 0.02\n",
            "iteration: 307790 loss: 0.0035 lr: 0.02\n",
            "iteration: 307800 loss: 0.0037 lr: 0.02\n",
            "iteration: 307810 loss: 0.0037 lr: 0.02\n",
            "iteration: 307820 loss: 0.0031 lr: 0.02\n",
            "iteration: 307830 loss: 0.0029 lr: 0.02\n",
            "iteration: 307840 loss: 0.0035 lr: 0.02\n",
            "iteration: 307850 loss: 0.0044 lr: 0.02\n",
            "iteration: 307860 loss: 0.0043 lr: 0.02\n",
            "iteration: 307870 loss: 0.0031 lr: 0.02\n",
            "iteration: 307880 loss: 0.0038 lr: 0.02\n",
            "iteration: 307890 loss: 0.0034 lr: 0.02\n",
            "iteration: 307900 loss: 0.0037 lr: 0.02\n",
            "iteration: 307910 loss: 0.0029 lr: 0.02\n",
            "iteration: 307920 loss: 0.0025 lr: 0.02\n",
            "iteration: 307930 loss: 0.0047 lr: 0.02\n",
            "iteration: 307940 loss: 0.0038 lr: 0.02\n",
            "iteration: 307950 loss: 0.0036 lr: 0.02\n",
            "iteration: 307960 loss: 0.0041 lr: 0.02\n",
            "iteration: 307970 loss: 0.0030 lr: 0.02\n",
            "iteration: 307980 loss: 0.0039 lr: 0.02\n",
            "iteration: 307990 loss: 0.0029 lr: 0.02\n",
            "iteration: 308000 loss: 0.0035 lr: 0.02\n",
            "iteration: 308010 loss: 0.0040 lr: 0.02\n",
            "iteration: 308020 loss: 0.0031 lr: 0.02\n",
            "iteration: 308030 loss: 0.0038 lr: 0.02\n",
            "iteration: 308040 loss: 0.0032 lr: 0.02\n",
            "iteration: 308050 loss: 0.0029 lr: 0.02\n",
            "iteration: 308060 loss: 0.0032 lr: 0.02\n",
            "iteration: 308070 loss: 0.0033 lr: 0.02\n",
            "iteration: 308080 loss: 0.0024 lr: 0.02\n",
            "iteration: 308090 loss: 0.0036 lr: 0.02\n",
            "iteration: 308100 loss: 0.0032 lr: 0.02\n",
            "iteration: 308110 loss: 0.0041 lr: 0.02\n",
            "iteration: 308120 loss: 0.0029 lr: 0.02\n",
            "iteration: 308130 loss: 0.0031 lr: 0.02\n",
            "iteration: 308140 loss: 0.0026 lr: 0.02\n",
            "iteration: 308150 loss: 0.0036 lr: 0.02\n",
            "iteration: 308160 loss: 0.0038 lr: 0.02\n",
            "iteration: 308170 loss: 0.0042 lr: 0.02\n",
            "iteration: 308180 loss: 0.0034 lr: 0.02\n",
            "iteration: 308190 loss: 0.0045 lr: 0.02\n",
            "iteration: 308200 loss: 0.0041 lr: 0.02\n",
            "iteration: 308210 loss: 0.0040 lr: 0.02\n",
            "iteration: 308220 loss: 0.0038 lr: 0.02\n",
            "iteration: 308230 loss: 0.0032 lr: 0.02\n",
            "iteration: 308240 loss: 0.0046 lr: 0.02\n",
            "iteration: 308250 loss: 0.0045 lr: 0.02\n",
            "iteration: 308260 loss: 0.0036 lr: 0.02\n",
            "iteration: 308270 loss: 0.0025 lr: 0.02\n",
            "iteration: 308280 loss: 0.0036 lr: 0.02\n",
            "iteration: 308290 loss: 0.0030 lr: 0.02\n",
            "iteration: 308300 loss: 0.0035 lr: 0.02\n",
            "iteration: 308310 loss: 0.0039 lr: 0.02\n",
            "iteration: 308320 loss: 0.0035 lr: 0.02\n",
            "iteration: 308330 loss: 0.0042 lr: 0.02\n",
            "iteration: 308340 loss: 0.0025 lr: 0.02\n",
            "iteration: 308350 loss: 0.0050 lr: 0.02\n",
            "iteration: 308360 loss: 0.0035 lr: 0.02\n",
            "iteration: 308370 loss: 0.0041 lr: 0.02\n",
            "iteration: 308380 loss: 0.0037 lr: 0.02\n",
            "iteration: 308390 loss: 0.0043 lr: 0.02\n",
            "iteration: 308400 loss: 0.0037 lr: 0.02\n",
            "iteration: 308410 loss: 0.0037 lr: 0.02\n",
            "iteration: 308420 loss: 0.0037 lr: 0.02\n",
            "iteration: 308430 loss: 0.0035 lr: 0.02\n",
            "iteration: 308440 loss: 0.0029 lr: 0.02\n",
            "iteration: 308450 loss: 0.0028 lr: 0.02\n",
            "iteration: 308460 loss: 0.0030 lr: 0.02\n",
            "iteration: 308470 loss: 0.0038 lr: 0.02\n",
            "iteration: 308480 loss: 0.0034 lr: 0.02\n",
            "iteration: 308490 loss: 0.0026 lr: 0.02\n",
            "iteration: 308500 loss: 0.0040 lr: 0.02\n",
            "iteration: 308510 loss: 0.0033 lr: 0.02\n",
            "iteration: 308520 loss: 0.0024 lr: 0.02\n",
            "iteration: 308530 loss: 0.0033 lr: 0.02\n",
            "iteration: 308540 loss: 0.0031 lr: 0.02\n",
            "iteration: 308550 loss: 0.0022 lr: 0.02\n",
            "iteration: 308560 loss: 0.0036 lr: 0.02\n",
            "iteration: 308570 loss: 0.0031 lr: 0.02\n",
            "iteration: 308580 loss: 0.0034 lr: 0.02\n",
            "iteration: 308590 loss: 0.0040 lr: 0.02\n",
            "iteration: 308600 loss: 0.0037 lr: 0.02\n",
            "iteration: 308610 loss: 0.0033 lr: 0.02\n",
            "iteration: 308620 loss: 0.0041 lr: 0.02\n",
            "iteration: 308630 loss: 0.0030 lr: 0.02\n",
            "iteration: 308640 loss: 0.0038 lr: 0.02\n",
            "iteration: 308650 loss: 0.0039 lr: 0.02\n",
            "iteration: 308660 loss: 0.0027 lr: 0.02\n",
            "iteration: 308670 loss: 0.0043 lr: 0.02\n",
            "iteration: 308680 loss: 0.0039 lr: 0.02\n",
            "iteration: 308690 loss: 0.0035 lr: 0.02\n",
            "iteration: 308700 loss: 0.0040 lr: 0.02\n",
            "iteration: 308710 loss: 0.0046 lr: 0.02\n",
            "iteration: 308720 loss: 0.0031 lr: 0.02\n",
            "iteration: 308730 loss: 0.0030 lr: 0.02\n",
            "iteration: 308740 loss: 0.0031 lr: 0.02\n",
            "iteration: 308750 loss: 0.0040 lr: 0.02\n",
            "iteration: 308760 loss: 0.0032 lr: 0.02\n",
            "iteration: 308770 loss: 0.0025 lr: 0.02\n",
            "iteration: 308780 loss: 0.0026 lr: 0.02\n",
            "iteration: 308790 loss: 0.0033 lr: 0.02\n",
            "iteration: 308800 loss: 0.0035 lr: 0.02\n",
            "iteration: 308810 loss: 0.0027 lr: 0.02\n",
            "iteration: 308820 loss: 0.0044 lr: 0.02\n",
            "iteration: 308830 loss: 0.0034 lr: 0.02\n",
            "iteration: 308840 loss: 0.0035 lr: 0.02\n",
            "iteration: 308850 loss: 0.0034 lr: 0.02\n",
            "iteration: 308860 loss: 0.0041 lr: 0.02\n",
            "iteration: 308870 loss: 0.0029 lr: 0.02\n",
            "iteration: 308880 loss: 0.0026 lr: 0.02\n",
            "iteration: 308890 loss: 0.0031 lr: 0.02\n",
            "iteration: 308900 loss: 0.0034 lr: 0.02\n",
            "iteration: 308910 loss: 0.0035 lr: 0.02\n",
            "iteration: 308920 loss: 0.0036 lr: 0.02\n",
            "iteration: 308930 loss: 0.0034 lr: 0.02\n",
            "iteration: 308940 loss: 0.0039 lr: 0.02\n",
            "iteration: 308950 loss: 0.0039 lr: 0.02\n",
            "iteration: 308960 loss: 0.0031 lr: 0.02\n",
            "iteration: 308970 loss: 0.0038 lr: 0.02\n",
            "iteration: 308980 loss: 0.0031 lr: 0.02\n",
            "iteration: 308990 loss: 0.0032 lr: 0.02\n",
            "iteration: 309000 loss: 0.0034 lr: 0.02\n",
            "iteration: 309010 loss: 0.0027 lr: 0.02\n",
            "iteration: 309020 loss: 0.0033 lr: 0.02\n",
            "iteration: 309030 loss: 0.0029 lr: 0.02\n",
            "iteration: 309040 loss: 0.0028 lr: 0.02\n",
            "iteration: 309050 loss: 0.0030 lr: 0.02\n",
            "iteration: 309060 loss: 0.0040 lr: 0.02\n",
            "iteration: 309070 loss: 0.0034 lr: 0.02\n",
            "iteration: 309080 loss: 0.0036 lr: 0.02\n",
            "iteration: 309090 loss: 0.0028 lr: 0.02\n",
            "iteration: 309100 loss: 0.0031 lr: 0.02\n",
            "iteration: 309110 loss: 0.0034 lr: 0.02\n",
            "iteration: 309120 loss: 0.0029 lr: 0.02\n",
            "iteration: 309130 loss: 0.0033 lr: 0.02\n",
            "iteration: 309140 loss: 0.0040 lr: 0.02\n",
            "iteration: 309150 loss: 0.0035 lr: 0.02\n",
            "iteration: 309160 loss: 0.0039 lr: 0.02\n",
            "iteration: 309170 loss: 0.0038 lr: 0.02\n",
            "iteration: 309180 loss: 0.0039 lr: 0.02\n",
            "iteration: 309190 loss: 0.0026 lr: 0.02\n",
            "iteration: 309200 loss: 0.0033 lr: 0.02\n",
            "iteration: 309210 loss: 0.0037 lr: 0.02\n",
            "iteration: 309220 loss: 0.0039 lr: 0.02\n",
            "iteration: 309230 loss: 0.0035 lr: 0.02\n",
            "iteration: 309240 loss: 0.0038 lr: 0.02\n",
            "iteration: 309250 loss: 0.0046 lr: 0.02\n",
            "iteration: 309260 loss: 0.0038 lr: 0.02\n",
            "iteration: 309270 loss: 0.0030 lr: 0.02\n",
            "iteration: 309280 loss: 0.0044 lr: 0.02\n",
            "iteration: 309290 loss: 0.0032 lr: 0.02\n",
            "iteration: 309300 loss: 0.0039 lr: 0.02\n",
            "iteration: 309310 loss: 0.0022 lr: 0.02\n",
            "iteration: 309320 loss: 0.0025 lr: 0.02\n",
            "iteration: 309330 loss: 0.0036 lr: 0.02\n",
            "iteration: 309340 loss: 0.0029 lr: 0.02\n",
            "iteration: 309350 loss: 0.0032 lr: 0.02\n",
            "iteration: 309360 loss: 0.0038 lr: 0.02\n",
            "iteration: 309370 loss: 0.0030 lr: 0.02\n",
            "iteration: 309380 loss: 0.0044 lr: 0.02\n",
            "iteration: 309390 loss: 0.0030 lr: 0.02\n",
            "iteration: 309400 loss: 0.0041 lr: 0.02\n",
            "iteration: 309410 loss: 0.0032 lr: 0.02\n",
            "iteration: 309420 loss: 0.0034 lr: 0.02\n",
            "iteration: 309430 loss: 0.0029 lr: 0.02\n",
            "iteration: 309440 loss: 0.0022 lr: 0.02\n",
            "iteration: 309450 loss: 0.0038 lr: 0.02\n",
            "iteration: 309460 loss: 0.0027 lr: 0.02\n",
            "iteration: 309470 loss: 0.0030 lr: 0.02\n",
            "iteration: 309480 loss: 0.0032 lr: 0.02\n",
            "iteration: 309490 loss: 0.0035 lr: 0.02\n",
            "iteration: 309500 loss: 0.0041 lr: 0.02\n",
            "iteration: 309510 loss: 0.0030 lr: 0.02\n",
            "iteration: 309520 loss: 0.0040 lr: 0.02\n",
            "iteration: 309530 loss: 0.0035 lr: 0.02\n",
            "iteration: 309540 loss: 0.0040 lr: 0.02\n",
            "iteration: 309550 loss: 0.0040 lr: 0.02\n",
            "iteration: 309560 loss: 0.0041 lr: 0.02\n",
            "iteration: 309570 loss: 0.0029 lr: 0.02\n",
            "iteration: 309580 loss: 0.0037 lr: 0.02\n",
            "iteration: 309590 loss: 0.0035 lr: 0.02\n",
            "iteration: 309600 loss: 0.0033 lr: 0.02\n",
            "iteration: 309610 loss: 0.0029 lr: 0.02\n",
            "iteration: 309620 loss: 0.0032 lr: 0.02\n",
            "iteration: 309630 loss: 0.0037 lr: 0.02\n",
            "iteration: 309640 loss: 0.0032 lr: 0.02\n",
            "iteration: 309650 loss: 0.0030 lr: 0.02\n",
            "iteration: 309660 loss: 0.0033 lr: 0.02\n",
            "iteration: 309670 loss: 0.0028 lr: 0.02\n",
            "iteration: 309680 loss: 0.0050 lr: 0.02\n",
            "iteration: 309690 loss: 0.0035 lr: 0.02\n",
            "iteration: 309700 loss: 0.0029 lr: 0.02\n",
            "iteration: 309710 loss: 0.0043 lr: 0.02\n",
            "iteration: 309720 loss: 0.0040 lr: 0.02\n",
            "iteration: 309730 loss: 0.0028 lr: 0.02\n",
            "iteration: 309740 loss: 0.0021 lr: 0.02\n",
            "iteration: 309750 loss: 0.0036 lr: 0.02\n",
            "iteration: 309760 loss: 0.0036 lr: 0.02\n",
            "iteration: 309770 loss: 0.0032 lr: 0.02\n",
            "iteration: 309780 loss: 0.0048 lr: 0.02\n",
            "iteration: 309790 loss: 0.0035 lr: 0.02\n",
            "iteration: 309800 loss: 0.0039 lr: 0.02\n",
            "iteration: 309810 loss: 0.0037 lr: 0.02\n",
            "iteration: 309820 loss: 0.0031 lr: 0.02\n",
            "iteration: 309830 loss: 0.0047 lr: 0.02\n",
            "iteration: 309840 loss: 0.0038 lr: 0.02\n",
            "iteration: 309850 loss: 0.0034 lr: 0.02\n",
            "iteration: 309860 loss: 0.0039 lr: 0.02\n",
            "iteration: 309870 loss: 0.0036 lr: 0.02\n",
            "iteration: 309880 loss: 0.0039 lr: 0.02\n",
            "iteration: 309890 loss: 0.0037 lr: 0.02\n",
            "iteration: 309900 loss: 0.0039 lr: 0.02\n",
            "iteration: 309910 loss: 0.0042 lr: 0.02\n",
            "iteration: 309920 loss: 0.0038 lr: 0.02\n",
            "iteration: 309930 loss: 0.0035 lr: 0.02\n",
            "iteration: 309940 loss: 0.0040 lr: 0.02\n",
            "iteration: 309950 loss: 0.0030 lr: 0.02\n",
            "iteration: 309960 loss: 0.0037 lr: 0.02\n",
            "iteration: 309970 loss: 0.0040 lr: 0.02\n",
            "iteration: 309980 loss: 0.0033 lr: 0.02\n",
            "iteration: 309990 loss: 0.0030 lr: 0.02\n",
            "iteration: 310000 loss: 0.0035 lr: 0.02\n",
            "iteration: 310010 loss: 0.0027 lr: 0.02\n",
            "iteration: 310020 loss: 0.0032 lr: 0.02\n",
            "iteration: 310030 loss: 0.0038 lr: 0.02\n",
            "iteration: 310040 loss: 0.0044 lr: 0.02\n",
            "iteration: 310050 loss: 0.0034 lr: 0.02\n",
            "iteration: 310060 loss: 0.0034 lr: 0.02\n",
            "iteration: 310070 loss: 0.0040 lr: 0.02\n",
            "iteration: 310080 loss: 0.0036 lr: 0.02\n",
            "iteration: 310090 loss: 0.0030 lr: 0.02\n",
            "iteration: 310100 loss: 0.0033 lr: 0.02\n",
            "iteration: 310110 loss: 0.0050 lr: 0.02\n",
            "iteration: 310120 loss: 0.0030 lr: 0.02\n",
            "iteration: 310130 loss: 0.0029 lr: 0.02\n",
            "iteration: 310140 loss: 0.0033 lr: 0.02\n",
            "iteration: 310150 loss: 0.0036 lr: 0.02\n",
            "iteration: 310160 loss: 0.0036 lr: 0.02\n",
            "iteration: 310170 loss: 0.0042 lr: 0.02\n",
            "iteration: 310180 loss: 0.0031 lr: 0.02\n",
            "iteration: 310190 loss: 0.0050 lr: 0.02\n",
            "iteration: 310200 loss: 0.0034 lr: 0.02\n",
            "iteration: 310210 loss: 0.0033 lr: 0.02\n",
            "iteration: 310220 loss: 0.0033 lr: 0.02\n",
            "iteration: 310230 loss: 0.0044 lr: 0.02\n",
            "iteration: 310240 loss: 0.0027 lr: 0.02\n",
            "iteration: 310250 loss: 0.0029 lr: 0.02\n",
            "iteration: 310260 loss: 0.0039 lr: 0.02\n",
            "iteration: 310270 loss: 0.0021 lr: 0.02\n",
            "iteration: 310280 loss: 0.0053 lr: 0.02\n",
            "iteration: 310290 loss: 0.0034 lr: 0.02\n",
            "iteration: 310300 loss: 0.0037 lr: 0.02\n",
            "iteration: 310310 loss: 0.0033 lr: 0.02\n",
            "iteration: 310320 loss: 0.0041 lr: 0.02\n",
            "iteration: 310330 loss: 0.0034 lr: 0.02\n",
            "iteration: 310340 loss: 0.0029 lr: 0.02\n",
            "iteration: 310350 loss: 0.0031 lr: 0.02\n",
            "iteration: 310360 loss: 0.0038 lr: 0.02\n",
            "iteration: 310370 loss: 0.0026 lr: 0.02\n",
            "iteration: 310380 loss: 0.0032 lr: 0.02\n",
            "iteration: 310390 loss: 0.0037 lr: 0.02\n",
            "iteration: 310400 loss: 0.0040 lr: 0.02\n",
            "iteration: 310410 loss: 0.0041 lr: 0.02\n",
            "iteration: 310420 loss: 0.0042 lr: 0.02\n",
            "iteration: 310430 loss: 0.0037 lr: 0.02\n",
            "iteration: 310440 loss: 0.0035 lr: 0.02\n",
            "iteration: 310450 loss: 0.0046 lr: 0.02\n",
            "iteration: 310460 loss: 0.0033 lr: 0.02\n",
            "iteration: 310470 loss: 0.0039 lr: 0.02\n",
            "iteration: 310480 loss: 0.0040 lr: 0.02\n",
            "iteration: 310490 loss: 0.0034 lr: 0.02\n",
            "iteration: 310500 loss: 0.0030 lr: 0.02\n",
            "iteration: 310510 loss: 0.0030 lr: 0.02\n",
            "iteration: 310520 loss: 0.0045 lr: 0.02\n",
            "iteration: 310530 loss: 0.0029 lr: 0.02\n",
            "iteration: 310540 loss: 0.0040 lr: 0.02\n",
            "iteration: 310550 loss: 0.0028 lr: 0.02\n",
            "iteration: 310560 loss: 0.0042 lr: 0.02\n",
            "iteration: 310570 loss: 0.0040 lr: 0.02\n",
            "iteration: 310580 loss: 0.0034 lr: 0.02\n",
            "iteration: 310590 loss: 0.0031 lr: 0.02\n",
            "iteration: 310600 loss: 0.0036 lr: 0.02\n",
            "iteration: 310610 loss: 0.0030 lr: 0.02\n",
            "iteration: 310620 loss: 0.0034 lr: 0.02\n",
            "iteration: 310630 loss: 0.0030 lr: 0.02\n",
            "iteration: 310640 loss: 0.0042 lr: 0.02\n",
            "iteration: 310650 loss: 0.0033 lr: 0.02\n",
            "iteration: 310660 loss: 0.0032 lr: 0.02\n",
            "iteration: 310670 loss: 0.0034 lr: 0.02\n",
            "iteration: 310680 loss: 0.0032 lr: 0.02\n",
            "iteration: 310690 loss: 0.0034 lr: 0.02\n",
            "iteration: 310700 loss: 0.0031 lr: 0.02\n",
            "iteration: 310710 loss: 0.0043 lr: 0.02\n",
            "iteration: 310720 loss: 0.0037 lr: 0.02\n",
            "iteration: 310730 loss: 0.0037 lr: 0.02\n",
            "iteration: 310740 loss: 0.0035 lr: 0.02\n",
            "iteration: 310750 loss: 0.0033 lr: 0.02\n",
            "iteration: 310760 loss: 0.0035 lr: 0.02\n",
            "iteration: 310770 loss: 0.0029 lr: 0.02\n",
            "iteration: 310780 loss: 0.0027 lr: 0.02\n",
            "iteration: 310790 loss: 0.0040 lr: 0.02\n",
            "iteration: 310800 loss: 0.0031 lr: 0.02\n",
            "iteration: 310810 loss: 0.0035 lr: 0.02\n",
            "iteration: 310820 loss: 0.0032 lr: 0.02\n",
            "iteration: 310830 loss: 0.0029 lr: 0.02\n",
            "iteration: 310840 loss: 0.0032 lr: 0.02\n",
            "iteration: 310850 loss: 0.0039 lr: 0.02\n",
            "iteration: 310860 loss: 0.0030 lr: 0.02\n",
            "iteration: 310870 loss: 0.0037 lr: 0.02\n",
            "iteration: 310880 loss: 0.0039 lr: 0.02\n",
            "iteration: 310890 loss: 0.0034 lr: 0.02\n",
            "iteration: 310900 loss: 0.0035 lr: 0.02\n",
            "iteration: 310910 loss: 0.0034 lr: 0.02\n",
            "iteration: 310920 loss: 0.0030 lr: 0.02\n",
            "iteration: 310930 loss: 0.0036 lr: 0.02\n",
            "iteration: 310940 loss: 0.0040 lr: 0.02\n",
            "iteration: 310950 loss: 0.0034 lr: 0.02\n",
            "iteration: 310960 loss: 0.0035 lr: 0.02\n",
            "iteration: 310970 loss: 0.0031 lr: 0.02\n",
            "iteration: 310980 loss: 0.0032 lr: 0.02\n",
            "iteration: 310990 loss: 0.0030 lr: 0.02\n",
            "iteration: 311000 loss: 0.0029 lr: 0.02\n",
            "iteration: 311010 loss: 0.0036 lr: 0.02\n",
            "iteration: 311020 loss: 0.0031 lr: 0.02\n",
            "iteration: 311030 loss: 0.0026 lr: 0.02\n",
            "iteration: 311040 loss: 0.0028 lr: 0.02\n",
            "iteration: 311050 loss: 0.0031 lr: 0.02\n",
            "iteration: 311060 loss: 0.0032 lr: 0.02\n",
            "iteration: 311070 loss: 0.0026 lr: 0.02\n",
            "iteration: 311080 loss: 0.0032 lr: 0.02\n",
            "iteration: 311090 loss: 0.0032 lr: 0.02\n",
            "iteration: 311100 loss: 0.0034 lr: 0.02\n",
            "iteration: 311110 loss: 0.0048 lr: 0.02\n",
            "iteration: 311120 loss: 0.0034 lr: 0.02\n",
            "iteration: 311130 loss: 0.0038 lr: 0.02\n",
            "iteration: 311140 loss: 0.0035 lr: 0.02\n",
            "iteration: 311150 loss: 0.0025 lr: 0.02\n",
            "iteration: 311160 loss: 0.0039 lr: 0.02\n",
            "iteration: 311170 loss: 0.0037 lr: 0.02\n",
            "iteration: 311180 loss: 0.0031 lr: 0.02\n",
            "iteration: 311190 loss: 0.0034 lr: 0.02\n",
            "iteration: 311200 loss: 0.0039 lr: 0.02\n",
            "iteration: 311210 loss: 0.0029 lr: 0.02\n",
            "iteration: 311220 loss: 0.0036 lr: 0.02\n",
            "iteration: 311230 loss: 0.0032 lr: 0.02\n",
            "iteration: 311240 loss: 0.0037 lr: 0.02\n",
            "iteration: 311250 loss: 0.0031 lr: 0.02\n",
            "iteration: 311260 loss: 0.0029 lr: 0.02\n",
            "iteration: 311270 loss: 0.0035 lr: 0.02\n",
            "iteration: 311280 loss: 0.0033 lr: 0.02\n",
            "iteration: 311290 loss: 0.0024 lr: 0.02\n",
            "iteration: 311300 loss: 0.0023 lr: 0.02\n",
            "iteration: 311310 loss: 0.0028 lr: 0.02\n",
            "iteration: 311320 loss: 0.0032 lr: 0.02\n",
            "iteration: 311330 loss: 0.0037 lr: 0.02\n",
            "iteration: 311340 loss: 0.0035 lr: 0.02\n",
            "iteration: 311350 loss: 0.0031 lr: 0.02\n",
            "iteration: 311360 loss: 0.0032 lr: 0.02\n",
            "iteration: 311370 loss: 0.0029 lr: 0.02\n",
            "iteration: 311380 loss: 0.0030 lr: 0.02\n",
            "iteration: 311390 loss: 0.0041 lr: 0.02\n",
            "iteration: 311400 loss: 0.0044 lr: 0.02\n",
            "iteration: 311410 loss: 0.0037 lr: 0.02\n",
            "iteration: 311420 loss: 0.0040 lr: 0.02\n",
            "iteration: 311430 loss: 0.0039 lr: 0.02\n",
            "iteration: 311440 loss: 0.0037 lr: 0.02\n",
            "iteration: 311450 loss: 0.0040 lr: 0.02\n",
            "iteration: 311460 loss: 0.0032 lr: 0.02\n",
            "iteration: 311470 loss: 0.0028 lr: 0.02\n",
            "iteration: 311480 loss: 0.0040 lr: 0.02\n",
            "iteration: 311490 loss: 0.0035 lr: 0.02\n",
            "iteration: 311500 loss: 0.0029 lr: 0.02\n",
            "iteration: 311510 loss: 0.0040 lr: 0.02\n",
            "iteration: 311520 loss: 0.0030 lr: 0.02\n",
            "iteration: 311530 loss: 0.0033 lr: 0.02\n",
            "iteration: 311540 loss: 0.0036 lr: 0.02\n",
            "iteration: 311550 loss: 0.0042 lr: 0.02\n",
            "iteration: 311560 loss: 0.0037 lr: 0.02\n",
            "iteration: 311570 loss: 0.0053 lr: 0.02\n",
            "iteration: 311580 loss: 0.0035 lr: 0.02\n",
            "iteration: 311590 loss: 0.0042 lr: 0.02\n",
            "iteration: 311600 loss: 0.0035 lr: 0.02\n",
            "iteration: 311610 loss: 0.0034 lr: 0.02\n",
            "iteration: 311620 loss: 0.0040 lr: 0.02\n",
            "iteration: 311630 loss: 0.0043 lr: 0.02\n",
            "iteration: 311640 loss: 0.0041 lr: 0.02\n",
            "iteration: 311650 loss: 0.0037 lr: 0.02\n",
            "iteration: 311660 loss: 0.0029 lr: 0.02\n",
            "iteration: 311670 loss: 0.0028 lr: 0.02\n",
            "iteration: 311680 loss: 0.0033 lr: 0.02\n",
            "iteration: 311690 loss: 0.0030 lr: 0.02\n",
            "iteration: 311700 loss: 0.0030 lr: 0.02\n",
            "iteration: 311710 loss: 0.0033 lr: 0.02\n",
            "iteration: 311720 loss: 0.0028 lr: 0.02\n",
            "iteration: 311730 loss: 0.0034 lr: 0.02\n",
            "iteration: 311740 loss: 0.0028 lr: 0.02\n",
            "iteration: 311750 loss: 0.0036 lr: 0.02\n",
            "iteration: 311760 loss: 0.0035 lr: 0.02\n",
            "iteration: 311770 loss: 0.0029 lr: 0.02\n",
            "iteration: 311780 loss: 0.0043 lr: 0.02\n",
            "iteration: 311790 loss: 0.0039 lr: 0.02\n",
            "iteration: 311800 loss: 0.0038 lr: 0.02\n",
            "iteration: 311810 loss: 0.0032 lr: 0.02\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwBXkB93kFCX"
      },
      "source": [
        "### Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 247
        },
        "id": "F7zABfgecmIz",
        "outputId": "0cd31c6b-4020-40d8-a93d-9974ccec36f7"
      },
      "source": [
        "%matplotlib notebook\n",
        "deeplabcut.evaluate_network(path_config_file,plotting=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running  DLC_resnet50_final_trackerOct20shuffle1_312000  with # of trainingiterations: 312000\n",
            "Analyzing data...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "345it [06:34,  1.14s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Done and results stored for snapshot:  snapshot-312000\n",
            "Results for 312000  training iterations: 95 1 train error: 3.44 pixels. Test error: 5.48  pixels.\n",
            "With pcutoff of 0.6  train error: 2.75 pixels. Test error: 4.74 pixels\n",
            "Thereby, the errors are given by the average distances between the labels by DLC and the scorer.\n",
            "Plotting...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "application/javascript": [
              "/* Put everything inside the global mpl namespace */\n",
              "window.mpl = {};\n",
              "\n",
              "\n",
              "mpl.get_websocket_type = function() {\n",
              "    if (typeof(WebSocket) !== 'undefined') {\n",
              "        return WebSocket;\n",
              "    } else if (typeof(MozWebSocket) !== 'undefined') {\n",
              "        return MozWebSocket;\n",
              "    } else {\n",
              "        alert('Your browser does not have WebSocket support. ' +\n",
              "              'Please try Chrome, Safari or Firefox ≥ 6. ' +\n",
              "              'Firefox 4 and 5 are also supported but you ' +\n",
              "              'have to enable WebSockets in about:config.');\n",
              "    };\n",
              "}\n",
              "\n",
              "mpl.figure = function(figure_id, websocket, ondownload, parent_element) {\n",
              "    this.id = figure_id;\n",
              "\n",
              "    this.ws = websocket;\n",
              "\n",
              "    this.supports_binary = (this.ws.binaryType != undefined);\n",
              "\n",
              "    if (!this.supports_binary) {\n",
              "        var warnings = document.getElementById(\"mpl-warnings\");\n",
              "        if (warnings) {\n",
              "            warnings.style.display = 'block';\n",
              "            warnings.textContent = (\n",
              "                \"This browser does not support binary websocket messages. \" +\n",
              "                    \"Performance may be slow.\");\n",
              "        }\n",
              "    }\n",
              "\n",
              "    this.imageObj = new Image();\n",
              "\n",
              "    this.context = undefined;\n",
              "    this.message = undefined;\n",
              "    this.canvas = undefined;\n",
              "    this.rubberband_canvas = undefined;\n",
              "    this.rubberband_context = undefined;\n",
              "    this.format_dropdown = undefined;\n",
              "\n",
              "    this.image_mode = 'full';\n",
              "\n",
              "    this.root = $('<div/>');\n",
              "    this._root_extra_style(this.root)\n",
              "    this.root.attr('style', 'display: inline-block');\n",
              "\n",
              "    $(parent_element).append(this.root);\n",
              "\n",
              "    this._init_header(this);\n",
              "    this._init_canvas(this);\n",
              "    this._init_toolbar(this);\n",
              "\n",
              "    var fig = this;\n",
              "\n",
              "    this.waiting = false;\n",
              "\n",
              "    this.ws.onopen =  function () {\n",
              "            fig.send_message(\"supports_binary\", {value: fig.supports_binary});\n",
              "            fig.send_message(\"send_image_mode\", {});\n",
              "            if (mpl.ratio != 1) {\n",
              "                fig.send_message(\"set_dpi_ratio\", {'dpi_ratio': mpl.ratio});\n",
              "            }\n",
              "            fig.send_message(\"refresh\", {});\n",
              "        }\n",
              "\n",
              "    this.imageObj.onload = function() {\n",
              "            if (fig.image_mode == 'full') {\n",
              "                // Full images could contain transparency (where diff images\n",
              "                // almost always do), so we need to clear the canvas so that\n",
              "                // there is no ghosting.\n",
              "                fig.context.clearRect(0, 0, fig.canvas.width, fig.canvas.height);\n",
              "            }\n",
              "            fig.context.drawImage(fig.imageObj, 0, 0);\n",
              "        };\n",
              "\n",
              "    this.imageObj.onunload = function() {\n",
              "        fig.ws.close();\n",
              "    }\n",
              "\n",
              "    this.ws.onmessage = this._make_on_message_function(this);\n",
              "\n",
              "    this.ondownload = ondownload;\n",
              "}\n",
              "\n",
              "mpl.figure.prototype._init_header = function() {\n",
              "    var titlebar = $(\n",
              "        '<div class=\"ui-dialog-titlebar ui-widget-header ui-corner-all ' +\n",
              "        'ui-helper-clearfix\"/>');\n",
              "    var titletext = $(\n",
              "        '<div class=\"ui-dialog-title\" style=\"width: 100%; ' +\n",
              "        'text-align: center; padding: 3px;\"/>');\n",
              "    titlebar.append(titletext)\n",
              "    this.root.append(titlebar);\n",
              "    this.header = titletext[0];\n",
              "}\n",
              "\n",
              "\n",
              "\n",
              "mpl.figure.prototype._canvas_extra_style = function(canvas_div) {\n",
              "\n",
              "}\n",
              "\n",
              "\n",
              "mpl.figure.prototype._root_extra_style = function(canvas_div) {\n",
              "\n",
              "}\n",
              "\n",
              "mpl.figure.prototype._init_canvas = function() {\n",
              "    var fig = this;\n",
              "\n",
              "    var canvas_div = $('<div/>');\n",
              "\n",
              "    canvas_div.attr('style', 'position: relative; clear: both; outline: 0');\n",
              "\n",
              "    function canvas_keyboard_event(event) {\n",
              "        return fig.key_event(event, event['data']);\n",
              "    }\n",
              "\n",
              "    canvas_div.keydown('key_press', canvas_keyboard_event);\n",
              "    canvas_div.keyup('key_release', canvas_keyboard_event);\n",
              "    this.canvas_div = canvas_div\n",
              "    this._canvas_extra_style(canvas_div)\n",
              "    this.root.append(canvas_div);\n",
              "\n",
              "    var canvas = $('<canvas/>');\n",
              "    canvas.addClass('mpl-canvas');\n",
              "    canvas.attr('style', \"left: 0; top: 0; z-index: 0; outline: 0\")\n",
              "\n",
              "    this.canvas = canvas[0];\n",
              "    this.context = canvas[0].getContext(\"2d\");\n",
              "\n",
              "    var backingStore = this.context.backingStorePixelRatio ||\n",
              "\tthis.context.webkitBackingStorePixelRatio ||\n",
              "\tthis.context.mozBackingStorePixelRatio ||\n",
              "\tthis.context.msBackingStorePixelRatio ||\n",
              "\tthis.context.oBackingStorePixelRatio ||\n",
              "\tthis.context.backingStorePixelRatio || 1;\n",
              "\n",
              "    mpl.ratio = (window.devicePixelRatio || 1) / backingStore;\n",
              "\n",
              "    var rubberband = $('<canvas/>');\n",
              "    rubberband.attr('style', \"position: absolute; left: 0; top: 0; z-index: 1;\")\n",
              "\n",
              "    var pass_mouse_events = true;\n",
              "\n",
              "    canvas_div.resizable({\n",
              "        start: function(event, ui) {\n",
              "            pass_mouse_events = false;\n",
              "        },\n",
              "        resize: function(event, ui) {\n",
              "            fig.request_resize(ui.size.width, ui.size.height);\n",
              "        },\n",
              "        stop: function(event, ui) {\n",
              "            pass_mouse_events = true;\n",
              "            fig.request_resize(ui.size.width, ui.size.height);\n",
              "        },\n",
              "    });\n",
              "\n",
              "    function mouse_event_fn(event) {\n",
              "        if (pass_mouse_events)\n",
              "            return fig.mouse_event(event, event['data']);\n",
              "    }\n",
              "\n",
              "    rubberband.mousedown('button_press', mouse_event_fn);\n",
              "    rubberband.mouseup('button_release', mouse_event_fn);\n",
              "    // Throttle sequential mouse events to 1 every 20ms.\n",
              "    rubberband.mousemove('motion_notify', mouse_event_fn);\n",
              "\n",
              "    rubberband.mouseenter('figure_enter', mouse_event_fn);\n",
              "    rubberband.mouseleave('figure_leave', mouse_event_fn);\n",
              "\n",
              "    canvas_div.on(\"wheel\", function (event) {\n",
              "        event = event.originalEvent;\n",
              "        event['data'] = 'scroll'\n",
              "        if (event.deltaY < 0) {\n",
              "            event.step = 1;\n",
              "        } else {\n",
              "            event.step = -1;\n",
              "        }\n",
              "        mouse_event_fn(event);\n",
              "    });\n",
              "\n",
              "    canvas_div.append(canvas);\n",
              "    canvas_div.append(rubberband);\n",
              "\n",
              "    this.rubberband = rubberband;\n",
              "    this.rubberband_canvas = rubberband[0];\n",
              "    this.rubberband_context = rubberband[0].getContext(\"2d\");\n",
              "    this.rubberband_context.strokeStyle = \"#000000\";\n",
              "\n",
              "    this._resize_canvas = function(width, height) {\n",
              "        // Keep the size of the canvas, canvas container, and rubber band\n",
              "        // canvas in synch.\n",
              "        canvas_div.css('width', width)\n",
              "        canvas_div.css('height', height)\n",
              "\n",
              "        canvas.attr('width', width * mpl.ratio);\n",
              "        canvas.attr('height', height * mpl.ratio);\n",
              "        canvas.attr('style', 'width: ' + width + 'px; height: ' + height + 'px;');\n",
              "\n",
              "        rubberband.attr('width', width);\n",
              "        rubberband.attr('height', height);\n",
              "    }\n",
              "\n",
              "    // Set the figure to an initial 600x600px, this will subsequently be updated\n",
              "    // upon first draw.\n",
              "    this._resize_canvas(600, 600);\n",
              "\n",
              "    // Disable right mouse context menu.\n",
              "    $(this.rubberband_canvas).bind(\"contextmenu\",function(e){\n",
              "        return false;\n",
              "    });\n",
              "\n",
              "    function set_focus () {\n",
              "        canvas.focus();\n",
              "        canvas_div.focus();\n",
              "    }\n",
              "\n",
              "    window.setTimeout(set_focus, 100);\n",
              "}\n",
              "\n",
              "mpl.figure.prototype._init_toolbar = function() {\n",
              "    var fig = this;\n",
              "\n",
              "    var nav_element = $('<div/>');\n",
              "    nav_element.attr('style', 'width: 100%');\n",
              "    this.root.append(nav_element);\n",
              "\n",
              "    // Define a callback function for later on.\n",
              "    function toolbar_event(event) {\n",
              "        return fig.toolbar_button_onclick(event['data']);\n",
              "    }\n",
              "    function toolbar_mouse_event(event) {\n",
              "        return fig.toolbar_button_onmouseover(event['data']);\n",
              "    }\n",
              "\n",
              "    for(var toolbar_ind in mpl.toolbar_items) {\n",
              "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
              "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
              "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
              "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
              "\n",
              "        if (!name) {\n",
              "            // put a spacer in here.\n",
              "            continue;\n",
              "        }\n",
              "        var button = $('<button/>');\n",
              "        button.addClass('ui-button ui-widget ui-state-default ui-corner-all ' +\n",
              "                        'ui-button-icon-only');\n",
              "        button.attr('role', 'button');\n",
              "        button.attr('aria-disabled', 'false');\n",
              "        button.click(method_name, toolbar_event);\n",
              "        button.mouseover(tooltip, toolbar_mouse_event);\n",
              "\n",
              "        var icon_img = $('<span/>');\n",
              "        icon_img.addClass('ui-button-icon-primary ui-icon');\n",
              "        icon_img.addClass(image);\n",
              "        icon_img.addClass('ui-corner-all');\n",
              "\n",
              "        var tooltip_span = $('<span/>');\n",
              "        tooltip_span.addClass('ui-button-text');\n",
              "        tooltip_span.html(tooltip);\n",
              "\n",
              "        button.append(icon_img);\n",
              "        button.append(tooltip_span);\n",
              "\n",
              "        nav_element.append(button);\n",
              "    }\n",
              "\n",
              "    var fmt_picker_span = $('<span/>');\n",
              "\n",
              "    var fmt_picker = $('<select/>');\n",
              "    fmt_picker.addClass('mpl-toolbar-option ui-widget ui-widget-content');\n",
              "    fmt_picker_span.append(fmt_picker);\n",
              "    nav_element.append(fmt_picker_span);\n",
              "    this.format_dropdown = fmt_picker[0];\n",
              "\n",
              "    for (var ind in mpl.extensions) {\n",
              "        var fmt = mpl.extensions[ind];\n",
              "        var option = $(\n",
              "            '<option/>', {selected: fmt === mpl.default_extension}).html(fmt);\n",
              "        fmt_picker.append(option);\n",
              "    }\n",
              "\n",
              "    // Add hover states to the ui-buttons\n",
              "    $( \".ui-button\" ).hover(\n",
              "        function() { $(this).addClass(\"ui-state-hover\");},\n",
              "        function() { $(this).removeClass(\"ui-state-hover\");}\n",
              "    );\n",
              "\n",
              "    var status_bar = $('<span class=\"mpl-message\"/>');\n",
              "    nav_element.append(status_bar);\n",
              "    this.message = status_bar[0];\n",
              "}\n",
              "\n",
              "mpl.figure.prototype.request_resize = function(x_pixels, y_pixels) {\n",
              "    // Request matplotlib to resize the figure. Matplotlib will then trigger a resize in the client,\n",
              "    // which will in turn request a refresh of the image.\n",
              "    this.send_message('resize', {'width': x_pixels, 'height': y_pixels});\n",
              "}\n",
              "\n",
              "mpl.figure.prototype.send_message = function(type, properties) {\n",
              "    properties['type'] = type;\n",
              "    properties['figure_id'] = this.id;\n",
              "    this.ws.send(JSON.stringify(properties));\n",
              "}\n",
              "\n",
              "mpl.figure.prototype.send_draw_message = function() {\n",
              "    if (!this.waiting) {\n",
              "        this.waiting = true;\n",
              "        this.ws.send(JSON.stringify({type: \"draw\", figure_id: this.id}));\n",
              "    }\n",
              "}\n",
              "\n",
              "\n",
              "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
              "    var format_dropdown = fig.format_dropdown;\n",
              "    var format = format_dropdown.options[format_dropdown.selectedIndex].value;\n",
              "    fig.ondownload(fig, format);\n",
              "}\n",
              "\n",
              "\n",
              "mpl.figure.prototype.handle_resize = function(fig, msg) {\n",
              "    var size = msg['size'];\n",
              "    if (size[0] != fig.canvas.width || size[1] != fig.canvas.height) {\n",
              "        fig._resize_canvas(size[0], size[1]);\n",
              "        fig.send_message(\"refresh\", {});\n",
              "    };\n",
              "}\n",
              "\n",
              "mpl.figure.prototype.handle_rubberband = function(fig, msg) {\n",
              "    var x0 = msg['x0'] / mpl.ratio;\n",
              "    var y0 = (fig.canvas.height - msg['y0']) / mpl.ratio;\n",
              "    var x1 = msg['x1'] / mpl.ratio;\n",
              "    var y1 = (fig.canvas.height - msg['y1']) / mpl.ratio;\n",
              "    x0 = Math.floor(x0) + 0.5;\n",
              "    y0 = Math.floor(y0) + 0.5;\n",
              "    x1 = Math.floor(x1) + 0.5;\n",
              "    y1 = Math.floor(y1) + 0.5;\n",
              "    var min_x = Math.min(x0, x1);\n",
              "    var min_y = Math.min(y0, y1);\n",
              "    var width = Math.abs(x1 - x0);\n",
              "    var height = Math.abs(y1 - y0);\n",
              "\n",
              "    fig.rubberband_context.clearRect(\n",
              "        0, 0, fig.canvas.width / mpl.ratio, fig.canvas.height / mpl.ratio);\n",
              "\n",
              "    fig.rubberband_context.strokeRect(min_x, min_y, width, height);\n",
              "}\n",
              "\n",
              "mpl.figure.prototype.handle_figure_label = function(fig, msg) {\n",
              "    // Updates the figure title.\n",
              "    fig.header.textContent = msg['label'];\n",
              "}\n",
              "\n",
              "mpl.figure.prototype.handle_cursor = function(fig, msg) {\n",
              "    var cursor = msg['cursor'];\n",
              "    switch(cursor)\n",
              "    {\n",
              "    case 0:\n",
              "        cursor = 'pointer';\n",
              "        break;\n",
              "    case 1:\n",
              "        cursor = 'default';\n",
              "        break;\n",
              "    case 2:\n",
              "        cursor = 'crosshair';\n",
              "        break;\n",
              "    case 3:\n",
              "        cursor = 'move';\n",
              "        break;\n",
              "    }\n",
              "    fig.rubberband_canvas.style.cursor = cursor;\n",
              "}\n",
              "\n",
              "mpl.figure.prototype.handle_message = function(fig, msg) {\n",
              "    fig.message.textContent = msg['message'];\n",
              "}\n",
              "\n",
              "mpl.figure.prototype.handle_draw = function(fig, msg) {\n",
              "    // Request the server to send over a new figure.\n",
              "    fig.send_draw_message();\n",
              "}\n",
              "\n",
              "mpl.figure.prototype.handle_image_mode = function(fig, msg) {\n",
              "    fig.image_mode = msg['mode'];\n",
              "}\n",
              "\n",
              "mpl.figure.prototype.updated_canvas_event = function() {\n",
              "    // Called whenever the canvas gets updated.\n",
              "    this.send_message(\"ack\", {});\n",
              "}\n",
              "\n",
              "// A function to construct a web socket function for onmessage handling.\n",
              "// Called in the figure constructor.\n",
              "mpl.figure.prototype._make_on_message_function = function(fig) {\n",
              "    return function socket_on_message(evt) {\n",
              "        if (evt.data instanceof Blob) {\n",
              "            /* FIXME: We get \"Resource interpreted as Image but\n",
              "             * transferred with MIME type text/plain:\" errors on\n",
              "             * Chrome.  But how to set the MIME type?  It doesn't seem\n",
              "             * to be part of the websocket stream */\n",
              "            evt.data.type = \"image/png\";\n",
              "\n",
              "            /* Free the memory for the previous frames */\n",
              "            if (fig.imageObj.src) {\n",
              "                (window.URL || window.webkitURL).revokeObjectURL(\n",
              "                    fig.imageObj.src);\n",
              "            }\n",
              "\n",
              "            fig.imageObj.src = (window.URL || window.webkitURL).createObjectURL(\n",
              "                evt.data);\n",
              "            fig.updated_canvas_event();\n",
              "            fig.waiting = false;\n",
              "            return;\n",
              "        }\n",
              "        else if (typeof evt.data === 'string' && evt.data.slice(0, 21) == \"data:image/png;base64\") {\n",
              "            fig.imageObj.src = evt.data;\n",
              "            fig.updated_canvas_event();\n",
              "            fig.waiting = false;\n",
              "            return;\n",
              "        }\n",
              "\n",
              "        var msg = JSON.parse(evt.data);\n",
              "        var msg_type = msg['type'];\n",
              "\n",
              "        // Call the  \"handle_{type}\" callback, which takes\n",
              "        // the figure and JSON message as its only arguments.\n",
              "        try {\n",
              "            var callback = fig[\"handle_\" + msg_type];\n",
              "        } catch (e) {\n",
              "            console.log(\"No handler for the '\" + msg_type + \"' message type: \", msg);\n",
              "            return;\n",
              "        }\n",
              "\n",
              "        if (callback) {\n",
              "            try {\n",
              "                // console.log(\"Handling '\" + msg_type + \"' message: \", msg);\n",
              "                callback(fig, msg);\n",
              "            } catch (e) {\n",
              "                console.log(\"Exception inside the 'handler_\" + msg_type + \"' callback:\", e, e.stack, msg);\n",
              "            }\n",
              "        }\n",
              "    };\n",
              "}\n",
              "\n",
              "// from http://stackoverflow.com/questions/1114465/getting-mouse-location-in-canvas\n",
              "mpl.findpos = function(e) {\n",
              "    //this section is from http://www.quirksmode.org/js/events_properties.html\n",
              "    var targ;\n",
              "    if (!e)\n",
              "        e = window.event;\n",
              "    if (e.target)\n",
              "        targ = e.target;\n",
              "    else if (e.srcElement)\n",
              "        targ = e.srcElement;\n",
              "    if (targ.nodeType == 3) // defeat Safari bug\n",
              "        targ = targ.parentNode;\n",
              "\n",
              "    // jQuery normalizes the pageX and pageY\n",
              "    // pageX,Y are the mouse positions relative to the document\n",
              "    // offset() returns the position of the element relative to the document\n",
              "    var x = e.pageX - $(targ).offset().left;\n",
              "    var y = e.pageY - $(targ).offset().top;\n",
              "\n",
              "    return {\"x\": x, \"y\": y};\n",
              "};\n",
              "\n",
              "/*\n",
              " * return a copy of an object with only non-object keys\n",
              " * we need this to avoid circular references\n",
              " * http://stackoverflow.com/a/24161582/3208463\n",
              " */\n",
              "function simpleKeys (original) {\n",
              "  return Object.keys(original).reduce(function (obj, key) {\n",
              "    if (typeof original[key] !== 'object')\n",
              "        obj[key] = original[key]\n",
              "    return obj;\n",
              "  }, {});\n",
              "}\n",
              "\n",
              "mpl.figure.prototype.mouse_event = function(event, name) {\n",
              "    var canvas_pos = mpl.findpos(event)\n",
              "\n",
              "    if (name === 'button_press')\n",
              "    {\n",
              "        this.canvas.focus();\n",
              "        this.canvas_div.focus();\n",
              "    }\n",
              "\n",
              "    var x = canvas_pos.x * mpl.ratio;\n",
              "    var y = canvas_pos.y * mpl.ratio;\n",
              "\n",
              "    this.send_message(name, {x: x, y: y, button: event.button,\n",
              "                             step: event.step,\n",
              "                             guiEvent: simpleKeys(event)});\n",
              "\n",
              "    /* This prevents the web browser from automatically changing to\n",
              "     * the text insertion cursor when the button is pressed.  We want\n",
              "     * to control all of the cursor setting manually through the\n",
              "     * 'cursor' event from matplotlib */\n",
              "    event.preventDefault();\n",
              "    return false;\n",
              "}\n",
              "\n",
              "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
              "    // Handle any extra behaviour associated with a key event\n",
              "}\n",
              "\n",
              "mpl.figure.prototype.key_event = function(event, name) {\n",
              "\n",
              "    // Prevent repeat events\n",
              "    if (name == 'key_press')\n",
              "    {\n",
              "        if (event.which === this._key)\n",
              "            return;\n",
              "        else\n",
              "            this._key = event.which;\n",
              "    }\n",
              "    if (name == 'key_release')\n",
              "        this._key = null;\n",
              "\n",
              "    var value = '';\n",
              "    if (event.ctrlKey && event.which != 17)\n",
              "        value += \"ctrl+\";\n",
              "    if (event.altKey && event.which != 18)\n",
              "        value += \"alt+\";\n",
              "    if (event.shiftKey && event.which != 16)\n",
              "        value += \"shift+\";\n",
              "\n",
              "    value += 'k';\n",
              "    value += event.which.toString();\n",
              "\n",
              "    this._key_event_extra(event, name);\n",
              "\n",
              "    this.send_message(name, {key: value,\n",
              "                             guiEvent: simpleKeys(event)});\n",
              "    return false;\n",
              "}\n",
              "\n",
              "mpl.figure.prototype.toolbar_button_onclick = function(name) {\n",
              "    if (name == 'download') {\n",
              "        this.handle_save(this, null);\n",
              "    } else {\n",
              "        this.send_message(\"toolbar_button\", {name: name});\n",
              "    }\n",
              "};\n",
              "\n",
              "mpl.figure.prototype.toolbar_button_onmouseover = function(tooltip) {\n",
              "    this.message.textContent = tooltip;\n",
              "};\n",
              "mpl.toolbar_items = [[\"Home\", \"Reset original view\", \"fa fa-home icon-home\", \"home\"], [\"Back\", \"Back to previous view\", \"fa fa-arrow-left icon-arrow-left\", \"back\"], [\"Forward\", \"Forward to next view\", \"fa fa-arrow-right icon-arrow-right\", \"forward\"], [\"\", \"\", \"\", \"\"], [\"Pan\", \"Pan axes with left mouse, zoom with right\", \"fa fa-arrows icon-move\", \"pan\"], [\"Zoom\", \"Zoom to rectangle\", \"fa fa-square-o icon-check-empty\", \"zoom\"], [\"\", \"\", \"\", \"\"], [\"Download\", \"Download plot\", \"fa fa-floppy-o icon-save\", \"download\"]];\n",
              "\n",
              "mpl.extensions = [\"eps\", \"jpeg\", \"pdf\", \"png\", \"ps\", \"raw\", \"svg\", \"tif\"];\n",
              "\n",
              "mpl.default_extension = \"png\";var comm_websocket_adapter = function(comm) {\n",
              "    // Create a \"websocket\"-like object which calls the given IPython comm\n",
              "    // object with the appropriate methods. Currently this is a non binary\n",
              "    // socket, so there is still some room for performance tuning.\n",
              "    var ws = {};\n",
              "\n",
              "    ws.close = function() {\n",
              "        comm.close()\n",
              "    };\n",
              "    ws.send = function(m) {\n",
              "        //console.log('sending', m);\n",
              "        comm.send(m);\n",
              "    };\n",
              "    // Register the callback with on_msg.\n",
              "    comm.on_msg(function(msg) {\n",
              "        //console.log('receiving', msg['content']['data'], msg);\n",
              "        // Pass the mpl event to the overridden (by mpl) onmessage function.\n",
              "        ws.onmessage(msg['content']['data'])\n",
              "    });\n",
              "    return ws;\n",
              "}\n",
              "\n",
              "mpl.mpl_figure_comm = function(comm, msg) {\n",
              "    // This is the function which gets called when the mpl process\n",
              "    // starts-up an IPython Comm through the \"matplotlib\" channel.\n",
              "\n",
              "    var id = msg.content.data.id;\n",
              "    // Get hold of the div created by the display call when the Comm\n",
              "    // socket was opened in Python.\n",
              "    var element = $(\"#\" + id);\n",
              "    var ws_proxy = comm_websocket_adapter(comm)\n",
              "\n",
              "    function ondownload(figure, format) {\n",
              "        window.open(figure.imageObj.src);\n",
              "    }\n",
              "\n",
              "    var fig = new mpl.figure(id, ws_proxy,\n",
              "                           ondownload,\n",
              "                           element.get(0));\n",
              "\n",
              "    // Call onopen now - mpl needs it, as it is assuming we've passed it a real\n",
              "    // web socket which is closed, not our websocket->open comm proxy.\n",
              "    ws_proxy.onopen();\n",
              "\n",
              "    fig.parent_element = element.get(0);\n",
              "    fig.cell_info = mpl.find_output_cell(\"<div id='\" + id + \"'></div>\");\n",
              "    if (!fig.cell_info) {\n",
              "        console.error(\"Failed to find cell for figure\", id, fig);\n",
              "        return;\n",
              "    }\n",
              "\n",
              "    var output_index = fig.cell_info[2]\n",
              "    var cell = fig.cell_info[0];\n",
              "\n",
              "};\n",
              "\n",
              "mpl.figure.prototype.handle_close = function(fig, msg) {\n",
              "    var width = fig.canvas.width/mpl.ratio\n",
              "    fig.root.unbind('remove')\n",
              "\n",
              "    // Update the output cell to use the data from the current canvas.\n",
              "    fig.push_to_output();\n",
              "    var dataURL = fig.canvas.toDataURL();\n",
              "    // Re-enable the keyboard manager in IPython - without this line, in FF,\n",
              "    // the notebook keyboard shortcuts fail.\n",
              "    IPython.keyboard_manager.enable()\n",
              "    $(fig.parent_element).html('<img src=\"' + dataURL + '\" width=\"' + width + '\">');\n",
              "    fig.close_ws(fig, msg);\n",
              "}\n",
              "\n",
              "mpl.figure.prototype.close_ws = function(fig, msg){\n",
              "    fig.send_message('closing', msg);\n",
              "    // fig.ws.close()\n",
              "}\n",
              "\n",
              "mpl.figure.prototype.push_to_output = function(remove_interactive) {\n",
              "    // Turn the data on the canvas into data in the output cell.\n",
              "    var width = this.canvas.width/mpl.ratio\n",
              "    var dataURL = this.canvas.toDataURL();\n",
              "    this.cell_info[1]['text/html'] = '<img src=\"' + dataURL + '\" width=\"' + width + '\">';\n",
              "}\n",
              "\n",
              "mpl.figure.prototype.updated_canvas_event = function() {\n",
              "    // Tell IPython that the notebook contents must change.\n",
              "    IPython.notebook.set_dirty(true);\n",
              "    this.send_message(\"ack\", {});\n",
              "    var fig = this;\n",
              "    // Wait a second, then push the new image to the DOM so\n",
              "    // that it is saved nicely (might be nice to debounce this).\n",
              "    setTimeout(function () { fig.push_to_output() }, 1000);\n",
              "}\n",
              "\n",
              "mpl.figure.prototype._init_toolbar = function() {\n",
              "    var fig = this;\n",
              "\n",
              "    var nav_element = $('<div/>');\n",
              "    nav_element.attr('style', 'width: 100%');\n",
              "    this.root.append(nav_element);\n",
              "\n",
              "    // Define a callback function for later on.\n",
              "    function toolbar_event(event) {\n",
              "        return fig.toolbar_button_onclick(event['data']);\n",
              "    }\n",
              "    function toolbar_mouse_event(event) {\n",
              "        return fig.toolbar_button_onmouseover(event['data']);\n",
              "    }\n",
              "\n",
              "    for(var toolbar_ind in mpl.toolbar_items){\n",
              "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
              "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
              "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
              "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
              "\n",
              "        if (!name) { continue; };\n",
              "\n",
              "        var button = $('<button class=\"btn btn-default\" href=\"#\" title=\"' + name + '\"><i class=\"fa ' + image + ' fa-lg\"></i></button>');\n",
              "        button.click(method_name, toolbar_event);\n",
              "        button.mouseover(tooltip, toolbar_mouse_event);\n",
              "        nav_element.append(button);\n",
              "    }\n",
              "\n",
              "    // Add the status bar.\n",
              "    var status_bar = $('<span class=\"mpl-message\" style=\"text-align:right; float: right;\"/>');\n",
              "    nav_element.append(status_bar);\n",
              "    this.message = status_bar[0];\n",
              "\n",
              "    // Add the close button to the window.\n",
              "    var buttongrp = $('<div class=\"btn-group inline pull-right\"></div>');\n",
              "    var button = $('<button class=\"btn btn-mini btn-primary\" href=\"#\" title=\"Stop Interaction\"><i class=\"fa fa-power-off icon-remove icon-large\"></i></button>');\n",
              "    button.click(function (evt) { fig.handle_close(fig, {}); } );\n",
              "    button.mouseover('Stop Interaction', toolbar_mouse_event);\n",
              "    buttongrp.append(button);\n",
              "    var titlebar = this.root.find($('.ui-dialog-titlebar'));\n",
              "    titlebar.prepend(buttongrp);\n",
              "}\n",
              "\n",
              "mpl.figure.prototype._root_extra_style = function(el){\n",
              "    var fig = this\n",
              "    el.on(\"remove\", function(){\n",
              "\tfig.close_ws(fig, {});\n",
              "    });\n",
              "}\n",
              "\n",
              "mpl.figure.prototype._canvas_extra_style = function(el){\n",
              "    // this is important to make the div 'focusable\n",
              "    el.attr('tabindex', 0)\n",
              "    // reach out to IPython and tell the keyboard manager to turn it's self\n",
              "    // off when our div gets focus\n",
              "\n",
              "    // location in version 3\n",
              "    if (IPython.notebook.keyboard_manager) {\n",
              "        IPython.notebook.keyboard_manager.register_events(el);\n",
              "    }\n",
              "    else {\n",
              "        // location in version 2\n",
              "        IPython.keyboard_manager.register_events(el);\n",
              "    }\n",
              "\n",
              "}\n",
              "\n",
              "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
              "    var manager = IPython.notebook.keyboard_manager;\n",
              "    if (!manager)\n",
              "        manager = IPython.keyboard_manager;\n",
              "\n",
              "    // Check for shift+enter\n",
              "    if (event.shiftKey && event.which == 13) {\n",
              "        this.canvas_div.blur();\n",
              "        // select the cell after this one\n",
              "        var index = IPython.notebook.find_cell_index(this.cell_info[0]);\n",
              "        IPython.notebook.select(index + 1);\n",
              "    }\n",
              "}\n",
              "\n",
              "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
              "    fig.ondownload(fig, null);\n",
              "}\n",
              "\n",
              "\n",
              "mpl.find_output_cell = function(html_output) {\n",
              "    // Return the cell and output element which can be found *uniquely* in the notebook.\n",
              "    // Note - this is a bit hacky, but it is done because the \"notebook_saving.Notebook\"\n",
              "    // IPython event is triggered only after the cells have been serialised, which for\n",
              "    // our purposes (turning an active figure into a static one), is too late.\n",
              "    var cells = IPython.notebook.get_cells();\n",
              "    var ncells = cells.length;\n",
              "    for (var i=0; i<ncells; i++) {\n",
              "        var cell = cells[i];\n",
              "        if (cell.cell_type === 'code'){\n",
              "            for (var j=0; j<cell.output_area.outputs.length; j++) {\n",
              "                var data = cell.output_area.outputs[j];\n",
              "                if (data.data) {\n",
              "                    // IPython >= 3 moved mimebundle to data attribute of output\n",
              "                    data = data.data;\n",
              "                }\n",
              "                if (data['text/html'] == html_output) {\n",
              "                    return [cell, data, j];\n",
              "                }\n",
              "            }\n",
              "        }\n",
              "    }\n",
              "}\n",
              "\n",
              "// Register the function which deals with the matplotlib target/channel.\n",
              "// The kernel may be null if the page has been refreshed.\n",
              "if (IPython.notebook.kernel != null) {\n",
              "    IPython.notebook.kernel.comm_manager.register_target('matplotlib', mpl.mpl_figure_comm);\n",
              "}\n"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<div id='59a74377-db32-4375-970c-f0d2827896ee'></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 345/345 [00:58<00:00,  5.89it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The network is evaluated and the results are stored in the subdirectory 'evaluation_results'.\n",
            "If it generalizes well, choose the best model for prediction and update the config file with the appropriate index for the 'snapshotindex'.\n",
            "Use the function 'analyze_video' to make predictions on new videos.\n",
            "Otherwise consider retraining the network (see DeepLabCut workflow Fig 2)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FvyHjHj3co3U"
      },
      "source": [
        "#deeplabcut.extract_save_all_maps(path_config_file, shuffle=1, Indices=[0, 5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TFvNo-dekJP8"
      },
      "source": [
        "### Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UBJ3OPIacrGX"
      },
      "source": [
        "videofile_path = '/content/drive/My Drive/Stage/white_TRIM.avi'\n",
        "deeplabcut.analyze_videos(path_config_file, [videofile_path], save_as_csv=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LjDe1CZVcsrR"
      },
      "source": [
        "deeplabcut.plot_trajectories(path_config_file, videofile_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Z33vM8AcuW7"
      },
      "source": [
        "deeplabcut.create_labeled_video(path_config_file,videofile_path, videotype='.avi', draw_skeleton = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s4HNeV7kd7GQ",
        "outputId": "f700b4c7-34fe-440c-b86c-6170f9a6c0ac"
      },
      "source": [
        "deeplabcut.analyze_videos_converth5_to_csv('/content/drive/My Drive/Stage/Noise_check/2019/',videotype=VideoType)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found output file for scorer: DLC_resnet50_downsampled_trackerSep16shuffle1_180000\n",
            "Converting /content/drive/My Drive/Stage/Noise_check/2019/rat_training_OS_N_S_SD_CI_1_t0001downsampledDLC_resnet50_downsampled_trackerSep16shuffle1_180000.h5...\n",
            "Found output file for scorer: DLC_resnet50_downsampled_trackerSep16shuffle1_180000\n",
            "Converting /content/drive/My Drive/Stage/Noise_check/2019/rat_training_OS_N_S_SD_CI_1_t0002downsampledDLC_resnet50_downsampled_trackerSep16shuffle1_180000.h5...\n",
            "Found output file for scorer: DLC_resnet50_downsampled_trackerSep16shuffle1_180000\n",
            "Converting /content/drive/My Drive/Stage/Noise_check/2019/rat_training_OS_N_S_SD_CI_1_t0003downsampledDLC_resnet50_downsampled_trackerSep16shuffle1_180000.h5...\n",
            "Found output file for scorer: DLC_resnet50_downsampled_trackerSep16shuffle1_180000\n",
            "Converting /content/drive/My Drive/Stage/Noise_check/2019/rat_training_OS_N_S_SD_CI_1_t0004downsampledDLC_resnet50_downsampled_trackerSep16shuffle1_180000.h5...\n",
            "Found output file for scorer: DLC_resnet50_downsampled_trackerSep16shuffle1_180000\n",
            "Converting /content/drive/My Drive/Stage/Noise_check/2019/rat_training_OS_N_S_SD_CI_1_t0005downsampledDLC_resnet50_downsampled_trackerSep16shuffle1_180000.h5...\n",
            "Found output file for scorer: DLC_resnet50_downsampled_trackerSep16shuffle1_180000\n",
            "Converting /content/drive/My Drive/Stage/Noise_check/2019/rat_training_OS_N_S_SD_CI_1_t0006downsampledDLC_resnet50_downsampled_trackerSep16shuffle1_180000.h5...\n",
            "Found output file for scorer: DLC_resnet50_downsampled_trackerSep16shuffle1_180000\n",
            "Converting /content/drive/My Drive/Stage/Noise_check/2019/rat_training_OS_N_S_SD_CI_1_t0007downsampledDLC_resnet50_downsampled_trackerSep16shuffle1_180000.h5...\n",
            "Found output file for scorer: DLC_resnet50_downsampled_trackerSep16shuffle1_180000\n",
            "Converting /content/drive/My Drive/Stage/Noise_check/2019/rat_training_OS_N_S_SD_CI_1_t0008downsampledDLC_resnet50_downsampled_trackerSep16shuffle1_180000.h5...\n",
            "Found output file for scorer: DLC_resnet50_downsampled_trackerSep16shuffle1_180000\n",
            "Converting /content/drive/My Drive/Stage/Noise_check/2019/rat_training_OS_N_S_SD_CI_1_t0009downsampledDLC_resnet50_downsampled_trackerSep16shuffle1_180000.h5...\n",
            "Found output file for scorer: DLC_resnet50_downsampled_trackerSep16shuffle1_180000\n",
            "Converting /content/drive/My Drive/Stage/Noise_check/2019/rat_training_OS_N_S_SD_CI_1_t0010downsampledDLC_resnet50_downsampled_trackerSep16shuffle1_180000.h5...\n",
            "Found output file for scorer: DLC_resnet50_downsampled_trackerSep16shuffle1_180000\n",
            "Converting /content/drive/My Drive/Stage/Noise_check/2019/rat_training_OS_N_S_SD_CI_1_t0011downsampledDLC_resnet50_downsampled_trackerSep16shuffle1_180000.h5...\n",
            "Found output file for scorer: DLC_resnet50_downsampled_trackerSep16shuffle1_180000\n",
            "Converting /content/drive/My Drive/Stage/Noise_check/2019/rat_training_OS_N_S_SD_CI_1_t0012downsampledDLC_resnet50_downsampled_trackerSep16shuffle1_180000.h5...\n",
            "Found output file for scorer: DLC_resnet50_downsampled_trackerSep16shuffle1_180000\n",
            "Converting /content/drive/My Drive/Stage/Noise_check/2019/rat_training_OS_N_S_SD_CI_1_t0013downsampledDLC_resnet50_downsampled_trackerSep16shuffle1_180000.h5...\n",
            "Found output file for scorer: DLC_resnet50_downsampled_trackerSep16shuffle1_180000\n",
            "Converting /content/drive/My Drive/Stage/Noise_check/2019/rat_training_OS_N_S_SD_CI_1_t0014downsampledDLC_resnet50_downsampled_trackerSep16shuffle1_180000.h5...\n",
            "Found output file for scorer: DLC_resnet50_downsampled_trackerSep16shuffle1_180000\n",
            "Converting /content/drive/My Drive/Stage/Noise_check/2019/rat_training_OS_N_S_SD_CI_1_t0015downsampledDLC_resnet50_downsampled_trackerSep16shuffle1_180000.h5...\n",
            "Found output file for scorer: DLC_resnet50_downsampled_trackerSep16shuffle1_180000\n",
            "Converting /content/drive/My Drive/Stage/Noise_check/2019/rat_training_OS_rgs14_5_24h_72h_inc_216_221_t0001downsampledDLC_resnet50_downsampled_trackerSep16shuffle1_180000.h5...\n",
            "Found output file for scorer: DLC_resnet50_downsampled_trackerSep16shuffle1_180000\n",
            "Converting /content/drive/My Drive/Stage/Noise_check/2019/rat_training_OS_rgs14_5_24h_72h_inc_216_221_t0002downsampledDLC_resnet50_downsampled_trackerSep16shuffle1_180000.h5...\n",
            "Found output file for scorer: DLC_resnet50_downsampled_trackerSep16shuffle1_180000\n",
            "Converting /content/drive/My Drive/Stage/Noise_check/2019/rat_training_OS_rgs14_5_24h_72h_inc_216_221_t0003downsampledDLC_resnet50_downsampled_trackerSep16shuffle1_180000.h5...\n",
            "Found output file for scorer: DLC_resnet50_downsampled_trackerSep16shuffle1_180000\n",
            "Converting /content/drive/My Drive/Stage/Noise_check/2019/rat_training_OS_rgs14_5_24h_72h_inc_216_221_t0004downsampledDLC_resnet50_downsampled_trackerSep16shuffle1_180000.h5...\n",
            "Found output file for scorer: DLC_resnet50_downsampled_trackerSep16shuffle1_180000\n",
            "Converting /content/drive/My Drive/Stage/Noise_check/2019/rat_training_OS_rgs14_5_24h_72h_inc_216_221_t0005downsampledDLC_resnet50_downsampled_trackerSep16shuffle1_180000.h5...\n",
            "Found output file for scorer: DLC_resnet50_downsampled_trackerSep16shuffle1_180000\n",
            "Converting /content/drive/My Drive/Stage/Noise_check/2019/rat_training_OS_rgs14_5_24h_72h_inc_216_221_t0006downsampledDLC_resnet50_downsampled_trackerSep16shuffle1_180000.h5...\n",
            "Found output file for scorer: DLC_resnet50_downsampled_trackerSep16shuffle1_180000\n",
            "Converting /content/drive/My Drive/Stage/Noise_check/2019/rat_training_OS_rgs14_5_24h_72h_inc_216_221_t0007downsampledDLC_resnet50_downsampled_trackerSep16shuffle1_180000.h5...\n",
            "Found output file for scorer: DLC_resnet50_downsampled_trackerSep16shuffle1_180000\n",
            "Converting /content/drive/My Drive/Stage/Noise_check/2019/rat_training_OS_rgs14_5_24h_72h_inc_216_221_t0008downsampledDLC_resnet50_downsampled_trackerSep16shuffle1_180000.h5...\n",
            "Found output file for scorer: DLC_resnet50_downsampled_trackerSep16shuffle1_180000\n",
            "Converting /content/drive/My Drive/Stage/Noise_check/2019/rat_training_OS_rgs14_5_24h_72h_inc_216_221_t0009downsampledDLC_resnet50_downsampled_trackerSep16shuffle1_180000.h5...\n",
            "Found output file for scorer: DLC_resnet50_downsampled_trackerSep16shuffle1_180000\n",
            "Converting /content/drive/My Drive/Stage/Noise_check/2019/rat_training_OS_rgs14_5_24h_72h_inc_216_221_t0010downsampledDLC_resnet50_downsampled_trackerSep16shuffle1_180000.h5...\n",
            "Found output file for scorer: DLC_resnet50_downsampled_trackerSep16shuffle1_180000\n",
            "Converting /content/drive/My Drive/Stage/Noise_check/2019/rat_training_OS_rgs14_5_24h_72h_inc_216_221_t0011downsampledDLC_resnet50_downsampled_trackerSep16shuffle1_180000.h5...\n",
            "Found output file for scorer: DLC_resnet50_downsampled_trackerSep16shuffle1_180000\n",
            "Converting /content/drive/My Drive/Stage/Noise_check/2019/rat_training_OS_rgs14_5_24h_72h_inc_216_221_t0012downsampledDLC_resnet50_downsampled_trackerSep16shuffle1_180000.h5...\n",
            "Found output file for scorer: DLC_resnet50_downsampled_trackerSep16shuffle1_180000\n",
            "Converting /content/drive/My Drive/Stage/Noise_check/2019/rat_training_OS_rgs14_5_24h_72h_inc_216_221_t0013downsampledDLC_resnet50_downsampled_trackerSep16shuffle1_180000.h5...\n",
            "Found output file for scorer: DLC_resnet50_downsampled_trackerSep16shuffle1_180000\n",
            "Converting /content/drive/My Drive/Stage/Noise_check/2019/rat_training_OS_rgs14_5_24h_72h_inc_216_221_t0014downsampledDLC_resnet50_downsampled_trackerSep16shuffle1_180000.h5...\n",
            "Found output file for scorer: DLC_resnet50_downsampled_trackerSep16shuffle1_180000\n",
            "Converting /content/drive/My Drive/Stage/Noise_check/2019/rat_training_OS_rgs14_5_24h_72h_inc_216_221_t0015downsampledDLC_resnet50_downsampled_trackerSep16shuffle1_180000.h5...\n",
            "All pose files were converted.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1sPAlokkY04"
      },
      "source": [
        "### Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iKkhjg-znmLk",
        "outputId": "af365d08-76e0-4fb2-cbfc-921d639ccecf"
      },
      "source": [
        " import os\n",
        " basepath = '/content/drive/My Drive/Stage/Downsampled'\n",
        "\n",
        "tracker_name = 'DLC_resnet50_downsampled_trackerSep16shuffle1_180000'\n",
        "all_files = []\n",
        "for root, dirs, files in os.walk(basepath):\n",
        "    for file in files:\n",
        "        if file.endswith('.avi'):\n",
        "            file = file.strip('.avi')\n",
        "            file = file+tracker_name+'.csv'\n",
        "            all_files.append(file)\n",
        "            print(file)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trail_1_whitedownsampledDLC_resnet50_downsampled_trackerSep16shuffle1_180000.csv\n",
            "trail_1_bluedownsampledDLC_resnet50_downsampled_trackerSep16shuffle1_180000.csv\n",
            "trail_1_greendownsampledDLC_resnet50_downsampled_trackerSep16shuffle1_180000.csv\n",
            "trail_1_reddownsampledDLC_resnet50_downsampled_trackerSep16shuffle1_180000.csv\n",
            "trail_1_white(2)downsampledDLC_resnet50_downsampled_trackerSep16shuffle1_180000.csv\n",
            "trail_2_bluedownsampledDLC_resnet50_downsampled_trackerSep16shuffle1_180000.csv\n",
            "trail_2_greendownsampledDLC_resnet50_downsampled_trackerSep16shuffle1_180000.csv\n",
            "trail_2_reddownsampledDLC_resnet50_downsampled_trackerSep16shuffle1_180000.csv\n",
            "trail_2_white(2)downsampledDLC_resnet50_downsampled_trackerSep16shuffle1_180000.csv\n",
            "trail_2_whitedownsampledDLC_resnet50_downsampled_trackerSep16shuffle1_180000.csv\n",
            "trail_3_bluedownsampledDLC_resnet50_downsampled_trackerSep16shuffle1_180000.csv\n",
            "trail_3_greendownsampledDLC_resnet50_downsampled_trackerSep16shuffle1_180000.csv\n",
            "trail_3_whitedownsampledDLC_resnet50_downsampled_trackerSep16shuffle1_180000.csv\n",
            "trail_4_bluedownsampledDLC_resnet50_downsampled_trackerSep16shuffle1_180000.csv\n",
            "trail_4_greendownsampledDLC_resnet50_downsampled_trackerSep16shuffle1_180000.csv\n",
            "trail_4_white(2)downsampledDLC_resnet50_downsampled_trackerSep16shuffle1_180000.csv\n",
            "trail_4_whitedownsampledDLC_resnet50_downsampled_trackerSep16shuffle1_180000.csv\n",
            "trail_5_greendownsampledDLC_resnet50_downsampled_trackerSep16shuffle1_180000.csv\n",
            "trail_5_white(2)downsampledDLC_resnet50_downsampled_trackerSep16shuffle1_180000.csv\n",
            "trail_5_whitedownsampledDLC_resnet50_downsampled_trackerSep16shuffle1_180000.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-g47BNDPkXob",
        "outputId": "010a64e1-35f3-4aa0-861f-70b54ff3b687"
      },
      "source": [
        "import os\n",
        "import deeplabcut\n",
        "\n",
        "def get_analysis_files(directory, tracker_name):\n",
        "  #tracker_name = 'DLC_resnet50_downsampled_trackerSep16shuffle1_180000'\n",
        "  all_analysis_files = []\n",
        "  all_skeleton_files = []\n",
        "  for root, dirs, files in os.walk(directory):\n",
        "      for file in files:\n",
        "          if file.endswith('.avi'):\n",
        "              file = file.strip('.avi')\n",
        "              file_a = file+tracker_name+'.csv'\n",
        "              file_s = file+tracker_name+'_skeleton.csv'\n",
        "              all_analysis_files.append(file_a)\n",
        "              all_skeleton_files.append(file_s)\n",
        "  \n",
        "def main_deeplabcut(directory, config, videoType, save_csv, create_labeled, create_plots, analyze_skeleton):\n",
        "  # analyse\n",
        "  deeplabcut.analyze_videos(config, [directory], save_as_csv=save_csv)\n",
        "  if create_plots:\n",
        "    # plot trajectory\n",
        "    deeplabcut.plot_trajectories(config, [directory])\n",
        "  if create_labeled:\n",
        "    deeplabcut.create_labeled_video(config, [directory], videotype=videoType, draw_skeleton = True)\n",
        "  if analyze_skeleton:\n",
        "    deeplabcut.analyzeskeleton(config, [directory], videotype=videoType, shuffle=1, trainingsetindex=0, save_as_csv=True, destfolder=None)\n",
        "\n",
        "\n",
        "ProjectFolderName = 'Stage/final_tracker-Sanne-2021-10-20'\n",
        "VideoType = 'avi' \n",
        "path_config_file = '/content/drive/My Drive/'+ProjectFolderName+'/config.yaml'\n",
        "\n",
        "main_deeplabcut('/content/drive/My Drive/Stage/Noise_check/2020/',path_config_file,VideoType,True,True,True,True)\n",
        "main_deeplabcut('/content/drive/My Drive/Stage/Noise_check/2017/',path_config_file,VideoType,save_csv=False,create_labeled=False,create_plots=True,analyze_skeleton=False)\n",
        "main_deeplabcut('/content/drive/My Drive/Stage/Noise_check/2018/',path_config_file,VideoType,save_csv=False,create_labeled=False,create_plots=True,analyze_skeleton=False)\n",
        "main_deeplabcut('/content/drive/My Drive/Stage/Noise_check/2019/',path_config_file,VideoType,save_csv=False,create_labeled=False,create_plots=True,analyze_skeleton=False)\n",
        "main_deeplabcut('/content/drive/My Drive/Stage/Trials/Downsampled/',path_config_file,VideoType,save_csv=False,create_labeled=False,create_plots=True,analyze_skeleton=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using snapshot-312000 for model /content/drive/My Drive/Stage/final_tracker-Sanne-2021-10-20/dlc-models/iteration-0/final_trackerOct20-trainset95shuffle1\n",
            "Analyzing all the videos in the directory...\n",
            "Starting to analyze %  /content/drive/My Drive/Stage/Noise_check/2020/rat_training_OS_N_S_SD_CI_1_t0004_raw_2020downsampled.avi\n",
            "/content/drive/My Drive/Stage/Noise_check/2020  already exists!\n",
            "Loading  /content/drive/My Drive/Stage/Noise_check/2020/rat_training_OS_N_S_SD_CI_1_t0004_raw_2020downsampled.avi\n",
            "Duration of video [s]:  336.37 , recorded with  30.0 fps!\n",
            "Overall # of frames:  10091  found with (before cropping) frame dimensions:  341 256\n",
            "Starting to extract posture\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  3%|▎         | 300/10091 [01:16<44:24,  3.67it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hk67h0rHfiRF"
      },
      "source": [
        "## Automatic assesment of tracker\n",
        "\n",
        "- ### tracking an object:\n",
        "\t##### signs:\n",
        "\t- Huge jump\n",
        "\t- Constant coordinates for a long period of time\n",
        "\t- likelihood can be high in this case (sometimes its just super certain its a rat while its not)\n",
        "- ### Uncertain tracking:\n",
        "\t##### signs:\n",
        "\t- Likelihood around 20-80%\n",
        "\t- Tracking graph might seem reasonable but is probably tracking wrong parts\n",
        "\t\t- Should manually assess the video to be certain\n",
        "\t- Large gaps when applying pc-cutOff (basically everything below 0,6 is not taken into account when plotting the trajectory or making a video)\n",
        "\n",
        "\n",
        "### What do I want to achieve:\n",
        "\n",
        "\t- Average likelihood of all bodyparts seperatly\n",
        "\t- Average likelihood of everything combined\n",
        "\t- List of videos that performed poorly\n",
        "\t- Reason as to why they performed poorly\n",
        "\t\t- Tracking an object (object not used before has similar features)\n",
        "\t\t- Uncertain tracking (new situation not trained on before)\n",
        "\n",
        "\n",
        "### Scoring the performance\n",
        "\n",
        "things to consider in score:\n",
        "\n",
        "\n",
        "*   Head and spine most important\n",
        "*   Tail parts least important\n",
        "*   Outliers can mean lots of false positives\n",
        "*   High likelihood is good if most is high and the rest very low if there is more in between than low, the model is uncertain\n",
        "\n",
        "\n",
        "### likelihood system:\n",
        "I give each video a score based on their likelihoods by calculating how much of their values are above 80%, between 80% and 20% and below 20%. If most is above 80% it is good but there might still be uncertainty. If most if above 80% and the more is below 20% than in the between area it means there is less uncertainty and it is good. If the likelihood is not on average above 80% it means it is simply uncertain\n",
        "\n",
        "these percentages should change\n",
        "\n",
        "*   1 = high likelihood and not much uncertainty\n",
        "*   2 = high likelihood but with some uncertainty\n",
        "*   3 = low likelihood with much uncertainty\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nCCt01McfrUS",
        "outputId": "58b27803-ebc6-4cfb-8e5e-4b37fcfaae75"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "import os\n",
        "\n",
        "\n",
        "def main3():\n",
        "  file_path = '/content/drive/My Drive/Stage/Noise_check/2017/'\n",
        "  #analysis_file = 'rat_training_OS_SERT_5d_t0013downsampledDLC_resnet50_downsampled_trackerSep16shuffle1_180000.csv'\n",
        "  \n",
        "  \n",
        "  all_files = []\n",
        "  for root, dirs, files in os.walk(file_path):\n",
        "      for file in files:\n",
        "          if file.endswith('.csv'):\n",
        "              if 'skeleton' not in file:\n",
        "                all_files.append(file)\n",
        "                print(file)\n",
        "  parts_dic = {'x' : 'Head', 'x.1' : 'Side left', 'x.2' : 'Side right', 'x.3' : 'Spine', 'x.4' : 'Tail base', 'x.5': 'Tail middle', 'x.6': 'Tail end'}\n",
        "  parts = ['x', 'x.1', 'x.2', 'x.3', 'x.4', 'x.5', 'x.6']\n",
        "  result_dic = {}\n",
        "\n",
        "  \n",
        "  for analysis_file in all_files:\n",
        "    DF_analysis = pd.read_csv(file_path+analysis_file, skiprows=2)\n",
        "    t = DF_analysis.diff()\n",
        "  #  print('Outlier/Jump analysis')\n",
        "    full_o = []\n",
        "    for value in parts:\n",
        "      # if the outlier value is above like 0.1% of everyting its already too much\n",
        "      # outlier value should be below 0.09% Basically as little as possible bc jumps of 100 pixels should not occur\n",
        "      outliers =t.loc[t[value]> 100]\n",
        "      #print(parts_dic[value])\n",
        "      #print(str(len(outliers)/len(DF_analysis)*100) + ' %')\n",
        "      full_o.append(len(outliers)/len(DF_analysis)*100)\n",
        "    print(sum(full_o)/len(full_o))\n",
        "    if sum(full_o)/len(full_o) < 0.5:\n",
        "      print('false')\n",
        "      result_dic[analysis_file] = {'outlier' : False}\n",
        "    else:\n",
        "      print('true')\n",
        "      result_dic[analysis_file] = {'outlier' : True}\n",
        "    result_dic[analysis_file]['outlier_results'] = full_o\n",
        "\n",
        "    #print()\n",
        "    #print('likelihood analysis')\n",
        "    full_l = []\n",
        "    full_c = []\n",
        "    full_low = []\n",
        "    for part in parts:\n",
        "      likelihood = part.replace('x', 'likelihood')\n",
        "      # if a and b are low and c is high it is good\n",
        "      # if b and a are high en c is low it is very bad\n",
        "      # if b is low and b is average and c is high it is good\n",
        "      ## Find out what high, low and average values are\n",
        "      uncertain = DF_analysis.loc[DF_analysis[likelihood].between(0.1,0.9)]\n",
        "      low = DF_analysis.loc[DF_analysis[likelihood] < 0.1]\n",
        "      high = DF_analysis.loc[DF_analysis[likelihood] > 0.9]\n",
        "      full_l.append(len(high)/len(DF_analysis)*100)\n",
        "      full_c.append(len(uncertain)/len(DF_analysis)*100)\n",
        "      full_low.append(len(low)/len(DF_analysis)*100)\n",
        "      #print(parts_dic[part])\n",
        "      #print(len(uncertain)/len(DF_analysis)*100)\n",
        "      #print(len(low)/len(DF_analysis)*100)\n",
        "      #print(len(high)/len(DF_analysis)*100)\n",
        "    if sum(full_l)/len(full_l) >= 80.0 and sum(full_low)/len(full_low) > sum(full_c)/len(full_c):\n",
        "      result_dic[analysis_file]['likelihood'] = 1\n",
        "    elif sum(full_l)/len(full_l) >= 80.0:\n",
        "      result_dic[analysis_file]['likelihood'] = 2\n",
        "    else:\n",
        "      result_dic[analysis_file]['likelihood'] = 3\n",
        "    result_dic[analysis_file]['likelihood_results'] = full_l\n",
        "    max_l = 'x.' + str(result_dic[analysis_file]['likelihood_results'].\n",
        "                      index(max(result_dic[analysis_file]['likelihood_results']))\n",
        "                      )\n",
        "    min_l = 'x.' + str(result_dic[analysis_file]['likelihood_results'].\n",
        "                      index(min(result_dic[analysis_file]['likelihood_results']))\n",
        "                      )\n",
        "    max_o = 'x.' + str(result_dic[analysis_file]['outlier_results'].\n",
        "                      index(max(result_dic[analysis_file]['outlier_results']))\n",
        "                      )\n",
        "    min_o = 'x.' + str(result_dic[analysis_file]['outlier_results'].\n",
        "                      index(min(result_dic[analysis_file]['outlier_results']))\n",
        "                      )\n",
        "  return result_dic\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "d = main3()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "rat_training_OS_SERT_5d_t0001downsampledDLC_resnet50_downsampled_trackerSep16shuffle1_180000.csv\n",
            "rat_training_OS_SERT_5d_t0002downsampledDLC_resnet50_downsampled_trackerSep16shuffle1_180000.csv\n",
            "rat_training_OS_SERT_5d_t0003downsampledDLC_resnet50_downsampled_trackerSep16shuffle1_180000.csv\n",
            "rat_training_OS_SERT_5d_t0004downsampledDLC_resnet50_downsampled_trackerSep16shuffle1_180000.csv\n",
            "rat_training_OS_SERT_5d_t0005downsampledDLC_resnet50_downsampled_trackerSep16shuffle1_180000.csv\n",
            "rat_training_OS_SERT_5d_t0006downsampledDLC_resnet50_downsampled_trackerSep16shuffle1_180000.csv\n",
            "rat_training_OS_SERT_5d_t0007downsampledDLC_resnet50_downsampled_trackerSep16shuffle1_180000.csv\n",
            "rat_training_OS_SERT_5d_t0008downsampledDLC_resnet50_downsampled_trackerSep16shuffle1_180000.csv\n",
            "rat_training_OS_SERT_5d_t0009downsampledDLC_resnet50_downsampled_trackerSep16shuffle1_180000.csv\n",
            "rat_training_OS_SERT_5d_t0010downsampledDLC_resnet50_downsampled_trackerSep16shuffle1_180000.csv\n",
            "rat_training_OS_SERT_5d_t0011downsampledDLC_resnet50_downsampled_trackerSep16shuffle1_180000.csv\n",
            "rat_training_OS_SERT_5d_t0012downsampledDLC_resnet50_downsampled_trackerSep16shuffle1_180000.csv\n",
            "rat_training_OS_SERT_5d_t0013downsampledDLC_resnet50_downsampled_trackerSep16shuffle1_180000.csv\n",
            "rat_training_OS_SERT_5d_t0014downsampledDLC_resnet50_downsampled_trackerSep16shuffle1_180000.csv\n",
            "rat_training_OS_SERT_5d_t0015downsampledDLC_resnet50_downsampled_trackerSep16shuffle1_180000.csv\n",
            "rat_training_OS_SERT_5d_t0016downsampledDLC_resnet50_downsampled_trackerSep16shuffle1_180000.csv\n",
            "rat_training_OS_SERT_5d_t0017downsampledDLC_resnet50_downsampled_trackerSep16shuffle1_180000.csv\n",
            "rat_training_OS_SERT_5d_t0018downsampledDLC_resnet50_downsampled_trackerSep16shuffle1_180000.csv\n",
            "rat_training_OS_SERT_5d_t0019downsampledDLC_resnet50_downsampled_trackerSep16shuffle1_180000.csv\n",
            "rat_training_OS_SERT_5d_t0020downsampledDLC_resnet50_downsampled_trackerSep16shuffle1_180000.csv\n",
            "1.3694621642184117\n",
            "true\n",
            "0.8635527330998872\n",
            "true\n",
            "1.097076439542193\n",
            "true\n",
            "0.9384129607224009\n",
            "true\n",
            "1.3243683781581093\n",
            "true\n",
            "0.9836937256292095\n",
            "true\n",
            "1.2038523274478332\n",
            "true\n",
            "1.3429227968605448\n",
            "true\n",
            "1.946572789397391\n",
            "true\n",
            "0.16494845360824745\n",
            "false\n",
            "1.0692010692010694\n",
            "true\n",
            "1.022131098112525\n",
            "true\n",
            "0.854100854100854\n",
            "true\n",
            "0.2560057154764385\n",
            "false\n",
            "0.8945513689346706\n",
            "true\n",
            "0.7148889961389963\n",
            "true\n",
            "2.0174708818635607\n",
            "true\n",
            "0.9001465354825205\n",
            "true\n",
            "1.9880476891197933\n",
            "true\n",
            "0.6136424272967125\n",
            "true\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yy2sqc6gnUB1",
        "outputId": "01349f1d-ac85-4569-a24a-6ceb23753f7f"
      },
      "source": [
        "d_2 = pd.DataFrame.from_dict(d, orient='index')\n",
        "videos_for_manual_inspection = d_2[(d_2['likelihood'].between(2,3)) & (d_2['outlier']==True)]\n",
        "print(videos_for_manual_inspection)\n",
        "d_2.to_csv('assess_2.csv')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                                    outlier  ...                                 likelihood_results\n",
            "rat_training_OS_SERT_5d_t0001downsampledDLC_res...     True  ...  [20.995493650143384, 55.36665301106104, 75.747...\n",
            "rat_training_OS_SERT_5d_t0002downsampledDLC_res...     True  ...  [16.784378894889905, 59.45159950145409, 58.558...\n",
            "rat_training_OS_SERT_5d_t0003downsampledDLC_res...     True  ...  [18.472395184723954, 27.41801577418016, 39.684...\n",
            "rat_training_OS_SERT_5d_t0004downsampledDLC_res...     True  ...  [13.509605453418716, 43.069613716174345, 44.24...\n",
            "rat_training_OS_SERT_5d_t0005downsampledDLC_res...     True  ...  [9.963325183374083, 20.925020374898125, 26.792...\n",
            "rat_training_OS_SERT_5d_t0006downsampledDLC_res...     True  ...  [7.07196029776675, 38.13068651778329, 43.36228...\n",
            "rat_training_OS_SERT_5d_t0007downsampledDLC_res...     True  ...  [18.830628381190177, 19.662921348314608, 30.02...\n",
            "rat_training_OS_SERT_5d_t0008downsampledDLC_res...     True  ...  [24.65009400459578, 36.724462084813034, 39.064...\n",
            "rat_training_OS_SERT_5d_t0009downsampledDLC_res...     True  ...  [26.175191551045767, 31.72499482294471, 57.299...\n",
            "rat_training_OS_SERT_5d_t0011downsampledDLC_res...     True  ...  [61.62162162162163, 68.56548856548856, 84.2619...\n",
            "rat_training_OS_SERT_5d_t0012downsampledDLC_res...     True  ...  [45.14563106796117, 39.61587167581258, 74.8205...\n",
            "rat_training_OS_SERT_5d_t0013downsampledDLC_res...     True  ...  [25.614250614250615, 41.584766584766584, 63.77...\n",
            "rat_training_OS_SERT_5d_t0015downsampledDLC_res...     True  ...  [18.532574320050603, 22.51739405439595, 40.375...\n",
            "rat_training_OS_SERT_5d_t0016downsampledDLC_res...     True  ...  [12.246621621621621, 38.28125, 41.955236486486...\n",
            "rat_training_OS_SERT_5d_t0017downsampledDLC_res...     True  ...  [51.331114808652245, 50.374376039933445, 72.25...\n",
            "rat_training_OS_SERT_5d_t0018downsampledDLC_res...     True  ...  [7.682646012141511, 30.604982206405694, 71.048...\n",
            "rat_training_OS_SERT_5d_t0019downsampledDLC_res...     True  ...  [33.46647046457852, 19.76035316375867, 79.5669...\n",
            "rat_training_OS_SERT_5d_t0020downsampledDLC_res...     True  ...  [1.016808466486823, 36.812616725461716, 46.067...\n",
            "\n",
            "[18 rows x 4 columns]\n"
          ]
        }
      ]
    }
  ]
}